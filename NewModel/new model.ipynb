{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import logging\n",
    "import gdown\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "import albumentations as A\n",
    "\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from scipy.io import loadmat\n",
    "from sklearn.manifold import TSNE\n",
    "from torchmetrics.classification import MulticlassF1Score, JaccardIndex, MulticlassPrecision, MulticlassRecall, MulticlassAveragePrecision\n",
    "import pandas as pd\n",
    "from torchinfo import torchinfo\n",
    "\n",
    "from transformers import ConvNextV2Model, BertModel, BertTokenizer, ViTModel, ViTConfig\n",
    "from transformers import AutoTokenizer, AutoModel, RobertaTokenizer, CLIPModel, CLIPTokenizer, CLIPProcessor\n",
    "from transformers import DeiTConfig, DeiTFeatureExtractor, DeiTImageProcessor, DeiTModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    \n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '../Datasets/ocular-disease-recognition-odir5k/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = ROOT_DIR + 'dataset_single_eye.csv'\n",
    "TEST_CSV = ROOT_DIR + 'TESTING_dataset_single_eye.csv'\n",
    "IMG_PATH = ROOT_DIR + 'preprocessed_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.io.read_image(IMG_PATH + '0_left.jpg').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df = pd.read_csv(CSV_PATH)\n",
    "test_df = pd.read_csv(TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df:pd.DataFrame):\n",
    "    df['Keywords'] = df['Keywords'].str.lower()\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: \" \".join(x.split()))\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
    "    return df\n",
    "train_val_df = preprocess_text(train_val_df)\n",
    "test_df = preprocess_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_val_df['Keywords'].apply(lambda x: len(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4877, 861)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(train_val_df, test_size = 0.15, random_state= 123456)\n",
    "len(train_df), len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "rescale_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(IMG_SIZE, antialias = False, interpolation = torchvision.transforms.InterpolationMode.NEAREST),\n",
    "    # torchvision.transforms.Normalize(\n",
    "    #     timm.data.constants.IMAGENET_DEFAULT_MEAN,\n",
    "    #     timm.data.constants.IMAGENET_DEFAULT_STD\n",
    "    # )\n",
    "])\n",
    "\n",
    "augmentation = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(p = 0.5),\n",
    "    torchvision.transforms.RandomVerticalFlip(p= 0.5),\n",
    "    #torchvision.transforms.RandomRotation(90)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'moderate non proliferative retinopathy'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_df['Keywords'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\krish/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "processor = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODIRDatasetMM(Dataset) :\n",
    "    def __init__(self, df, IMG_FOLDER, tokenizer = processor, feature_extractor = rescale_transform, augmentation = None) :\n",
    "        '''\n",
    "        id : list of samples ids as string\n",
    "        '''\n",
    "        self.text = [tokenizer(text = x, padding = 'max_length', max_length = 45, truncation = True, return_tensors = 'pt') for x in df['Keywords']]\n",
    "        self.eye = df['eye']\n",
    "        self.labels = torch.tensor(df[['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']].to_numpy()).float()\n",
    "        self.img_dir = [IMG_PATH + x for x in df['Image']]\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "        self.images = [feature_extractor(torchvision.io.read_image(x).float()/255.0) for x in self.img_dir]\n",
    "        #self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        batch_imgs = self.images[idx]\n",
    "        if(self.augmentation is not None):\n",
    "            batch_imgs = self.augmentation(batch_imgs)\n",
    "        return batch_imgs, self.text[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ODIRDatasetMM(train_df, IMG_PATH, augmentation = augmentation)\n",
    "val_dataset   = ODIRDatasetMM(val_df, IMG_PATH)\n",
    "test_dataset  = ODIRDatasetMM(test_df, IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size= BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrastive learning on training data finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare two models: BERT vs ConvNext, try to compute contrastive losses\n",
    "class ContrastiveLearning(nn.Module):\n",
    "    def __init__(self, drop_prob = 0.4):\n",
    "        super().__init__()\n",
    "        self.img_model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)# torchvision.models.efficientnet_b3(weights = torchvision.models.EfficientNet_B3_Weights.DEFAULT)#torchvision.models.vit_b_32(weights = torchvision.models.ViT_B_32_Weights.DEFAULT)#torch.hub.load('facebookresearch/deit:main', 'deit_large_patch16_224', pretrained=True) #output 768 features\n",
    "        self.img_model.head = nn.Linear(768, 768)                                          \n",
    "        # self.img_model.classifier = nn.Sequential(\n",
    "        #     nn.Linear(1536, 768)\n",
    "        # )\n",
    "\n",
    "        self.txt_model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'roberta-base')         #output 768 features\n",
    "        \n",
    "        # image model classification head\n",
    "        self.fc1 = nn.Linear(768, 768)\n",
    "        self.fc2 = nn.Linear(768, 768)\n",
    "        self.img_head = nn.Linear(768, 8)\n",
    "        \n",
    "        # text model classification ehad\n",
    "        self.fc3 = nn.Linear(768, 768)\n",
    "        self.fc4 = nn.Linear(768, 768)\n",
    "        self.text_head = nn.Linear(768, 8)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, img_input = None, input_ids = None, attn_mask = None, contrastive = False):#, text_class = False):\n",
    "        if(contrastive):\n",
    "            # pretraining\n",
    "            out_txt = self.txt_model(input_ids, attn_mask)['pooler_output']\n",
    "            out_img = self.img_model(img_input)\n",
    "\n",
    "            out_txt_ret = F.normalize(out_txt, p = 2.0, dim = 1)\n",
    "            out_img_ret = F.normalize(out_txt, p = 2.0, dim = 1)\n",
    "\n",
    "            #img path\n",
    "            resi_img = out_img\n",
    "            img_route_out = self.dropout_layer(out_img)\n",
    "            img_route_out = F.relu(self.fc1(img_route_out))\n",
    "            img_route_out = self.dropout_layer(img_route_out)\n",
    "            img_route_out = F.relu(self.fc2(img_route_out))\n",
    "            img_route_out = img_route_out + resi_img\n",
    "\n",
    "            img_route_out = F.sigmoid(self.img_head(img_route_out))\n",
    "\n",
    "            #text path\n",
    "            resi_txt = out_txt\n",
    "            txt_route_out = self.dropout_layer(out_txt)\n",
    "            txt_route_out = F.relu(self.fc3(txt_route_out))\n",
    "            txt_route_out = self.dropout_layer(txt_route_out)\n",
    "            txt_route_out = F.relu(self.fc4(txt_route_out))\n",
    "            txt_route_out = txt_route_out + resi_txt\n",
    "\n",
    "            txt_route_out = F.sigmoid(self.text_head(txt_route_out))\n",
    "            return out_img_ret, out_txt_ret, img_route_out, txt_route_out\n",
    "\n",
    "            #return out_img, out_txt\n",
    "        else:\n",
    "            #if(not text_class):\n",
    "            out = self.img_model(img_input)\n",
    "            resi_img = out\n",
    "            img_route_out = self.dropout_layer(out)\n",
    "            img_route_out = F.relu(self.fc1(img_route_out))\n",
    "            img_route_out = self.dropout_layer(img_route_out)\n",
    "            img_route_out = F.relu(self.fc2(img_route_out))\n",
    "            img_route_out = img_route_out + resi_img\n",
    "\n",
    "            out = F.sigmoid(self.img_head(out))\n",
    "            return out\n",
    "            # else:\n",
    "            #     out = self.txt_model(input_ids, attn_mask)['pooler_output']\n",
    "            #     resi = out\n",
    "            #     out = self.dropout_layer(out)\n",
    "            #     out = F.relu(self.fc2(out))\n",
    "            #     out = resi + out\n",
    "\n",
    "            #     out = F.sigmoid(self.text_head(out))\n",
    "            #     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(ten1, ten2, temperature = nn.Parameter(torch.tensor(.25).to(device))):    #...\n",
    "    #steps = hadamard product\n",
    "    # trivial for loop \n",
    "    sim = torch.einsum('i d, j d -> i j', ten1, ten2) * temperature.exp()\n",
    "    labels = torch.arange(ten1.size(0), device = device)\n",
    "    loss = (F.cross_entropy(sim, labels) + F.cross_entropy(sim.t(), labels)) / 2\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\krish/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\krish/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = ContrastiveLearning().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:57<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfreezing text model component\n",
      "Epoch [1/15], Train Loss: 7.4549, Train Accuracy: 0.9022 img, 0.9472 txt\n",
      "Epoch [1/15], Val Loss: 6.8340, Val Accuracy: 0.9014 img, 0.9988 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:56<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [2/15], Train Loss: 6.5524, Train Accuracy: 0.9059 img, 0.9988 txt\n",
      "Epoch [2/15], Val Loss: 6.5402, Val Accuracy: 0.9014 img, 0.9993 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:46<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [3/15], Train Loss: 6.3652, Train Accuracy: 0.9124 img, 0.9985 txt\n",
      "Epoch [3/15], Val Loss: 6.4451, Val Accuracy: 0.9019 img, 0.9993 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:46<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [4/15], Train Loss: 6.2923, Train Accuracy: 0.9182 img, 0.9983 txt\n",
      "Epoch [4/15], Val Loss: 6.3909, Val Accuracy: 0.9030 img, 0.9991 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:46<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [5/15], Train Loss: 6.2498, Train Accuracy: 0.9256 img, 0.9982 txt\n",
      "Epoch [5/15], Val Loss: 6.3742, Val Accuracy: 0.9028 img, 0.9991 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:46<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [6/15], Train Loss: 6.2215, Train Accuracy: 0.9318 img, 0.9982 txt\n",
      "Epoch [6/15], Val Loss: 6.3451, Val Accuracy: 0.9034 img, 0.9990 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:46<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [7/15], Train Loss: 6.1933, Train Accuracy: 0.9382 img, 0.9982 txt\n",
      "Epoch [7/15], Val Loss: 6.3375, Val Accuracy: 0.9038 img, 0.9990 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:47<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [8/15], Train Loss: 6.1806, Train Accuracy: 0.9455 img, 0.9982 txt\n",
      "Epoch [8/15], Val Loss: 6.3243, Val Accuracy: 0.9044 img, 0.9991 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:46<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [9/15], Train Loss: 6.1662, Train Accuracy: 0.9514 img, 0.9981 txt\n",
      "Epoch [9/15], Val Loss: 6.3245, Val Accuracy: 0.9046 img, 0.9991 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:46<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [10/15], Train Loss: 6.1446, Train Accuracy: 0.9596 img, 0.9982 txt\n",
      "Epoch [10/15], Val Loss: 6.3252, Val Accuracy: 0.9045 img, 0.9991 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:46<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [11/15], Train Loss: 6.1315, Train Accuracy: 0.9686 img, 0.9982 txt\n",
      "Epoch [11/15], Val Loss: 6.3254, Val Accuracy: 0.9044 img, 0.9991 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:46<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [12/15], Train Loss: 6.1245, Train Accuracy: 0.9707 img, 0.9982 txt\n",
      "Epoch [12/15], Val Loss: 6.3233, Val Accuracy: 0.9042 img, 0.9991 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:46<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [13/15], Train Loss: 6.1149, Train Accuracy: 0.9772 img, 0.9983 txt\n",
      "Epoch [13/15], Val Loss: 6.3381, Val Accuracy: 0.9044 img, 0.9993 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:46<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [14/15], Train Loss: 6.1017, Train Accuracy: 0.9814 img, 0.9987 txt\n",
      "Epoch [14/15], Val Loss: 6.3473, Val Accuracy: 0.9038 img, 0.9993 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:46<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [15/15], Train Loss: 6.0958, Train Accuracy: 0.9851 img, 0.9991 txt\n",
      "Epoch [15/15], Val Loss: 6.3373, Val Accuracy: 0.9039 img, 0.9994 txt\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)\n",
    "weights = torch.tensor([.7,1.2,1.5,1.5,1.5,1.7, 1.5, 1.2]).to(device)\n",
    "criterion_text = nn.BCELoss(weights)\n",
    "criterion_image = nn.BCELoss(weights)\n",
    "cont_loss = contrastive_loss\n",
    "AVERAGING = 'micro'\n",
    "acc_train_img = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "acc_train_text = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "acc_val_img   = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)\n",
    "acc_val_text   = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)\n",
    "\n",
    "\n",
    "train_text = False\n",
    "EPOCHS = 15\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "\n",
    "      total_acc_train = 0\n",
    "      total_loss_train = 0\n",
    "\n",
    "      for train_image, train_text, train_label in tqdm(train_dataloader):\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          train_label = train_label.to(device)\n",
    "          train_image = train_image.to(device)\n",
    "          mask = train_text['attention_mask'].to(device)\n",
    "          input_id = train_text['input_ids'].squeeze(1).to(device)\n",
    "          \n",
    "          # logits_per_image, logits_per_text\n",
    "          out_img, out_txt, predictions_img, predictions_text = model.forward(train_image, input_id, mask, contrastive = True)\n",
    "          #predictions_img =  model.forward(train_image, input_id, mask, contrastive = False)\n",
    "          acc_train_img(predictions_img, train_label)\n",
    "          acc_train_text(predictions_text, train_label)\n",
    "\n",
    "\n",
    "          closs = cont_loss(out_img, out_txt)\n",
    "          text_loss = criterion_text(predictions_text, train_label)\n",
    "          img_loss = criterion_image(predictions_img, train_label)\n",
    "          # if(train_text):\n",
    "          #   batch_loss = closs*2. + text_loss*5. + img_loss*2.\n",
    "          # else:\n",
    "          #   batch_loss = closs + img_loss\n",
    "          batch_loss = closs*2. + text_loss*2. + img_loss*.5\n",
    "          batch_loss.backward()\n",
    "          optimizer.step()\n",
    "          total_loss_train += batch_loss.item()\n",
    "      \n",
    "      total_acc_val = 0\n",
    "      total_loss_val = 0\n",
    "\n",
    "      with torch.no_grad():\n",
    "\n",
    "          for val_image, val_text, val_label in val_dataloader:\n",
    "\n",
    "              val_label = val_label.to(device)\n",
    "              val_image = val_image.to(device)\n",
    "              mask = val_text['attention_mask'].to(device)\n",
    "              input_id = val_text['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              out_img, out_txt, predictions_img, predictions_text = model.forward(val_image, input_id, mask, contrastive = True)\n",
    "              #predictions_img =  model.forward(val_image, input_id, mask, contrastive = False)\n",
    "              acc_val_img(predictions_img, val_label)\n",
    "              acc_val_text(predictions_text, val_label)\n",
    "\n",
    "\n",
    "              closs = cont_loss(out_img, out_txt)\n",
    "              text_loss = criterion_text(predictions_text, val_label)\n",
    "              img_loss = criterion_image(predictions_img, val_label)\n",
    "              # if(train_text):\n",
    "              #   batch_loss = closs*2. + text_loss*5. + img_loss*2.\n",
    "              # else:\n",
    "              #   batch_loss = closs\n",
    "              batch_loss = closs*2. + text_loss*2. + img_loss*.5\n",
    "              total_loss_val += batch_loss.item()\n",
    "\n",
    "             # acc_val(predictions, val_label)\n",
    "              \n",
    "      \n",
    "      avg_train_loss = total_loss_train/len(train_df)\n",
    "    #   train_accuracy = total_acc_train/len(train_df)\n",
    "\n",
    "      avg_val_loss = total_loss_val/len(val_df)\n",
    "    #   val_accuracy = total_acc_val/len(dev_df)\n",
    "\n",
    "      if(acc_train_text.compute() >= 0.99):\n",
    "        print(\"Fixing Text model component!\")\n",
    "        train_text = False\n",
    "        model.txt_model.requires_grad_(False)\n",
    "        model.text_head.requires_grad_(False)\n",
    "      else:\n",
    "        print(\"Unfreezing text model component\")\n",
    "        train_text = False\n",
    "        model.txt_model.requires_grad_(True)\n",
    "        model.text_head.requires_grad_(True)\n",
    "\n",
    "      print(\"Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.4f} img, {:.4f} txt\".format(epoch_num+1, EPOCHS, avg_train_loss*BATCH_SIZE, acc_train_img.compute(), acc_train_text.compute()))\n",
    "      print(\"Epoch [{}/{}], Val Loss: {:.4f}, Val Accuracy: {:.4f} img, {:.4f} txt\".format(epoch_num+1, EPOCHS, avg_val_loss*BATCH_SIZE, acc_val_img.compute(), acc_val_text.compute()))\n",
    "\n",
    "      acc_train_img.reset()\n",
    "      acc_train_text.reset()\n",
    "      acc_val_img.reset\n",
    "      acc_val_text.reset()\n",
    "\n",
    "torch.save(model.state_dict(), './' + 'finetuned' + '.pt' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"finetuned.pt\", 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:02<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.912030\n",
      "Prec: 0.635193\n",
      "Recall: 0.696\n",
      "F1-score: 0.664\n",
      "F-Beta-score: 0.658\n",
      "Kappa: 0.000\n",
      "AUC: 0.924\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss(torch.tensor([0.5, 1, 5, 5, 5, 6, 5, 1]).float().to(device))\n",
    "\n",
    "test_loss = 0\n",
    "test_acc  = 0\n",
    "\n",
    "AVERAGING = 'micro'\n",
    "PREC = torchmetrics.classification.MultilabelPrecision(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "ACC = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "REC = torchmetrics.classification.MultilabelRecall(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "F1_SCORE = torchmetrics.classification.MultilabelF1Score(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "F_BETA_SCORE = torchmetrics.classification.MultilabelFBetaScore(beta = 0.8, num_classes = 8, num_labels = 8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "KAPPA = torchmetrics.classification.MulticlassCohenKappa(8).to(device)#, validate_args = False)\n",
    "AUC = torchmetrics.classification.MultilabelAUROC(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "\n",
    "for train_image, train_text, train_label in tqdm(test_dataloader): \n",
    "    with torch.no_grad():\n",
    "        train_label = train_label.to(device)\n",
    "        train_image = train_image.to(device)\n",
    "        mask = train_text['attention_mask'].to(device)\n",
    "        input_id = train_text['input_ids'].squeeze(1).to(device)\n",
    "        \n",
    "        # logits_per_image, logits_per_text\n",
    "        #out_img, out_txt = model.forward(train_image, input_id, mask, contrastive = True)\n",
    "        predictions = model.forward(train_image, contrastive = False)\n",
    "\n",
    "\n",
    "\n",
    "        train_label = train_label.long()\n",
    "        PREC(predictions, train_label)\n",
    "        ACC(predictions, train_label)\n",
    "        REC(predictions, train_label)\n",
    "        F1_SCORE(predictions, train_label)\n",
    "        F_BETA_SCORE(predictions, train_label)\n",
    "        KAPPA(predictions, train_label)\n",
    "        AUC(predictions, train_label)\n",
    "\n",
    "\n",
    "add_prec = PREC.compute()\n",
    "add_acc = ACC.compute()\n",
    "add_rec = REC.compute()\n",
    "add_f1 = F1_SCORE.compute()\n",
    "add_fbeta = F_BETA_SCORE.compute()\n",
    "add_kappa = KAPPA.compute()\n",
    "add_auc = AUC.compute()\n",
    "\n",
    "avg_test_loss = test_loss/len(test_df)*BATCH_SIZE\n",
    "avg_test_acc  = test_acc /len(test_df)\n",
    "\n",
    "print(\"Acc: {:3f}\\nPrec: {:3f}\\nRecall: {:.3f}\\nF1-score: {:.3f}\\nF-Beta-score: {:.3f}\\nKappa: {:.3f}\\nAUC: {:.3f}\".format(add_acc, add_prec,add_rec, add_f1, add_fbeta, add_kappa, add_auc))\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('torchnew')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aae2c75324e7fdf5ebd22146e2daffaa477f8ea149f0e685be4c317c2939a685"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
