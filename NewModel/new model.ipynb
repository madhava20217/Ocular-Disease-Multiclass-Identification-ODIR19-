{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import logging\n",
    "import gdown\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "import albumentations as A\n",
    "\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from scipy.io import loadmat\n",
    "from sklearn.manifold import TSNE\n",
    "from torchmetrics.classification import MulticlassF1Score, JaccardIndex, MulticlassPrecision, MulticlassRecall, MulticlassAveragePrecision\n",
    "import pandas as pd\n",
    "from torchinfo import torchinfo\n",
    "\n",
    "from transformers import ConvNextV2Model, BertModel, BertTokenizer, ViTModel, ViTConfig\n",
    "from transformers import AutoTokenizer, AutoModel, RobertaTokenizer, CLIPModel, CLIPTokenizer, CLIPProcessor\n",
    "from transformers import DeiTConfig, DeiTFeatureExtractor, DeiTImageProcessor, DeiTModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    \n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '../Datasets/ocular-disease-recognition-odir5k/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = ROOT_DIR + 'dataset_single_eye.csv'\n",
    "TEST_CSV = ROOT_DIR + 'TESTING_dataset_single_eye.csv'\n",
    "IMG_PATH = ROOT_DIR + 'preprocessed_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.io.read_image(IMG_PATH + '0_left.jpg').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df = pd.read_csv(CSV_PATH)\n",
    "test_df = pd.read_csv(TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df:pd.DataFrame):\n",
    "    df['Keywords'] = df['Keywords'].str.lower()\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: \" \".join(x.split()))\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
    "    return df\n",
    "train_val_df = preprocess_text(train_val_df)\n",
    "test_df = preprocess_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_val_df['Keywords'].apply(lambda x: len(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4877, 861)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(train_val_df, test_size = 0.15)\n",
    "len(train_df), len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "rescale_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(IMG_SIZE, antialias = True),\n",
    "    torchvision.transforms.Normalize(\n",
    "        timm.data.constants.IMAGENET_DEFAULT_MEAN,\n",
    "        timm.data.constants.IMAGENET_DEFAULT_STD\n",
    "    )\n",
    "])\n",
    "\n",
    "augmentation = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomAutocontrast(0.25),\n",
    "    torchvision.transforms.RandomAdjustSharpness(0.8, p = 0.5),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p = 0.5),\n",
    "    torchvision.transforms.RandomVerticalFlip(p= 0.5),\n",
    "    torchvision.transforms.RandomRotation(20)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'moderate non proliferative retinopathy'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_df['Keywords'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\krish/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "processor = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODIRDatasetMM(Dataset) :\n",
    "    def __init__(self, df, IMG_FOLDER, tokenizer = processor, feature_extractor = rescale_transform, augmentation = None) :\n",
    "        '''\n",
    "        id : list of samples ids as string\n",
    "        '''\n",
    "        self.text = [tokenizer(text = x, padding = 'max_length', max_length = 45, truncation = True, return_tensors = 'pt') for x in df['Keywords']]\n",
    "        self.eye = df['eye']\n",
    "        self.labels = torch.tensor(df[['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']].to_numpy()).float()\n",
    "        self.img_dir = [IMG_PATH + x for x in df['Image']]\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "        self.images = [feature_extractor(torchvision.io.read_image(x).float()/255.0) for x in self.img_dir]\n",
    "        #self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        batch_imgs = self.images[idx]\n",
    "        if(self.augmentation is not None):\n",
    "            batch_imgs = self.augmentation(batch_imgs)\n",
    "        return batch_imgs, self.text[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ODIRDatasetMM(train_df, IMG_PATH, augmentation = augmentation)\n",
    "val_dataset   = ODIRDatasetMM(val_df, IMG_PATH)\n",
    "test_dataset  = ODIRDatasetMM(test_df, IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size= BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrastive learning on training data finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare two models: BERT vs ConvNext, try to compute contrastive losses\n",
    "class ContrastiveLearning(nn.Module):\n",
    "    def __init__(self, drop_prob = 0.4):\n",
    "        super().__init__()\n",
    "        self.img_model = torchvision.models.vit_b_32(weights = torchvision.models.ViT_B_32_Weights.DEFAULT)#torch.hub.load('facebookresearch/deit:main', 'deit_large_patch16_224', pretrained=True) #output 768 features\n",
    "        self.img_model.heads = nn.Identity()                                             \n",
    "        \n",
    "        self.txt_model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'roberta-base')         #output 768 features\n",
    "        \n",
    "        # image model classification head\n",
    "        self.fc1 = nn.Linear(768, 768)\n",
    "        self.fc2 = nn.Linear(768, 768)\n",
    "        self.img_head = nn.Linear(768, 8)\n",
    "        \n",
    "        # text model classification ehad\n",
    "        self.fc3 = nn.Linear(768, 768)\n",
    "        self.fc4 = nn.Linear(768, 768)\n",
    "        self.text_head = nn.Linear(768, 8)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, img_input = None, input_ids = None, attn_mask = None, contrastive = False):#, text_class = False):\n",
    "        if(contrastive):\n",
    "            # pretraining\n",
    "            out_txt = self.txt_model(input_ids, attn_mask)['pooler_output']\n",
    "            out_img = self.img_model(img_input)\n",
    "\n",
    "            out_txt_ret = F.normalize(out_txt, p = 2.0, dim = 1)\n",
    "            out_img_ret = F.normalize(out_txt, p = 2.0, dim = 1)\n",
    "\n",
    "            #img path\n",
    "            resi_img = out_img\n",
    "            img_route_out = self.dropout_layer(out_img)\n",
    "            img_route_out = F.relu(self.fc1(img_route_out))\n",
    "            img_route_out = self.dropout_layer(img_route_out)\n",
    "            img_route_out = F.relu(self.fc2(img_route_out))\n",
    "            img_route_out = img_route_out + resi_img\n",
    "\n",
    "            img_route_out = F.sigmoid(self.img_head(img_route_out))\n",
    "\n",
    "            #text path\n",
    "            resi_txt = out_txt\n",
    "            txt_route_out = self.dropout_layer(out_txt)\n",
    "            txt_route_out = F.relu(self.fc3(txt_route_out))\n",
    "            txt_route_out = self.dropout_layer(txt_route_out)\n",
    "            txt_route_out = F.relu(self.fc4(txt_route_out))\n",
    "            txt_route_out = txt_route_out + resi_txt\n",
    "\n",
    "            txt_route_out = F.sigmoid(self.text_head(txt_route_out))\n",
    "            return out_img_ret, out_txt_ret, img_route_out, txt_route_out\n",
    "\n",
    "            #return out_img, out_txt\n",
    "        else:\n",
    "            #if(not text_class):\n",
    "            out = self.img_model(img_input)\n",
    "            resi = out\n",
    "            out = self.dropout_layer(out)\n",
    "            out = F.relu(self.fc1(out))\n",
    "            out = out + resi\n",
    "\n",
    "            out = F.sigmoid(self.img_head(out))\n",
    "            return out\n",
    "            # else:\n",
    "            #     out = self.txt_model(input_ids, attn_mask)['pooler_output']\n",
    "            #     resi = out\n",
    "            #     out = self.dropout_layer(out)\n",
    "            #     out = F.relu(self.fc2(out))\n",
    "            #     out = resi + out\n",
    "\n",
    "            #     out = F.sigmoid(self.text_head(out))\n",
    "            #     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(ten1, ten2, temperature = nn.Parameter(torch.tensor(.25).to(device))):    #...\n",
    "    #steps = hadamard product\n",
    "    # trivial for loop \n",
    "    sim = torch.einsum('i d, j d -> i j', ten1, ten2) * temperature.exp()\n",
    "    labels = torch.arange(ten1.size(0), device = device)\n",
    "    loss = (F.cross_entropy(sim, labels) + F.cross_entropy(sim.t(), labels)) / 2\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\krish/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = ContrastiveLearning().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:31<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfreezing text model component\n",
      "Epoch [1/15], Train Loss: 5.1866, Train Accuracy: 0.8761 img, 0.9890 txt\n",
      "Epoch [1/15], Val Loss: 4.5529, Val Accuracy: 0.8886 img, 0.9996 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:30<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [2/15], Train Loss: 4.4667, Train Accuracy: 0.8841 img, 0.9999 txt\n",
      "Epoch [2/15], Val Loss: 4.4295, Val Accuracy: 0.8902 img, 0.9996 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:12<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [3/15], Train Loss: 4.3877, Train Accuracy: 0.8896 img, 1.0000 txt\n",
      "Epoch [3/15], Val Loss: 4.4139, Val Accuracy: 0.8904 img, 0.9996 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:12<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [4/15], Train Loss: 4.3850, Train Accuracy: 0.8897 img, 1.0000 txt\n",
      "Epoch [4/15], Val Loss: 4.4012, Val Accuracy: 0.8918 img, 0.9997 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:12<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [5/15], Train Loss: 4.3600, Train Accuracy: 0.8935 img, 1.0000 txt\n",
      "Epoch [5/15], Val Loss: 4.3870, Val Accuracy: 0.8931 img, 0.9996 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:12<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [6/15], Train Loss: 4.3505, Train Accuracy: 0.8951 img, 1.0000 txt\n",
      "Epoch [6/15], Val Loss: 4.3800, Val Accuracy: 0.8940 img, 0.9997 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:12<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [7/15], Train Loss: 4.3469, Train Accuracy: 0.8942 img, 1.0000 txt\n",
      "Epoch [7/15], Val Loss: 4.3959, Val Accuracy: 0.8941 img, 0.9997 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:12<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [8/15], Train Loss: 4.3446, Train Accuracy: 0.8976 img, 1.0000 txt\n",
      "Epoch [8/15], Val Loss: 4.4125, Val Accuracy: 0.8924 img, 0.9997 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:12<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [9/15], Train Loss: 4.3227, Train Accuracy: 0.8991 img, 1.0000 txt\n",
      "Epoch [9/15], Val Loss: 4.3933, Val Accuracy: 0.8931 img, 0.9997 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:12<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [10/15], Train Loss: 4.3194, Train Accuracy: 0.9017 img, 1.0000 txt\n",
      "Epoch [10/15], Val Loss: 4.3706, Val Accuracy: 0.8937 img, 0.9997 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:12<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [11/15], Train Loss: 4.3179, Train Accuracy: 0.9020 img, 1.0000 txt\n",
      "Epoch [11/15], Val Loss: 4.3803, Val Accuracy: 0.8939 img, 0.9997 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:12<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [12/15], Train Loss: 4.3098, Train Accuracy: 0.9021 img, 1.0000 txt\n",
      "Epoch [12/15], Val Loss: 4.3643, Val Accuracy: 0.8945 img, 0.9997 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:11<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [13/15], Train Loss: 4.3026, Train Accuracy: 0.9037 img, 1.0000 txt\n",
      "Epoch [13/15], Val Loss: 4.3658, Val Accuracy: 0.8949 img, 0.9997 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:11<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [14/15], Train Loss: 4.2967, Train Accuracy: 0.9040 img, 1.0000 txt\n",
      "Epoch [14/15], Val Loss: 4.3624, Val Accuracy: 0.8955 img, 0.9997 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:11<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Text model component!\n",
      "Epoch [15/15], Train Loss: 4.2825, Train Accuracy: 0.9079 img, 1.0000 txt\n",
      "Epoch [15/15], Val Loss: 4.3569, Val Accuracy: 0.8958 img, 0.9997 txt\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-5)\n",
    "criterion = nn.BCELoss(torch.tensor([1,1.2,1.5,1.5,1.5,1.5, 1.5, 1.2]).to(device))\n",
    "criterion_text = nn.BCELoss(weights)\n",
    "criterion_image = nn.BCELoss(weights)\n",
    "cont_loss = contrastive_loss\n",
    "AVERAGING = 'micro'\n",
    "acc_train_img = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "acc_train_text = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "acc_val_img   = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)\n",
    "acc_val_text   = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)\n",
    "\n",
    "\n",
    "train_text = False\n",
    "EPOCHS = 15\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "\n",
    "      total_acc_train = 0\n",
    "      total_loss_train = 0\n",
    "\n",
    "      for train_image, train_text, train_label in tqdm(train_dataloader):\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          train_label = train_label.to(device)\n",
    "          train_image = train_image.to(device)\n",
    "          mask = train_text['attention_mask'].to(device)\n",
    "          input_id = train_text['input_ids'].squeeze(1).to(device)\n",
    "          \n",
    "          # logits_per_image, logits_per_text\n",
    "          out_img, out_txt, predictions_img, predictions_text = model.forward(train_image, input_id, mask, contrastive = True)\n",
    "          acc_train_img(predictions_img, train_label)\n",
    "          acc_train_text(predictions_text, train_label)\n",
    "\n",
    "\n",
    "          closs = cont_loss(out_img, out_txt)\n",
    "          text_loss = criterion_text(predictions_text, train_label)\n",
    "          img_loss = criterion_image(predictions_img, train_label)\n",
    "          if(train_text):\n",
    "            batch_loss = closs*2. + text_loss*5. + img_loss*2.\n",
    "          else:\n",
    "            batch_loss = closs + img_loss\n",
    "          batch_loss.backward()\n",
    "          optimizer.step()\n",
    "          total_loss_train += batch_loss.item()\n",
    "      \n",
    "      total_acc_val = 0\n",
    "      total_loss_val = 0\n",
    "\n",
    "      with torch.no_grad():\n",
    "\n",
    "          for val_image, val_text, val_label in val_dataloader:\n",
    "\n",
    "              val_label = val_label.to(device)\n",
    "              val_image = val_image.to(device)\n",
    "              mask = val_text['attention_mask'].to(device)\n",
    "              input_id = val_text['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              out_img, out_txt, predictions_img, predictions_text = model.forward(val_image, input_id, mask, contrastive = True)\n",
    "\n",
    "              acc_val_img(predictions_img, val_label)\n",
    "              acc_val_text(predictions_text, val_label)\n",
    "\n",
    "\n",
    "              closs = cont_loss(out_img, out_txt)\n",
    "              text_loss = criterion_text(predictions_text, val_label)\n",
    "              img_loss = criterion_image(predictions_img, val_label)\n",
    "              if(train_text):\n",
    "                batch_loss = closs*2. + text_loss*5. + img_loss*2.\n",
    "              else:\n",
    "                batch_loss = closs\n",
    "              total_loss_val += batch_loss.item()\n",
    "\n",
    "             # acc_val(predictions, val_label)\n",
    "              \n",
    "      \n",
    "      avg_train_loss = total_loss_train/len(train_df)\n",
    "    #   train_accuracy = total_acc_train/len(train_df)\n",
    "\n",
    "      avg_val_loss = total_loss_val/len(val_df)\n",
    "    #   val_accuracy = total_acc_val/len(dev_df)\n",
    "\n",
    "      if(acc_train_text.compute() >= 0.99):\n",
    "        print(\"Fixing Text model component!\")\n",
    "        train_text = False\n",
    "        model.txt_model.requires_grad_(False)\n",
    "        model.text_head.requires_grad_(False)\n",
    "      else:\n",
    "        print(\"Unfreezing text model component\")\n",
    "        train_text = False\n",
    "        model.txt_model.requires_grad_(True)\n",
    "        model.text_head.requires_grad_(True)\n",
    "\n",
    "      print(\"Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.4f} img, {:.4f} txt\".format(epoch_num+1, EPOCHS, avg_train_loss*BATCH_SIZE, acc_train_img.compute(), acc_train_text.compute()))\n",
    "      print(\"Epoch [{}/{}], Val Loss: {:.4f}, Val Accuracy: {:.4f} img, {:.4f} txt\".format(epoch_num+1, EPOCHS, avg_val_loss*BATCH_SIZE, acc_val_img.compute(), acc_val_text.compute()))\n",
    "\n",
    "      acc_train_img.reset()\n",
    "      acc_train_text.reset()\n",
    "      acc_val_img.reset\n",
    "      acc_val_text.reset()\n",
    "\n",
    "torch.save(model.state_dict(), './' + 'finetuned' + '.pt' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"finetuned.pt\", 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:02<00:00, 17.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.894788\n",
      "Prec: 0.576865\n",
      "Recall: 0.594\n",
      "F1-score: 0.585\n",
      "F-Beta-score: 0.583\n",
      "Kappa: 0.000\n",
      "AUC: 0.917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss(torch.tensor([0.5, 1, 5, 5, 5, 6, 5, 1]).float().to(device))\n",
    "\n",
    "test_loss = 0\n",
    "test_acc  = 0\n",
    "\n",
    "AVERAGING = 'micro'\n",
    "PREC = torchmetrics.classification.MultilabelPrecision(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "ACC = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "REC = torchmetrics.classification.MultilabelRecall(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "F1_SCORE = torchmetrics.classification.MultilabelF1Score(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "F_BETA_SCORE = torchmetrics.classification.MultilabelFBetaScore(beta = 0.8, num_classes = 8, num_labels = 8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "KAPPA = torchmetrics.classification.MulticlassCohenKappa(8).to(device)#, validate_args = False)\n",
    "AUC = torchmetrics.classification.MultilabelAUROC(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "\n",
    "for train_image, train_text, train_label in tqdm(test_dataloader): \n",
    "    with torch.no_grad():\n",
    "        train_label = train_label.to(device)\n",
    "        train_image = train_image.to(device)\n",
    "        mask = train_text['attention_mask'].to(device)\n",
    "        input_id = train_text['input_ids'].squeeze(1).to(device)\n",
    "        \n",
    "        # logits_per_image, logits_per_text\n",
    "        #out_img, out_txt = model.forward(train_image, input_id, mask, contrastive = True)\n",
    "        predictions = model.forward(train_image, contrastive = False)\n",
    "\n",
    "\n",
    "\n",
    "        train_label = train_label.long()\n",
    "        PREC(predictions, train_label)\n",
    "        ACC(predictions, train_label)\n",
    "        REC(predictions, train_label)\n",
    "        F1_SCORE(predictions, train_label)\n",
    "        F_BETA_SCORE(predictions, train_label)\n",
    "        KAPPA(predictions, train_label)\n",
    "        AUC(predictions, train_label)\n",
    "\n",
    "\n",
    "add_prec = PREC.compute()\n",
    "add_acc = ACC.compute()\n",
    "add_rec = REC.compute()\n",
    "add_f1 = F1_SCORE.compute()\n",
    "add_fbeta = F_BETA_SCORE.compute()\n",
    "add_kappa = KAPPA.compute()\n",
    "add_auc = AUC.compute()\n",
    "\n",
    "avg_test_loss = test_loss/len(test_df)*BATCH_SIZE\n",
    "avg_test_acc  = test_acc /len(test_df)\n",
    "\n",
    "print(\"Acc: {:3f}\\nPrec: {:3f}\\nRecall: {:.3f}\\nF1-score: {:.3f}\\nF-Beta-score: {:.3f}\\nKappa: {:.3f}\\nAUC: {:.3f}\".format(add_acc, add_prec,add_rec, add_f1, add_fbeta, add_kappa, add_auc))\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('torchnew')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aae2c75324e7fdf5ebd22146e2daffaa477f8ea149f0e685be4c317c2939a685"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
