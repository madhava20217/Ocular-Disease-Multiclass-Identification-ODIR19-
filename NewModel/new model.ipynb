{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import logging\n",
    "import gdown\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "import albumentations as A\n",
    "\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from scipy.io import loadmat\n",
    "from sklearn.manifold import TSNE\n",
    "from torchmetrics.classification import MulticlassF1Score, JaccardIndex, MulticlassPrecision, MulticlassRecall, MulticlassAveragePrecision\n",
    "import pandas as pd\n",
    "from torchinfo import torchinfo\n",
    "\n",
    "from transformers import ConvNextV2Model, BertModel, BertTokenizer, ViTModel, ViTConfig\n",
    "from transformers import AutoTokenizer, AutoModel, RobertaTokenizer, CLIPModel, CLIPTokenizer, CLIPProcessor\n",
    "from transformers import DeiTConfig, DeiTFeatureExtractor, DeiTImageProcessor, DeiTModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    \n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '../Datasets/ocular-disease-recognition-odir5k/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = ROOT_DIR + 'dataset_single_eye.csv'\n",
    "TEST_CSV = ROOT_DIR + 'TESTING_dataset_single_eye.csv'\n",
    "IMG_PATH = ROOT_DIR + 'preprocessed_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.io.read_image(IMG_PATH + '0_left.jpg').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df = pd.read_csv(CSV_PATH)\n",
    "test_df = pd.read_csv(TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df:pd.DataFrame):\n",
    "    df['Keywords'] = df['Keywords'].str.lower()\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: \" \".join(x.split()))\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
    "    return df\n",
    "train_val_df = preprocess_text(train_val_df)\n",
    "test_df = preprocess_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_val_df['Keywords'].apply(lambda x: len(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4877, 861)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(train_val_df, test_size = 0.15)\n",
    "len(train_df), len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (299, 299)\n",
    "\n",
    "rescale_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(IMG_SIZE, antialias = True, interpolation = 3),\n",
    "    torchvision.transforms.Normalize(\n",
    "        timm.data.constants.IMAGENET_DEFAULT_MEAN,\n",
    "        timm.data.constants.IMAGENET_DEFAULT_STD\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'moderate non proliferative retinopathy'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_df['Keywords'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\krish/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "processor = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODIRDatasetMM(Dataset) :\n",
    "    def __init__(self, df, IMG_FOLDER, tokenizer = processor, feature_extractor = rescale_transform) :\n",
    "        '''\n",
    "        id : list of samples ids as string\n",
    "        '''\n",
    "        self.text = [tokenizer(text = x, padding = 'max_length', max_length = 45, truncation = True, return_tensors = 'pt') for x in df['Keywords']]\n",
    "        self.eye = df['eye']\n",
    "        self.labels = torch.tensor(df[['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']].to_numpy()).float()\n",
    "        self.img_dir = [IMG_PATH + x for x in df['Image']]\n",
    "\n",
    "        self.images = [feature_extractor(torchvision.io.read_image(x).float()) for x in self.img_dir]\n",
    "        #self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.text[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ODIRDatasetMM(train_df, IMG_PATH)\n",
    "val_dataset   = ODIRDatasetMM(val_df, IMG_PATH)\n",
    "test_dataset  = ODIRDatasetMM(test_df, IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size= BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = torchvision.models.efficientnet_b0(weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT).to(device)\n",
    "# torchinfo.summary(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrastive learning on training data finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare two models: BERT vs ConvNext, try to compute contrastive losses\n",
    "class ContrastiveLearning(nn.Module):\n",
    "    def __init__(self, drop_prob = 0.4):\n",
    "        super().__init__()\n",
    "        self.img_model = torchvision.models.efficientnet_b0(weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT) #torch.hub.load('facebookresearch/deit:main', 'deit_large_patch16_224', pretrained=True) #output 768 features\n",
    "        self.img_model.classifier = nn.Sequential(\n",
    "            nn.Linear(1280, 768)\n",
    "        )                                                   \n",
    "        \n",
    "        self.txt_model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'roberta-base')         #output 768 features\n",
    "        \n",
    "        # image model classification head\n",
    "        self.fc1 = nn.Linear(768, 768)\n",
    "        self.fc2 = nn.Linear(768, 768)\n",
    "        self.img_head = nn.Linear(768, 8)\n",
    "        \n",
    "        # text model classification ehad\n",
    "        self.fc3 = nn.Linear(768, 768)\n",
    "        self.fc4 = nn.Linear(768, 768)\n",
    "        self.text_head = nn.Linear(768, 8)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, img_input = None, input_ids = None, attn_mask = None, contrastive = False):#, text_class = False):\n",
    "        if(contrastive):\n",
    "            # pretraining\n",
    "            out_txt = self.txt_model(input_ids, attn_mask)['pooler_output']\n",
    "            out_img = self.img_model(img_input)\n",
    "\n",
    "            out_txt_ret = F.normalize(out_txt, p = 2.0, dim = 1)\n",
    "            out_img_ret = F.normalize(out_txt, p = 2.0, dim = 1)\n",
    "\n",
    "            #img path\n",
    "            resi_img = out_img\n",
    "            img_route_out = self.dropout_layer(out_img)\n",
    "            img_route_out = F.relu(self.fc1(img_route_out))\n",
    "            img_route_out = self.dropout_layer(img_route_out)\n",
    "            img_route_out = F.relu(self.fc2(img_route_out))\n",
    "            img_route_out = img_route_out + resi_img\n",
    "\n",
    "            img_route_out = F.sigmoid(self.img_head(out_img))\n",
    "\n",
    "            #text path\n",
    "            resi_txt = out_txt\n",
    "            txt_route_out = self.dropout_layer(out_txt)\n",
    "            txt_route_out = F.relu(self.fc3(txt_route_out))\n",
    "            txt_route_out = self.dropout_layer(txt_route_out)\n",
    "            txt_route_out = F.relu(self.fc4(txt_route_out))\n",
    "            txt_route_out = txt_route_out + resi_txt\n",
    "\n",
    "            txt_route_out = F.sigmoid(self.text_head(out_txt))\n",
    "            return out_img_ret, out_txt_ret, img_route_out, txt_route_out\n",
    "\n",
    "            #return out_img, out_txt\n",
    "        else:\n",
    "            #if(not text_class):\n",
    "            out = self.img_model(img_input)\n",
    "            resi = out\n",
    "            out = self.dropout_layer(out)\n",
    "            out = F.relu(self.fc1(out))\n",
    "            out = out + resi\n",
    "\n",
    "            out = F.sigmoid(self.img_head(out))\n",
    "            return out\n",
    "            # else:\n",
    "            #     out = self.txt_model(input_ids, attn_mask)['pooler_output']\n",
    "            #     resi = out\n",
    "            #     out = self.dropout_layer(out)\n",
    "            #     out = F.relu(self.fc2(out))\n",
    "            #     out = resi + out\n",
    "\n",
    "            #     out = F.sigmoid(self.text_head(out))\n",
    "            #     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(ten1, ten2, temperature = nn.Parameter(torch.tensor(.25).to(device))):    #...\n",
    "    #steps = hadamard product\n",
    "    # trivial for loop \n",
    "    sim = torch.einsum('i d, j d -> i j', ten1, ten2) * temperature.exp()\n",
    "    labels = torch.arange(ten1.size(0), device = device)\n",
    "    loss = (F.cross_entropy(sim, labels) + F.cross_entropy(sim.t(), labels)) / 2\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ContrastiveLearning().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:04<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Train Loss: 4.1680, Train Accuracy: 0.8909 img, 0.9698 txt\n",
      "Epoch [1/25], Val Loss: 1.9324, Val Accuracy: 0.9059 img, 0.9914 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:10<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], Train Loss: 3.9850, Train Accuracy: 0.9163 img, 0.9955 txt\n",
      "Epoch [2/25], Val Loss: 1.9174, Val Accuracy: 0.9097 img, 0.9955 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:10<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], Train Loss: 4.2722, Train Accuracy: 0.9308 img, 0.9713 txt\n",
      "Epoch [3/25], Val Loss: 2.5390, Val Accuracy: 0.9116 img, 0.8876 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:11<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], Train Loss: 4.8987, Train Accuracy: 0.9450 img, 0.8994 txt\n",
      "Epoch [4/25], Val Loss: 2.2408, Val Accuracy: 0.9119 img, 0.9097 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:11<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25], Train Loss: 4.6718, Train Accuracy: 0.9606 img, 0.9063 txt\n",
      "Epoch [5/25], Val Loss: 2.2397, Val Accuracy: 0.9115 img, 0.9097 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 72/305 [00:16<00:54,  4.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m input_id \u001b[38;5;241m=\u001b[39m train_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# logits_per_image, logits_per_text\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m out_img, out_txt, predictions_img, predictions_text \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(train_image, input_id, mask, contrastive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m acc_train_img(predictions_img, train_label)\n\u001b[0;32m     30\u001b[0m acc_train_text(predictions_text, train_label)\n",
      "Cell \u001b[1;32mIn[78], line 28\u001b[0m, in \u001b[0;36mContrastiveLearning.forward\u001b[1;34m(self, img_input, input_ids, attn_mask, contrastive)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(contrastive):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# pretraining\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     out_txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtxt_model(input_ids, attn_mask)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpooler_output\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 28\u001b[0m     out_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_model(img_input)\n\u001b[0;32m     30\u001b[0m     out_txt_ret \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(out_txt, p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     31\u001b[0m     out_img_ret \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(out_txt, p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torchvision\\models\\efficientnet.py:343\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 343\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_impl(x)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torchvision\\models\\efficientnet.py:333\u001b[0m, in \u001b[0;36mEfficientNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_impl\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 333\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures(x)\n\u001b[0;32m    335\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[0;32m    336\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torchvision\\models\\efficientnet.py:166\u001b[0m, in \u001b[0;36mMBConv.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock(\u001b[39minput\u001b[39m)\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_res_connect:\n\u001b[1;32m--> 166\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstochastic_depth(result)\n\u001b[0;32m    167\u001b[0m     result \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m    168\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torchvision\\ops\\stochastic_depth.py:62\u001b[0m, in \u001b[0;36mStochasticDepth.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 62\u001b[0m     \u001b[39mreturn\u001b[39;00m stochastic_depth(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torchvision\\ops\\stochastic_depth.py:41\u001b[0m, in \u001b[0;36mstochastic_depth\u001b[1;34m(input, p, mode, training)\u001b[0m\n\u001b[0;32m     39\u001b[0m     size \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mndim\n\u001b[0;32m     40\u001b[0m noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mempty(size, dtype\u001b[39m=\u001b[39m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> 41\u001b[0m noise \u001b[39m=\u001b[39m noise\u001b[39m.\u001b[39mbernoulli_(survival_rate)\n\u001b[0;32m     42\u001b[0m \u001b[39mif\u001b[39;00m survival_rate \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m     43\u001b[0m     noise\u001b[39m.\u001b[39mdiv_(survival_rate)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
    "weights = torch.tensor([1,1.2,1.5,1.5,1.5,1.5, 1.5, 1.2]).to(device)\n",
    "criterion_text = nn.BCELoss(weights)\n",
    "criterion_image = nn.BCELoss(weights)\n",
    "cont_loss = contrastive_loss\n",
    "AVERAGING = 'micro'\n",
    "acc_train_img = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "acc_train_text = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "acc_val_img   = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)\n",
    "acc_val_text   = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)\n",
    "\n",
    "EPOCHS = 25\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "\n",
    "      total_acc_train = 0\n",
    "      total_loss_train = 0\n",
    "\n",
    "      for train_image, train_text, train_label in tqdm(train_dataloader):\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          train_label = train_label.to(device)\n",
    "          train_image = train_image.to(device)\n",
    "          mask = train_text['attention_mask'].to(device)\n",
    "          input_id = train_text['input_ids'].squeeze(1).to(device)\n",
    "          \n",
    "          # logits_per_image, logits_per_text\n",
    "          out_img, out_txt, predictions_img, predictions_text = model.forward(train_image, input_id, mask, contrastive = True)\n",
    "          acc_train_img(predictions_img, train_label)\n",
    "          acc_train_text(predictions_text, train_label)\n",
    "\n",
    "\n",
    "          closs = cont_loss(out_img, out_txt)\n",
    "          text_loss = criterion_text(predictions_text, train_label)\n",
    "          img_loss = criterion_image(predictions_img, train_label)\n",
    "          batch_loss = closs*2. + text_loss*0.5 + img_loss*.15\n",
    "          batch_loss.backward()\n",
    "          optimizer.step()\n",
    "          total_loss_train += batch_loss.item()\n",
    "      \n",
    "      total_acc_val = 0\n",
    "      total_loss_val = 0\n",
    "\n",
    "      with torch.no_grad():\n",
    "\n",
    "          for val_image, val_text, val_label in val_dataloader:\n",
    "\n",
    "              val_label = val_label.to(device)\n",
    "              val_image = val_image.to(device)\n",
    "              mask = val_text['attention_mask'].to(device)\n",
    "              input_id = val_text['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              out_img, out_txt, predictions_img, predictions_text = model.forward(val_image, input_id, mask, contrastive = True)\n",
    "\n",
    "              acc_val_img(predictions_img, val_label)\n",
    "              acc_val_text(predictions_text, val_label)\n",
    "\n",
    "\n",
    "              closs = cont_loss(out_img, out_txt)\n",
    "              text_loss = criterion_text(predictions_text, val_label)\n",
    "              img_loss = criterion_image(predictions_img, val_label)\n",
    "              batch_loss = closs#closs + l3\n",
    "              total_loss_val += batch_loss.item()\n",
    "\n",
    "             # acc_val(predictions, val_label)\n",
    "              \n",
    "      \n",
    "      avg_train_loss = total_loss_train/len(train_df)\n",
    "    #   train_accuracy = total_acc_train/len(train_df)\n",
    "\n",
    "      avg_val_loss = total_loss_val/len(val_df)\n",
    "    #   val_accuracy = total_acc_val/len(dev_df)\n",
    "\n",
    "      print(\"Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.4f} img, {:.4f} txt\".format(epoch_num+1, EPOCHS, avg_train_loss*BATCH_SIZE, acc_train_img.compute(), acc_train_text.compute()))\n",
    "      print(\"Epoch [{}/{}], Val Loss: {:.4f}, Val Accuracy: {:.4f} img, {:.4f} txt\".format(epoch_num+1, EPOCHS, avg_val_loss*BATCH_SIZE, acc_val_img.compute(), acc_val_text.compute()))\n",
    "\n",
    "      acc_train_img.reset()\n",
    "      acc_train_text.reset()\n",
    "      acc_val_img.reset\n",
    "      acc_val_text.reset()\n",
    "\n",
    "      if(epoch_num%5 == 0):\n",
    "        torch.save(model.state_dict(), './' + str(epoch_num+21) + '.pt' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"27.pt\", 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr = 0.00001)\n",
    "\n",
    "# loss_img = nn.CrossEntropyLoss()\n",
    "# loss_text = nn.CrossEntropyLoss()\n",
    "\n",
    "# cont_loss = contrastive_loss\n",
    "# criterion = nn.BCELoss()#torch.tensor([0.5, 1, 5, 5, 5, 6, 5, 1]).float().to(device))\n",
    "# AVERAGING = 'micro'\n",
    "# acc_train = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "# acc_val   = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)\n",
    "\n",
    "# EPOCHS = 10\n",
    "\n",
    "# for epoch_num in range(EPOCHS):\n",
    "\n",
    "#       total_acc_train = 0\n",
    "#       total_loss_train = 0\n",
    "\n",
    "#       for train_image, train_text, train_label in tqdm(train_dataloader):\n",
    "#           optimizer.zero_grad()\n",
    "\n",
    "#           train_label = train_label.to(device)\n",
    "#           train_image = train_image.to(device)\n",
    "#           mask = train_text['attention_mask'].to(device)\n",
    "#           input_id = train_text['input_ids'].squeeze(1).to(device)\n",
    "          \n",
    "#           # logits_per_image, logits_per_text\n",
    "#           #out_img, out_txt = model.forward(train_image, input_id, mask, contrastive = True)\n",
    "#           predictions = model.forward(train_image, contrastive = False)\n",
    "\n",
    "#           acc_train(predictions, train_label)\n",
    "\n",
    "#           #ground_truth = torch.arange(len(train_image),dtype=torch.long,device=device)\n",
    "\n",
    "#           #closs = cont_loss(out_img, out_txt)\n",
    "#           l3 = criterion(predictions, train_label)\n",
    "#           batch_loss = l3 #closs + l3\n",
    "#           batch_loss.backward()\n",
    "#           optimizer.step()\n",
    "#           total_loss_train += batch_loss.item()\n",
    "          \n",
    "#         #   acc = (output['logits'].argmax(dim=1) == train_label).sum().item()\n",
    "#         #   total_acc_train += acc\n",
    "      \n",
    "#       total_acc_val = 0\n",
    "#       total_loss_val = 0\n",
    "\n",
    "#       with torch.no_grad():\n",
    "\n",
    "#           for val_image, val_text, val_label in val_dataloader:\n",
    "\n",
    "#               val_label = val_label.to(device)\n",
    "#               val_image = val_image.to(device)\n",
    "#               mask = val_text['attention_mask'].to(device)\n",
    "#               input_id = val_text['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "#               #out_img, out_txt = model.forward(val_image, input_id, mask, contrastive = True)\n",
    "#               predictions = model.forward(val_image, contrastive = False)\n",
    "#               #ground_truth = torch.arange(len(val_image),dtype=torch.long,device=device)\n",
    "\n",
    "#               #closs = cont_loss(out_img, out_txt)\n",
    "#               l3 = criterion(predictions, val_label)\n",
    "#               batch_loss = l3\n",
    "#               total_loss_val += batch_loss.item()\n",
    "\n",
    "#               acc_val(predictions, val_label)\n",
    "              \n",
    "      \n",
    "#       avg_train_loss = total_loss_train/len(train_df)\n",
    "#     #   train_accuracy = total_acc_train/len(train_df)\n",
    "\n",
    "#       avg_val_loss = total_loss_val/len(val_df)\n",
    "#     #   val_accuracy = total_acc_val/len(dev_df)\n",
    "\n",
    "#       print(f\"Epoch [{epoch_num+1}/{EPOCHS}], \"f\"Train Loss: {avg_train_loss*BATCH_SIZE:.4f}, \"f\"Train Accuracy: {acc_train.compute():.4f}\")\n",
    "#       print(f\"Epoch [{epoch_num+1}/{EPOCHS}], \"f\"Val Loss: {avg_val_loss*BATCH_SIZE:.4f}, \"f\"Val Accuracy: {acc_val.compute():.4f}\")\n",
    "#       print('-'*60)\n",
    "\n",
    "#       acc_train.reset()\n",
    "#       acc_val.reset()\n",
    "\n",
    "      \n",
    "#       if(epoch_num%5 == 0):\n",
    "#         torch.save(model.state_dict(), './' + 'finetune' + str(epoch_num+21) + '.pt' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"27.pt\", 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "# criterion = nn.BCELoss(torch.tensor([1, 1, 5, 5, 5, 6, 5, 1]))\n",
    "# test_loss = 0\n",
    "# test_acc  = 0\n",
    "# AVERAGING = AVERAGING'\n",
    "# acc_train = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "# acc_val   = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)\n",
    "\n",
    "# EPOCHS = 5\n",
    "\n",
    "# for epoch_num in range(EPOCHS):\n",
    "#       total_loss_train = 0\n",
    "\n",
    "#       for train_image, train_text, train_label in tqdm(train_dataloader):\n",
    "#           optimizer.zero_grad()\n",
    "\n",
    "#           train_label = train_label.to(device)\n",
    "#           train_image = train_image.to(device)\n",
    "#           mask = train_text['attention_mask'].to(device)\n",
    "#           input_id = train_text['input_ids'].squeeze(1).to(device)\n",
    "          \n",
    "#           # logits_per_image, logits_per_text\n",
    "#           out_img = model.forward(train_image, contrastive = False)\n",
    "#           #print(out_img.shape, train_label.shape)\n",
    "#           batch_loss = criterion(out_img, train_label)\n",
    "#           batch_loss.backward()\n",
    "#           optimizer.step()\n",
    "#           total_loss_train += batch_loss.item()\n",
    "          \n",
    "#           acc_train(out_img, train_label)\n",
    "#         #   acc = (output['logits'].argmax(dim=1) == train_label).sum().item()\n",
    "#         #   total_acc_train += acc\n",
    "      \n",
    "#       total_acc_val = 0\n",
    "#       total_loss_val = 0\n",
    "\n",
    "#       with torch.no_grad():\n",
    "\n",
    "#           for val_image, val_text, val_label in val_dataloader:\n",
    "\n",
    "#               val_label = val_label.to(device)\n",
    "#               val_image = val_image.to(device)\n",
    "#               mask = val_text['attention_mask'].to(device)\n",
    "#               input_id = val_text['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "#               out_img = model.forward(val_image, contrastive = False)\n",
    "              \n",
    "\n",
    "#               batch_loss = criterion(out_img, val_label)\n",
    "#               total_loss_val += batch_loss.item()\n",
    "#               acc_val(out_img,val_label)\n",
    "              \n",
    "      \n",
    "#       avg_train_loss = total_loss_train/len(train_df)\n",
    "#     #   train_accuracy = total_acc_train/len(train_df)\n",
    "\n",
    "#       avg_val_loss = total_loss_val/len(val_df)\n",
    "#     #   val_accuracy = total_acc_val/len(dev_df)\n",
    "\n",
    "#       print(f\"Epoch [{epoch_num+1}/{EPOCHS}], \"f\"Train Loss: {avg_train_loss*BATCH_SIZE:.4f}, \"f\"Train Accuracy: {acc_train.compute():.4f}\")\n",
    "#       print(f\"Epoch [{epoch_num+1}/{EPOCHS}], \"f\"Val Loss: {avg_val_loss*BATCH_SIZE:.4f}, \"f\"Val Accuracy: {acc_val.compute():.4f}\")\n",
    "#       print('-'*60)\n",
    "      \n",
    "#       acc_train.reset()\n",
    "#       acc_val.reset()\n",
    "\n",
    "#       torch.save(model.state_dict(), './' + str(epoch_num)+'finetuning' + '.pt' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]c:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n",
      "100%|██████████| 40/40 [00:04<00:00,  9.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.690245\n",
      "Prec: 0.369007\n",
      "Recall: 0.205\n",
      "F1-score: 0.261\n",
      "F-Beta-score: 0.278\n",
      "Kappa: 0.000\n",
      "AUC: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss(torch.tensor([0.5, 1, 5, 5, 5, 6, 5, 1]).float().to(device))\n",
    "\n",
    "test_loss = 0\n",
    "test_acc  = 0\n",
    "\n",
    "AVERAGING = 'weighted'\n",
    "PREC = torchmetrics.classification.MultilabelPrecision(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "ACC = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "REC = torchmetrics.classification.MultilabelRecall(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "F1_SCORE = torchmetrics.classification.MultilabelF1Score(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "F_BETA_SCORE = torchmetrics.classification.MultilabelFBetaScore(beta = 0.8, num_classes = 8, num_labels = 8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "KAPPA = torchmetrics.classification.MulticlassCohenKappa(8).to(device)#, validate_args = False)\n",
    "AUC = torchmetrics.classification.MultilabelAUROC(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "\n",
    "for train_image, train_text, train_label in tqdm(test_dataloader): \n",
    "    with torch.no_grad():\n",
    "        train_label = train_label.to(device)\n",
    "        train_image = train_image.to(device)\n",
    "        mask = train_text['attention_mask'].to(device)\n",
    "        input_id = train_text['input_ids'].squeeze(1).to(device)\n",
    "        \n",
    "        # logits_per_image, logits_per_text\n",
    "        #out_img, out_txt = model.forward(train_image, input_id, mask, contrastive = True)\n",
    "        predictions = model.forward(train_image, contrastive = False)\n",
    "\n",
    "\n",
    "\n",
    "        train_label = train_label.long()\n",
    "        PREC(predictions, train_label)\n",
    "        ACC(predictions, train_label)\n",
    "        REC(predictions, train_label)\n",
    "        F1_SCORE(predictions, train_label)\n",
    "        F_BETA_SCORE(predictions, train_label)\n",
    "        KAPPA(predictions, train_label)\n",
    "        AUC(predictions, train_label)\n",
    "\n",
    "\n",
    "add_prec = PREC.compute()\n",
    "add_acc = ACC.compute()\n",
    "add_rec = REC.compute()\n",
    "add_f1 = F1_SCORE.compute()\n",
    "add_fbeta = F_BETA_SCORE.compute()\n",
    "add_kappa = KAPPA.compute()\n",
    "add_auc = AUC.compute()\n",
    "\n",
    "avg_test_loss = test_loss/len(test_df)*BATCH_SIZE\n",
    "avg_test_acc  = test_acc /len(test_df)\n",
    "\n",
    "print(\"Acc: {:3f}\\nPrec: {:3f}\\nRecall: {:.3f}\\nF1-score: {:.3f}\\nF-Beta-score: {:.3f}\\nKappa: {:.3f}\\nAUC: {:.3f}\".format(add_acc, add_prec,add_rec, add_f1, add_fbeta, add_kappa, add_auc))\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('torchnew')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aae2c75324e7fdf5ebd22146e2daffaa477f8ea149f0e685be4c317c2939a685"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
