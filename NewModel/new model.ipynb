{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import logging\n",
    "import gdown\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "import albumentations as A\n",
    "\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from scipy.io import loadmat\n",
    "from sklearn.manifold import TSNE\n",
    "from torchmetrics.classification import MulticlassF1Score, JaccardIndex, MulticlassPrecision, MulticlassRecall, MulticlassAveragePrecision\n",
    "import pandas as pd\n",
    "from torchinfo import torchinfo\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, RobertaTokenizer, CLIPModel, CLIPTokenizer, CLIPProcessor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    \n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '../Datasets/ocular-disease-recognition-odir5k/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = ROOT_DIR + 'dataset_single_eye.csv'\n",
    "TEST_CSV = ROOT_DIR + 'TESTING_dataset_single_eye.csv'\n",
    "IMG_PATH = ROOT_DIR + 'preprocessed_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.io.read_image(IMG_PATH + '0_left.jpg').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df = pd.read_csv(CSV_PATH)\n",
    "test_df = pd.read_csv(TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df:pd.DataFrame):\n",
    "    df['Keywords'] = df['Keywords'].str.lower()\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: \" \".join(x.split()))\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
    "    return df\n",
    "train_val_df = preprocess_text(train_val_df)\n",
    "test_df = preprocess_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_val_df['Keywords'].apply(lambda x: len(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4877, 861)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(train_val_df, test_size = 0.15)\n",
    "len(train_df), len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "rescale_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(IMG_SIZE, antialias = True),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODIRDatasetMM(Dataset) :\n",
    "    def __init__(self, df, IMG_FOLDER, tokenizer = processor) :\n",
    "        '''\n",
    "        id : list of samples ids as string\n",
    "        '''\n",
    "        self.text = [tokenizer(text = x, padding = 'max_length', max_length = 25, truncation = True, return_tensors = 'pt') for x in df['Keywords']]\n",
    "        self.eye = df['eye']\n",
    "        self.labels = torch.tensor(df[['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']].to_numpy()).float()\n",
    "        self.img_dir = [IMG_PATH + x for x in df['Image']]\n",
    "\n",
    "        self.images = [rescale_transform(torchvision.io.read_image(x).float()/255.0) for x in self.img_dir]\n",
    "        #self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.text[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ODIRDatasetMM(train_df, IMG_PATH)\n",
    "val_dataset   = ODIRDatasetMM(val_df, IMG_PATH)\n",
    "test_dataset  = ODIRDatasetMM(test_df, IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size= BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrastive learning on training data finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ConvNextV2Model, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare two models: BERT vs ConvNext, try to compute contrastive losses\n",
    "class ContrastiveLearning(nn.Module):\n",
    "    def __init__(self, drop_prob = 0.4):\n",
    "        super().__init__()\n",
    "        self.img_model = ConvNextV2Model.from_pretrained(\"facebook/convnextv2-tiny-1k-224\")      #output 768 features\n",
    "        self.txt_model = RobertaModel.from_pretrained(\"roberta-base\")                            #output 768 features\n",
    "        \n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.fc2 = nn.Linear(256, 768)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.head = nn.Linear(768, 8)\n",
    "    def forward(self, img_input, input_ids = None, attn_mask = None, contrastive = False):\n",
    "        if(contrastive):\n",
    "            # pretraining\n",
    "            out_txt = self.txt_model(input_ids, attn_mask)\n",
    "            out_img = self.img_model(img_input)\n",
    "\n",
    "            return out_img, out_txt\n",
    "        else:\n",
    "            out = self.img_model(img_input, return_dict = False)[1]\n",
    "            resi = out\n",
    "            out = F.relu(self.fc1(out))\n",
    "            out = F.relu(self.fc2(out))\n",
    "            out = out + resi\n",
    "            out = self.head(out)\n",
    "            return F.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/convnextv2-tiny-1k-224 were not used when initializing ConvNextV2Model: ['convnextv2.encoder.stages.0.layers.1.grn.weight', 'convnextv2.encoder.stages.2.layers.1.grn.bias', 'convnextv2.encoder.stages.3.layers.1.grn.bias', 'convnextv2.encoder.stages.3.layers.2.grn.bias', 'convnextv2.encoder.stages.2.layers.0.grn.weight', 'convnextv2.encoder.stages.0.layers.2.grn.weight', 'convnextv2.encoder.stages.1.layers.2.grn.weight', 'convnextv2.encoder.stages.0.layers.0.grn.bias', 'convnextv2.encoder.stages.1.layers.1.grn.bias', 'convnextv2.encoder.stages.2.layers.2.grn.bias', 'classifier.weight', 'convnextv2.encoder.stages.1.layers.2.grn.bias', 'convnextv2.encoder.stages.0.layers.1.grn.bias', 'convnextv2.encoder.stages.2.layers.7.grn.weight', 'convnextv2.encoder.stages.2.layers.1.grn.weight', 'convnextv2.encoder.stages.2.layers.0.grn.bias', 'convnextv2.encoder.stages.3.layers.0.grn.weight', 'convnextv2.encoder.stages.1.layers.0.grn.weight', 'convnextv2.encoder.stages.2.layers.7.grn.bias', 'convnextv2.encoder.stages.3.layers.0.grn.bias', 'convnextv2.encoder.stages.2.layers.8.grn.weight', 'convnextv2.encoder.stages.1.layers.1.grn.weight', 'convnextv2.encoder.stages.3.layers.1.grn.weight', 'convnextv2.encoder.stages.2.layers.2.grn.weight', 'convnextv2.encoder.stages.0.layers.0.grn.weight', 'convnextv2.encoder.stages.2.layers.6.grn.bias', 'convnextv2.encoder.stages.0.layers.2.grn.bias', 'convnextv2.encoder.stages.2.layers.6.grn.weight', 'convnextv2.encoder.stages.2.layers.3.grn.bias', 'convnextv2.encoder.stages.2.layers.5.grn.weight', 'convnextv2.encoder.stages.2.layers.4.grn.bias', 'convnextv2.encoder.stages.3.layers.2.grn.weight', 'convnextv2.encoder.stages.2.layers.3.grn.weight', 'classifier.bias', 'convnextv2.encoder.stages.1.layers.0.grn.bias', 'convnextv2.encoder.stages.2.layers.8.grn.bias', 'convnextv2.encoder.stages.2.layers.5.grn.bias', 'convnextv2.encoder.stages.2.layers.4.grn.weight']\n",
      "- This IS expected if you are initializing ConvNextV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ConvNextV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = ContrastiveLearning().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:17<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: -7368.4465\n",
      "Epoch [1/5], Val Loss: -9049.8051\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 296/305 [01:15<00:02,  4.00it/s]"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_text = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "\n",
    "      total_acc_train = 0\n",
    "      total_loss_train = 0\n",
    "\n",
    "      for train_image, train_text, train_label in tqdm(train_dataloader):\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          train_image = train_image.to(device)\n",
    "          mask = train_text['attention_mask'].to(device)\n",
    "          input_id = train_text['input_ids'].squeeze(1).to(device)\n",
    "          \n",
    "          # logits_per_image, logits_per_text\n",
    "          out_img, out_txt = model.forward(train_image, input_id, mask, contrastive = True)\n",
    "\n",
    "          #print(out_txt['pooler_output'].shape)\n",
    "\n",
    "          #ground_truth = torch.arange(len(train_image),dtype=torch.long,device=device)\n",
    "          \n",
    "          batch_loss = (loss_img(out_txt['pooler_output'], out_img['pooler_output']) + loss_text(out_img['pooler_output'], out_txt['pooler_output']))\n",
    "          batch_loss.backward()\n",
    "          optimizer.step()\n",
    "          total_loss_train += batch_loss.item()\n",
    "          \n",
    "        #   acc = (output['logits'].argmax(dim=1) == train_label).sum().item()\n",
    "        #   total_acc_train += acc\n",
    "      \n",
    "      total_acc_val = 0\n",
    "      total_loss_val = 0\n",
    "\n",
    "      with torch.no_grad():\n",
    "\n",
    "          for val_image, val_text, val_label in val_dataloader:\n",
    "\n",
    "              val_label = val_label.to(device)\n",
    "              val_image = val_image.to(device)\n",
    "              mask = val_text['attention_mask'].to(device)\n",
    "              input_id = val_text['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              out_img, out_txt = model.forward(val_image, input_id, mask, contrastive = True)\n",
    "              \n",
    "              #ground_truth = torch.arange(len(val_image),dtype=torch.long,device=device)\n",
    "              batch_loss = (loss_img(out_txt['pooler_output'], out_img['pooler_output']) + loss_text(out_img['pooler_output'], out_txt['pooler_output']))\n",
    "              total_loss_val += batch_loss.item()\n",
    "              \n",
    "      \n",
    "      avg_train_loss = total_loss_train/len(train_df)\n",
    "    #   train_accuracy = total_acc_train/len(train_df)\n",
    "\n",
    "      avg_val_loss = total_loss_val/len(val_df)\n",
    "    #   val_accuracy = total_acc_val/len(dev_df)\n",
    "\n",
    "      print(f\"Epoch [{epoch_num+1}/{EPOCHS}], \"f\"Train Loss: {avg_train_loss*BATCH_SIZE:.4f}\")\n",
    "      print(f\"Epoch [{epoch_num+1}/{EPOCHS}], \"f\"Val Loss: {avg_val_loss*BATCH_SIZE:.4f}\")\n",
    "      print('-'*60)\n",
    "\n",
    "      torch.save(model.state_dict(), './' + str(epoch_num+21) + '.pt' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"25.pt\", 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [00:51<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.0191, Train Accuracy: 0.8711\n",
      "Epoch [1/10], Val Loss: 0.0189, Val Accuracy: 0.8750\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [00:51<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 0.0188, Train Accuracy: 0.8733\n",
      "Epoch [2/10], Val Loss: 0.0188, Val Accuracy: 0.8750\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [00:51<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 0.0186, Train Accuracy: 0.8737\n",
      "Epoch [3/10], Val Loss: 0.0186, Val Accuracy: 0.8773\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [00:51<00:00,  5.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 0.0185, Train Accuracy: 0.8752\n",
      "Epoch [4/10], Val Loss: 0.0186, Val Accuracy: 0.8775\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [00:51<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 0.0184, Train Accuracy: 0.8749\n",
      "Epoch [5/10], Val Loss: 0.0185, Val Accuracy: 0.8750\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 270/305 [00:45<00:05,  5.93it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m criterion(out_img, train_label)\n\u001b[0;32m     27\u001b[0m batch_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     29\u001b[0m total_loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     31\u001b[0m acc_train(out_img, train_label)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\optim\\adam.py:143\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    132\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    134\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    135\u001b[0m         group,\n\u001b[0;32m    136\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    141\u001b[0m         state_steps)\n\u001b[1;32m--> 143\u001b[0m     adam(\n\u001b[0;32m    144\u001b[0m         params_with_grad,\n\u001b[0;32m    145\u001b[0m         grads,\n\u001b[0;32m    146\u001b[0m         exp_avgs,\n\u001b[0;32m    147\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    148\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    149\u001b[0m         state_steps,\n\u001b[0;32m    150\u001b[0m         amsgrad\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mamsgrad\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    151\u001b[0m         beta1\u001b[39m=\u001b[39mbeta1,\n\u001b[0;32m    152\u001b[0m         beta2\u001b[39m=\u001b[39mbeta2,\n\u001b[0;32m    153\u001b[0m         lr\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    154\u001b[0m         weight_decay\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    155\u001b[0m         eps\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39meps\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    156\u001b[0m         maximize\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    157\u001b[0m         foreach\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mforeach\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    158\u001b[0m         capturable\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mcapturable\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    159\u001b[0m         differentiable\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    160\u001b[0m         fused\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mfused\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    161\u001b[0m         grad_scale\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgrad_scale\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[0;32m    162\u001b[0m         found_inf\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfound_inf\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\optim\\adam.py:283\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 283\u001b[0m func(params,\n\u001b[0;32m    284\u001b[0m      grads,\n\u001b[0;32m    285\u001b[0m      exp_avgs,\n\u001b[0;32m    286\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    287\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    288\u001b[0m      state_steps,\n\u001b[0;32m    289\u001b[0m      amsgrad\u001b[39m=\u001b[39mamsgrad,\n\u001b[0;32m    290\u001b[0m      beta1\u001b[39m=\u001b[39mbeta1,\n\u001b[0;32m    291\u001b[0m      beta2\u001b[39m=\u001b[39mbeta2,\n\u001b[0;32m    292\u001b[0m      lr\u001b[39m=\u001b[39mlr,\n\u001b[0;32m    293\u001b[0m      weight_decay\u001b[39m=\u001b[39mweight_decay,\n\u001b[0;32m    294\u001b[0m      eps\u001b[39m=\u001b[39meps,\n\u001b[0;32m    295\u001b[0m      maximize\u001b[39m=\u001b[39mmaximize,\n\u001b[0;32m    296\u001b[0m      capturable\u001b[39m=\u001b[39mcapturable,\n\u001b[0;32m    297\u001b[0m      differentiable\u001b[39m=\u001b[39mdifferentiable,\n\u001b[0;32m    298\u001b[0m      grad_scale\u001b[39m=\u001b[39mgrad_scale,\n\u001b[0;32m    299\u001b[0m      found_inf\u001b[39m=\u001b[39mfound_inf)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\optim\\adam.py:451\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    448\u001b[0m torch\u001b[39m.\u001b[39m_foreach_add_(device_exp_avgs, device_grads, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m    450\u001b[0m torch\u001b[39m.\u001b[39m_foreach_mul_(device_exp_avg_sqs, beta2)\n\u001b[1;32m--> 451\u001b[0m torch\u001b[39m.\u001b[39m_foreach_addcmul_(device_exp_avg_sqs, device_grads, device_grads, \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m    453\u001b[0m \u001b[39mif\u001b[39;00m capturable:\n\u001b[0;32m    454\u001b[0m     bias_correction1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_foreach_pow(beta1, device_state_steps)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "test_loss = 0\n",
    "test_acc  = 0\n",
    "AVERAGING = 'micro'\n",
    "acc_train = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "acc_val   = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "      total_loss_train = 0\n",
    "\n",
    "      for train_image, train_text, train_label in tqdm(train_dataloader):\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          train_label = train_label.to(device)\n",
    "          train_image = train_image.to(device)\n",
    "          mask = train_text['attention_mask'].to(device)\n",
    "          input_id = train_text['input_ids'].squeeze(1).to(device)\n",
    "          \n",
    "          # logits_per_image, logits_per_text\n",
    "          out_img = model.forward(train_image, contrastive = False)\n",
    "          #print(out_img.shape, train_label.shape)\n",
    "          batch_loss = criterion(out_img, train_label)\n",
    "          batch_loss.backward()\n",
    "          optimizer.step()\n",
    "          total_loss_train += batch_loss.item()\n",
    "          \n",
    "          acc_train(out_img, train_label)\n",
    "        #   acc = (output['logits'].argmax(dim=1) == train_label).sum().item()\n",
    "        #   total_acc_train += acc\n",
    "      \n",
    "      total_acc_val = 0\n",
    "      total_loss_val = 0\n",
    "\n",
    "      with torch.no_grad():\n",
    "\n",
    "          for val_image, val_text, val_label in val_dataloader:\n",
    "\n",
    "              val_label = val_label.to(device)\n",
    "              val_image = val_image.to(device)\n",
    "              mask = val_text['attention_mask'].to(device)\n",
    "              input_id = val_text['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              out_img = model.forward(val_image, contrastive = False)\n",
    "              \n",
    "\n",
    "              batch_loss = criterion(out_img, val_label)\n",
    "              total_loss_val += batch_loss.item()\n",
    "              acc_val(out_img,val_label)\n",
    "              \n",
    "      \n",
    "      avg_train_loss = total_loss_train/len(train_df)\n",
    "    #   train_accuracy = total_acc_train/len(train_df)\n",
    "\n",
    "      avg_val_loss = total_loss_val/len(val_df)\n",
    "    #   val_accuracy = total_acc_val/len(dev_df)\n",
    "\n",
    "      print(f\"Epoch [{epoch_num+1}/{EPOCHS}], \"f\"Train Loss: {avg_train_loss*BATCH_SIZE:.4f}, \"f\"Train Accuracy: {acc_train.compute():.4f}\")\n",
    "      print(f\"Epoch [{epoch_num+1}/{EPOCHS}], \"f\"Val Loss: {avg_val_loss*BATCH_SIZE:.4f}, \"f\"Val Accuracy: {acc_val.compute():.4f}\")\n",
    "      print('-'*60)\n",
    "      \n",
    "      acc_train.reset()\n",
    "      acc_val.reset()\n",
    "\n",
    "      torch.save(model.state_dict(), './' + str(epoch_num)+'finetuning' + '.pt' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:03<00:00, 12.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.125000\n",
      "Prec: 0.125000\n",
      "Recall: 1.000\n",
      "F1-score: 0.222\n",
      "F-Beta-score: 0.190\n",
      "Kappa: 0.000\n",
      "AUC: 0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "test_acc  = 0\n",
    "PREC = torchmetrics.classification.MultilabelPrecision(8, average = 'micro').to(device)#, validate_args = False)\n",
    "ACC = torchmetrics.classification.MultilabelAccuracy(8, average = 'micro').to(device)#, validate_args = False)\n",
    "REC = torchmetrics.classification.MultilabelRecall(8, average = 'micro').to(device)#, validate_args = False)\n",
    "F1_SCORE = torchmetrics.classification.MultilabelF1Score(8, average = 'micro').to(device)#, validate_args = False)\n",
    "F_BETA_SCORE = torchmetrics.classification.MultilabelFBetaScore(beta = 0.8, num_classes = 8, num_labels = 8, average = 'micro').to(device)#, validate_args = False)\n",
    "KAPPA = torchmetrics.classification.MulticlassCohenKappa(8).to(device)#, validate_args = False)\n",
    "AUC = torchmetrics.classification.MultilabelAUROC(8, average = 'micro').to(device)#, validate_args = False)\n",
    "\n",
    "for train_image, train_text, train_label in tqdm(test_dataloader): \n",
    "    train_image = train_image.to(device)\n",
    "    train_label = train_label.to(device)\n",
    "    with torch.no_grad():\n",
    "        scores = model(train_image, contrastive = False)\n",
    "        scores = torch.sigmoid(scores)\n",
    "    loss = criterion(scores, train_label.float())\n",
    "    test_loss+= loss.item()\n",
    "    predicted = torch.round(scores).to(device)\n",
    "    test_acc+= (torch.sum(predicted == train_label)/(BATCH_SIZE*8))\n",
    "\n",
    "    train_label = train_label.int()\n",
    "    PREC(predicted, train_label)\n",
    "    ACC(predicted, train_label)\n",
    "    REC(predicted, train_label)\n",
    "    F1_SCORE(predicted, train_label)\n",
    "    F_BETA_SCORE(predicted, train_label)\n",
    "    KAPPA(predicted, train_label)\n",
    "    AUC(predicted, train_label)\n",
    "\n",
    "\n",
    "add_prec = PREC.compute()\n",
    "add_acc = ACC.compute()\n",
    "add_rec = REC.compute()\n",
    "add_f1 = F1_SCORE.compute()\n",
    "add_fbeta = F_BETA_SCORE.compute()\n",
    "add_kappa = KAPPA.compute()\n",
    "add_auc = AUC.compute()\n",
    "\n",
    "avg_test_loss = test_loss/len(test_df)*BATCH_SIZE\n",
    "avg_test_acc  = test_acc /len(test_df)\n",
    "\n",
    "print(\"Acc: {:3f}\\nPrec: {:3f}\\nRecall: {:.3f}\\nF1-score: {:.3f}\\nF-Beta-score: {:.3f}\\nKappa: {:.3f}\\nAUC: {:.3f}\".format(add_acc, add_prec,add_rec, add_f1, add_fbeta, add_kappa, add_auc))\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('torchnew')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aae2c75324e7fdf5ebd22146e2daffaa477f8ea149f0e685be4c317c2939a685"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
