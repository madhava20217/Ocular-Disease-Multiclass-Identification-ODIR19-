{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import logging\n",
    "import gdown\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "import albumentations as A\n",
    "\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from scipy.io import loadmat\n",
    "from sklearn.manifold import TSNE\n",
    "from torchmetrics.classification import MulticlassF1Score, JaccardIndex, MulticlassPrecision, MulticlassRecall, MulticlassAveragePrecision\n",
    "import pandas as pd\n",
    "from torchinfo import torchinfo\n",
    "\n",
    "from transformers import ConvNextV2Model, BertModel, BertTokenizer, ViTModel, ViTConfig\n",
    "from transformers import AutoTokenizer, AutoModel, RobertaTokenizer, CLIPModel, CLIPTokenizer, CLIPProcessor\n",
    "from transformers import DeiTConfig, DeiTFeatureExtractor, DeiTImageProcessor, DeiTModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    \n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '../Datasets/ocular-disease-recognition-odir5k/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = ROOT_DIR + 'dataset_single_eye.csv'\n",
    "TEST_CSV = ROOT_DIR + 'TESTING_dataset_single_eye.csv'\n",
    "IMG_PATH = ROOT_DIR + 'preprocessed_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.io.read_image(IMG_PATH + '0_left.jpg').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df = pd.read_csv(CSV_PATH)\n",
    "test_df = pd.read_csv(TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df:pd.DataFrame):\n",
    "    df['Keywords'] = df['Keywords'].str.lower()\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: \" \".join(x.split()))\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
    "    return df\n",
    "train_val_df = preprocess_text(train_val_df)\n",
    "test_df = preprocess_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_val_df['Keywords'].apply(lambda x: len(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4877, 861)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(train_val_df, test_size = 0.15)\n",
    "len(train_df), len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "rescale_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(IMG_SIZE, antialias = True, interpolation = 3),\n",
    "    torchvision.transforms.Normalize(\n",
    "        timm.data.constants.IMAGENET_DEFAULT_MEAN,\n",
    "        timm.data.constants.IMAGENET_DEFAULT_STD\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'moderate non proliferative retinopathy'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_df['Keywords'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\krish/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8777, 2512, 4013, 15509, 18514, 2128, 25690, 20166, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
    "\n",
    "processor(\"moderate non proliferative retinopathy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 2088, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODIRDatasetMM(Dataset) :\n",
    "    def __init__(self, df, IMG_FOLDER, tokenizer = processor, feature_extractor = rescale_transform) :\n",
    "        '''\n",
    "        id : list of samples ids as string\n",
    "        '''\n",
    "        self.text = [tokenizer(text = x, padding = 'max_length', max_length = 45, truncation = True, return_tensors = 'pt') for x in df['Keywords']]\n",
    "        self.eye = df['eye']\n",
    "        self.labels = torch.tensor(df[['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']].to_numpy()).float()\n",
    "        self.img_dir = [IMG_PATH + x for x in df['Image']]\n",
    "\n",
    "        self.images = [feature_extractor(torchvision.io.read_image(x).float()) for x in self.img_dir]\n",
    "        #self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.text[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ODIRDatasetMM(train_df, IMG_PATH)\n",
    "val_dataset   = ODIRDatasetMM(val_df, IMG_PATH)\n",
    "test_dataset  = ODIRDatasetMM(test_df, IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size= BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrastive learning on training data finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare two models: BERT vs ConvNext, try to compute contrastive losses\n",
    "class ContrastiveLearning(nn.Module):\n",
    "    def __init__(self, drop_prob = 0.4):\n",
    "        super().__init__()\n",
    "        self.img_model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True) #output 768 features\n",
    "        self.img_model.head = nn.Identity()                                                        \n",
    "        \n",
    "        self.txt_model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')         #output 768 features\n",
    "        \n",
    "        # image model classification head\n",
    "        self.fc1 = nn.Linear(768, 768)\n",
    "\n",
    "    def forward(self, img_input, input_ids = None, attn_mask = None, contrastive = False, text_class = False):\n",
    "        if(contrastive):\n",
    "            # pretraining\n",
    "            out_txt = self.txt_model(input_ids, attn_mask)['pooler_output']\n",
    "            out_img = self.img_model(img_input)\n",
    "\n",
    "            out_txt = F.normalize(out_txt, p = 2.0, dim = 1)\n",
    "            out_img = F.normalize(out_txt, p = 2.0, dim = 1)\n",
    "\n",
    "            return out_img, out_txt\n",
    "        else:\n",
    "            out = self.img_model(img_input)\n",
    "            out = self.fc1(out)\n",
    "            return F.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\krish/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\krish/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = ContrastiveLearning().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(ten1, ten2, temperature = nn.Parameter(torch.tensor(.07).to(device))):    #...\n",
    "    #steps = hadamard product\n",
    "    # trivial for loop \n",
    "    sim = torch.einsum('i d, j d -> i j', ten1, ten2) * temperature.exp()\n",
    "    labels = torch.arange(ten1.size(0), device = device)\n",
    "    loss = (F.cross_entropy(sim, labels) + F.cross_entropy(sim.t(), labels)) / 2\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [00:55<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/7], Train Loss: 2.0605, \n",
      "Epoch [1/7], Val Loss: 2.0272, \n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [00:55<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/7], Train Loss: 2.0176, \n",
      "Epoch [2/7], Val Loss: 2.0120, \n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [00:55<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/7], Train Loss: 2.0118, \n",
      "Epoch [3/7], Val Loss: 2.0085, \n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [00:55<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/7], Train Loss: 2.0114, \n",
      "Epoch [4/7], Val Loss: 2.0142, \n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [00:55<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/7], Train Loss: 2.0551, \n",
      "Epoch [5/7], Val Loss: 2.5663, \n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [00:54<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/7], Train Loss: 2.4067, \n",
      "Epoch [6/7], Val Loss: 2.3470, \n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [00:56<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/7], Train Loss: 2.3601, \n",
      "Epoch [7/7], Val Loss: 2.7784, \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "criterion_text = nn.BCELoss(torch.tensor([0.8, 1, 1.25, 1.25, 1.25, 1.3, 1.25, 1]).float().to(device))\n",
    "criterion_image = nn.BCELoss(torch.tensor([0.8, 1, 1.25, 1.25, 1.25, 1.3, 1.25, 1]).float().to(device))\n",
    "cont_loss = contrastive_loss\n",
    "AVERAGING = 'micro'\n",
    "acc_train = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "acc_val   = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)\n",
    "\n",
    "EPOCHS = 7\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "\n",
    "      total_acc_train = 0\n",
    "      total_loss_train = 0\n",
    "\n",
    "      for train_image, train_text, train_label in tqdm(train_dataloader):\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          train_label = train_label.to(device)\n",
    "          train_image = train_image.to(device)\n",
    "          mask = train_text['attention_mask'].to(device)\n",
    "          input_id = train_text['input_ids'].squeeze(1).to(device)\n",
    "          \n",
    "          # logits_per_image, logits_per_text\n",
    "          out_img, out_txt = model.forward(train_image, input_id, mask, contrastive = True)\n",
    "          predictions_img = model.forward(train_image, contrastive = False)\n",
    "          acc_train(predictions_img, train_label)\n",
    "\n",
    "\n",
    "          closs = cont_loss(out_img, out_txt)\n",
    "          text_loss = criterion_text(predictions_text, train_label)\n",
    "          img_loss = criterion_image(predictions_img, train_label)\n",
    "          batch_loss = closs + text_loss + img_loss#closs + l3\n",
    "          batch_loss.backward()\n",
    "          optimizer.step()\n",
    "          total_loss_train += batch_loss.item()\n",
    "          \n",
    "        #   acc = (output['logits'].argmax(dim=1) == train_label).sum().item()\n",
    "        #   total_acc_train += acc\n",
    "      \n",
    "      total_acc_val = 0\n",
    "      total_loss_val = 0\n",
    "\n",
    "      with torch.no_grad():\n",
    "\n",
    "          for val_image, val_text, val_label in val_dataloader:\n",
    "\n",
    "              val_label = val_label.to(device)\n",
    "              val_image = val_image.to(device)\n",
    "              mask = val_text['attention_mask'].to(device)\n",
    "              input_id = val_text['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              out_img, out_txt = model.forward(val_image, input_id, mask, contrastive = True)\n",
    "              #predictions = model.forward(val_image, contrastive = False)\n",
    "              #ground_truth = torch.arange(len(val_image),dtype=torch.long,device=device)\n",
    "\n",
    "              # l1 = loss_img(out_txt, out_img)\n",
    "              # l2 = loss_text(out_img, out_txt)\n",
    "              closs = cont_loss(out_img, out_txt)\n",
    "              #l3 = criterion(predictions, val_label)\n",
    "              batch_loss = closs#closs + l3\n",
    "              total_loss_val += batch_loss.item()\n",
    "\n",
    "             # acc_val(predictions, val_label)\n",
    "              \n",
    "      \n",
    "      avg_train_loss = total_loss_train/len(train_df)\n",
    "    #   train_accuracy = total_acc_train/len(train_df)\n",
    "\n",
    "      avg_val_loss = total_loss_val/len(val_df)\n",
    "    #   val_accuracy = total_acc_val/len(dev_df)\n",
    "\n",
    "      print(f\"Epoch [{epoch_num+1}/{EPOCHS}], \"f\"Train Loss: {avg_train_loss*BATCH_SIZE:.4f}, \")#f\"Train Accuracy: {acc_train.compute():.4f}\")\n",
    "      print(f\"Epoch [{epoch_num+1}/{EPOCHS}], \"f\"Val Loss: {avg_val_loss*BATCH_SIZE:.4f}, \")#f\"Val Accuracy: {acc_val.compute():.4f}\")\n",
    "      print('-'*60)\n",
    "\n",
    "      acc_train.reset()\n",
    "      acc_val.reset()\n",
    "\n",
    "      if(epoch_num%3 == 0):\n",
    "        torch.save(model.state_dict(), './' + str(epoch_num+21) + '.pt' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"27.pt\", 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [00:59<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.3022, Train Accuracy: 0.8709\n",
      "Epoch [1/10], Val Loss: 0.2952, Val Accuracy: 0.8738\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:03<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 0.2887, Train Accuracy: 0.8758\n",
      "Epoch [2/10], Val Loss: 0.2957, Val Accuracy: 0.8757\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [01:00<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 0.2835, Train Accuracy: 0.8783\n",
      "Epoch [3/10], Val Loss: 0.2896, Val Accuracy: 0.8798\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [00:58<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 0.2776, Train Accuracy: 0.8821\n",
      "Epoch [4/10], Val Loss: 0.2876, Val Accuracy: 0.8712\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [00:57<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 0.2706, Train Accuracy: 0.8846\n",
      "Epoch [5/10], Val Loss: 0.2933, Val Accuracy: 0.8770\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 63/305 [00:12<00:46,  5.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# logits_per_image, logits_per_text\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#out_img, out_txt = model.forward(train_image, input_id, mask, contrastive = True)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(train_image, contrastive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 31\u001b[0m acc_train(predictions, train_label)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#ground_truth = torch.arange(len(train_image),dtype=torch.long,device=device)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#closs = cont_loss(out_img, out_txt)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m l3 \u001b[38;5;241m=\u001b[39m criterion(predictions, train_label)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torchmetrics\\metric.py:236\u001b[0m, in \u001b[0;36mMetric.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_full_state_update(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    235\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_reduce_state_update(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    238\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torchmetrics\\metric.py:302\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# allow grads for batch computation\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[39m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[1;32m--> 302\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    303\u001b[0m batch_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute()\n\u001b[0;32m    305\u001b[0m \u001b[39m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torchmetrics\\metric.py:390\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_grad):\n\u001b[0;32m    389\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m         update(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    391\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    392\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected all tensors to be on\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(err):\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torchmetrics\\classification\\stat_scores.py:458\u001b[0m, in \u001b[0;36mMultilabelStatScores.update\u001b[1;34m(self, preds, target)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_args:\n\u001b[0;32m    455\u001b[0m     _multilabel_stat_scores_tensor_validation(\n\u001b[0;32m    456\u001b[0m         preds, target, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultidim_average, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_index\n\u001b[0;32m    457\u001b[0m     )\n\u001b[1;32m--> 458\u001b[0m preds, target \u001b[39m=\u001b[39m _multilabel_stat_scores_format(\n\u001b[0;32m    459\u001b[0m     preds, target, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthreshold, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_index\n\u001b[0;32m    460\u001b[0m )\n\u001b[0;32m    461\u001b[0m tp, fp, tn, fn \u001b[39m=\u001b[39m _multilabel_stat_scores_update(preds, target, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultidim_average)\n\u001b[0;32m    462\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_state(tp, fp, tn, fn)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torchmetrics\\functional\\classification\\stat_scores.py:656\u001b[0m, in \u001b[0;36m_multilabel_stat_scores_format\u001b[1;34m(preds, target, num_labels, threshold, ignore_index)\u001b[0m\n\u001b[0;32m    654\u001b[0m         preds \u001b[39m=\u001b[39m preds\u001b[39m.\u001b[39msigmoid()\n\u001b[0;32m    655\u001b[0m     preds \u001b[39m=\u001b[39m preds \u001b[39m>\u001b[39m threshold\n\u001b[1;32m--> 656\u001b[0m preds \u001b[39m=\u001b[39m preds\u001b[39m.\u001b[39mreshape(\u001b[39m*\u001b[39mpreds\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    657\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mreshape(\u001b[39m*\u001b[39mtarget\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    659\u001b[0m \u001b[39mif\u001b[39;00m ignore_index \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr = 0.00001)\n",
    "\n",
    "# loss_img = nn.CrossEntropyLoss()\n",
    "# loss_text = nn.CrossEntropyLoss()\n",
    "\n",
    "# cont_loss = contrastive_loss\n",
    "# criterion = nn.BCELoss()#torch.tensor([0.5, 1, 5, 5, 5, 6, 5, 1]).float().to(device))\n",
    "# AVERAGING = 'micro'\n",
    "# acc_train = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "# acc_val   = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)\n",
    "\n",
    "# EPOCHS = 10\n",
    "\n",
    "# for epoch_num in range(EPOCHS):\n",
    "\n",
    "#       total_acc_train = 0\n",
    "#       total_loss_train = 0\n",
    "\n",
    "#       for train_image, train_text, train_label in tqdm(train_dataloader):\n",
    "#           optimizer.zero_grad()\n",
    "\n",
    "#           train_label = train_label.to(device)\n",
    "#           train_image = train_image.to(device)\n",
    "#           mask = train_text['attention_mask'].to(device)\n",
    "#           input_id = train_text['input_ids'].squeeze(1).to(device)\n",
    "          \n",
    "#           # logits_per_image, logits_per_text\n",
    "#           #out_img, out_txt = model.forward(train_image, input_id, mask, contrastive = True)\n",
    "#           predictions = model.forward(train_image, contrastive = False)\n",
    "\n",
    "#           acc_train(predictions, train_label)\n",
    "\n",
    "#           #ground_truth = torch.arange(len(train_image),dtype=torch.long,device=device)\n",
    "\n",
    "#           #closs = cont_loss(out_img, out_txt)\n",
    "#           l3 = criterion(predictions, train_label)\n",
    "#           batch_loss = l3 #closs + l3\n",
    "#           batch_loss.backward()\n",
    "#           optimizer.step()\n",
    "#           total_loss_train += batch_loss.item()\n",
    "          \n",
    "#         #   acc = (output['logits'].argmax(dim=1) == train_label).sum().item()\n",
    "#         #   total_acc_train += acc\n",
    "      \n",
    "#       total_acc_val = 0\n",
    "#       total_loss_val = 0\n",
    "\n",
    "#       with torch.no_grad():\n",
    "\n",
    "#           for val_image, val_text, val_label in val_dataloader:\n",
    "\n",
    "#               val_label = val_label.to(device)\n",
    "#               val_image = val_image.to(device)\n",
    "#               mask = val_text['attention_mask'].to(device)\n",
    "#               input_id = val_text['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "#               #out_img, out_txt = model.forward(val_image, input_id, mask, contrastive = True)\n",
    "#               predictions = model.forward(val_image, contrastive = False)\n",
    "#               #ground_truth = torch.arange(len(val_image),dtype=torch.long,device=device)\n",
    "\n",
    "#               #closs = cont_loss(out_img, out_txt)\n",
    "#               l3 = criterion(predictions, val_label)\n",
    "#               batch_loss = l3\n",
    "#               total_loss_val += batch_loss.item()\n",
    "\n",
    "#               acc_val(predictions, val_label)\n",
    "              \n",
    "      \n",
    "#       avg_train_loss = total_loss_train/len(train_df)\n",
    "#     #   train_accuracy = total_acc_train/len(train_df)\n",
    "\n",
    "#       avg_val_loss = total_loss_val/len(val_df)\n",
    "#     #   val_accuracy = total_acc_val/len(dev_df)\n",
    "\n",
    "#       print(f\"Epoch [{epoch_num+1}/{EPOCHS}], \"f\"Train Loss: {avg_train_loss*BATCH_SIZE:.4f}, \"f\"Train Accuracy: {acc_train.compute():.4f}\")\n",
    "#       print(f\"Epoch [{epoch_num+1}/{EPOCHS}], \"f\"Val Loss: {avg_val_loss*BATCH_SIZE:.4f}, \"f\"Val Accuracy: {acc_val.compute():.4f}\")\n",
    "#       print('-'*60)\n",
    "\n",
    "#       acc_train.reset()\n",
    "#       acc_val.reset()\n",
    "\n",
    "      \n",
    "#       if(epoch_num%5 == 0):\n",
    "#         torch.save(model.state_dict(), './' + 'finetune' + str(epoch_num+21) + '.pt' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '25.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m25.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(f))\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '25.pt'"
     ]
    }
   ],
   "source": [
    "with open(\"25.pt\", 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "# criterion = nn.BCELoss(torch.tensor([1, 1, 5, 5, 5, 6, 5, 1]))\n",
    "# test_loss = 0\n",
    "# test_acc  = 0\n",
    "# AVERAGING = AVERAGING'\n",
    "# acc_train = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "# acc_val   = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)\n",
    "\n",
    "# EPOCHS = 5\n",
    "\n",
    "# for epoch_num in range(EPOCHS):\n",
    "#       total_loss_train = 0\n",
    "\n",
    "#       for train_image, train_text, train_label in tqdm(train_dataloader):\n",
    "#           optimizer.zero_grad()\n",
    "\n",
    "#           train_label = train_label.to(device)\n",
    "#           train_image = train_image.to(device)\n",
    "#           mask = train_text['attention_mask'].to(device)\n",
    "#           input_id = train_text['input_ids'].squeeze(1).to(device)\n",
    "          \n",
    "#           # logits_per_image, logits_per_text\n",
    "#           out_img = model.forward(train_image, contrastive = False)\n",
    "#           #print(out_img.shape, train_label.shape)\n",
    "#           batch_loss = criterion(out_img, train_label)\n",
    "#           batch_loss.backward()\n",
    "#           optimizer.step()\n",
    "#           total_loss_train += batch_loss.item()\n",
    "          \n",
    "#           acc_train(out_img, train_label)\n",
    "#         #   acc = (output['logits'].argmax(dim=1) == train_label).sum().item()\n",
    "#         #   total_acc_train += acc\n",
    "      \n",
    "#       total_acc_val = 0\n",
    "#       total_loss_val = 0\n",
    "\n",
    "#       with torch.no_grad():\n",
    "\n",
    "#           for val_image, val_text, val_label in val_dataloader:\n",
    "\n",
    "#               val_label = val_label.to(device)\n",
    "#               val_image = val_image.to(device)\n",
    "#               mask = val_text['attention_mask'].to(device)\n",
    "#               input_id = val_text['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "#               out_img = model.forward(val_image, contrastive = False)\n",
    "              \n",
    "\n",
    "#               batch_loss = criterion(out_img, val_label)\n",
    "#               total_loss_val += batch_loss.item()\n",
    "#               acc_val(out_img,val_label)\n",
    "              \n",
    "      \n",
    "#       avg_train_loss = total_loss_train/len(train_df)\n",
    "#     #   train_accuracy = total_acc_train/len(train_df)\n",
    "\n",
    "#       avg_val_loss = total_loss_val/len(val_df)\n",
    "#     #   val_accuracy = total_acc_val/len(dev_df)\n",
    "\n",
    "#       print(f\"Epoch [{epoch_num+1}/{EPOCHS}], \"f\"Train Loss: {avg_train_loss*BATCH_SIZE:.4f}, \"f\"Train Accuracy: {acc_train.compute():.4f}\")\n",
    "#       print(f\"Epoch [{epoch_num+1}/{EPOCHS}], \"f\"Val Loss: {avg_val_loss*BATCH_SIZE:.4f}, \"f\"Val Accuracy: {acc_val.compute():.4f}\")\n",
    "#       print('-'*60)\n",
    "      \n",
    "#       acc_train.reset()\n",
    "#       acc_val.reset()\n",
    "\n",
    "#       torch.save(model.state_dict(), './' + str(epoch_num)+'finetuning' + '.pt' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:03<00:00, 11.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.870494\n",
      "Prec: 0.449339\n",
      "Recall: 0.160\n",
      "F1-score: 0.236\n",
      "F-Beta-score: 0.263\n",
      "Kappa: 0.000\n",
      "AUC: 0.850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss(torch.tensor([0.5, 1, 5, 5, 5, 6, 5, 1]).float().to(device))\n",
    "\n",
    "test_loss = 0\n",
    "test_acc  = 0\n",
    "\n",
    "AVERAGING = 'micro'\n",
    "PREC = torchmetrics.classification.MultilabelPrecision(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "ACC = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "REC = torchmetrics.classification.MultilabelRecall(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "F1_SCORE = torchmetrics.classification.MultilabelF1Score(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "F_BETA_SCORE = torchmetrics.classification.MultilabelFBetaScore(beta = 0.8, num_classes = 8, num_labels = 8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "KAPPA = torchmetrics.classification.MulticlassCohenKappa(8).to(device)#, validate_args = False)\n",
    "AUC = torchmetrics.classification.MultilabelAUROC(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "\n",
    "for train_image, train_text, train_label in tqdm(test_dataloader): \n",
    "    with torch.no_grad():\n",
    "        train_label = train_label.to(device)\n",
    "        train_image = train_image.to(device)\n",
    "        mask = train_text['attention_mask'].to(device)\n",
    "        input_id = train_text['input_ids'].squeeze(1).to(device)\n",
    "        \n",
    "        # logits_per_image, logits_per_text\n",
    "        #out_img, out_txt = model.forward(train_image, input_id, mask, contrastive = True)\n",
    "        predictions = model.forward(train_image, contrastive = False)\n",
    "\n",
    "\n",
    "\n",
    "        train_label = train_label.long()\n",
    "        PREC(predictions, train_label)\n",
    "        ACC(predictions, train_label)\n",
    "        REC(predictions, train_label)\n",
    "        F1_SCORE(predictions, train_label)\n",
    "        F_BETA_SCORE(predictions, train_label)\n",
    "        KAPPA(predictions, train_label)\n",
    "        AUC(predictions, train_label)\n",
    "\n",
    "\n",
    "add_prec = PREC.compute()\n",
    "add_acc = ACC.compute()\n",
    "add_rec = REC.compute()\n",
    "add_f1 = F1_SCORE.compute()\n",
    "add_fbeta = F_BETA_SCORE.compute()\n",
    "add_kappa = KAPPA.compute()\n",
    "add_auc = AUC.compute()\n",
    "\n",
    "avg_test_loss = test_loss/len(test_df)*BATCH_SIZE\n",
    "avg_test_acc  = test_acc /len(test_df)\n",
    "\n",
    "print(\"Acc: {:3f}\\nPrec: {:3f}\\nRecall: {:.3f}\\nF1-score: {:.3f}\\nF-Beta-score: {:.3f}\\nKappa: {:.3f}\\nAUC: {:.3f}\".format(add_acc, add_prec,add_rec, add_f1, add_fbeta, add_kappa, add_auc))\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('torchnew')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aae2c75324e7fdf5ebd22146e2daffaa477f8ea149f0e685be4c317c2939a685"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
