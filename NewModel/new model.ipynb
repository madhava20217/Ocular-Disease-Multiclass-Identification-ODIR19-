{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import logging\n",
    "import gdown\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "import albumentations as A\n",
    "\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from scipy.io import loadmat\n",
    "from sklearn.manifold import TSNE\n",
    "from torchmetrics.classification import MulticlassF1Score, JaccardIndex, MulticlassPrecision, MulticlassRecall, MulticlassAveragePrecision\n",
    "import pandas as pd\n",
    "from torchinfo import torchinfo\n",
    "\n",
    "from transformers import ConvNextV2Model, BertModel, BertTokenizer, ViTModel, ViTConfig\n",
    "from transformers import AutoTokenizer, AutoModel, RobertaTokenizer, CLIPModel, CLIPTokenizer, CLIPProcessor\n",
    "from transformers import DeiTConfig, DeiTFeatureExtractor, DeiTImageProcessor, DeiTModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from torchmetrics.functional import pairwise_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    \n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '../Datasets/ocular-disease-recognition-odir5k/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = ROOT_DIR + 'dataset_single_eye.csv'\n",
    "TEST_CSV = ROOT_DIR + 'TESTING_dataset_single_eye.csv'\n",
    "IMG_PATH = ROOT_DIR + 'preprocessed_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.io.read_image(IMG_PATH + '0_left.jpg').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df = pd.read_csv(CSV_PATH)\n",
    "test_df = pd.read_csv(TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df:pd.DataFrame):\n",
    "    df['Keywords'] = df['Keywords'].str.lower()\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: \" \".join(x.split()))\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
    "    return df\n",
    "train_val_df = preprocess_text(train_val_df)\n",
    "test_df = preprocess_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_val_df['Keywords'].apply(lambda x: len(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4877, 861)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(train_val_df, test_size = 0.15, random_state= 123456)\n",
    "len(train_df), len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "rescale_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.CenterCrop(IMG_SIZE),\n",
    "    #torchvision.transforms.Resize(IMG_SIZE, antialias = False, interpolation = torchvision.transforms.InterpolationMode.NEAREST),\n",
    "    # torchvision.transforms.Normalize(\n",
    "    #     timm.data.constants.IMAGENET_DEFAULT_MEAN,\n",
    "    #     timm.data.constants.IMAGENET_DEFAULT_STD\n",
    "    # )\n",
    "])\n",
    "\n",
    "augmentation = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(p = 0.5),\n",
    "    torchvision.transforms.RandomVerticalFlip(p= 0.5),\n",
    "    #torchvision.transforms.RandomRotation(90)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\krish/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "processor = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODIRDatasetMM(Dataset) :\n",
    "    def __init__(self, df, IMG_FOLDER, tokenizer = processor, feature_extractor = rescale_transform, augmentation = None) :\n",
    "        '''\n",
    "        id : list of samples ids as string\n",
    "        '''\n",
    "        self.text = [tokenizer(text = x, padding = 'max_length', max_length = 45, truncation = True, return_tensors = 'pt') for x in df['Keywords']]\n",
    "        self.eye = df['eye']\n",
    "        self.labels = torch.tensor(df[['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']].to_numpy()).float()\n",
    "        self.img_dir = [IMG_PATH + x for x in df['Image']]\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "        self.images = [feature_extractor(torchvision.io.read_image(x).float()/255.0) for x in self.img_dir]\n",
    "        #self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        batch_imgs = self.images[idx]\n",
    "        if(self.augmentation is not None):\n",
    "            batch_imgs = self.augmentation(batch_imgs)\n",
    "        return batch_imgs, self.text[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ODIRDatasetMM(train_df, IMG_PATH, augmentation = augmentation)\n",
    "val_dataset   = ODIRDatasetMM(val_df, IMG_PATH)\n",
    "test_dataset  = ODIRDatasetMM(test_df, IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size= BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrastive learning on training data finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare two models: BERT vs ConvNext, try to compute contrastive losses\n",
    "class ContrastiveLearning(nn.Module):\n",
    "    def __init__(self, drop_prob = 0.4):\n",
    "        super().__init__()\n",
    "        self.img_model = DeiTModel.from_pretrained(\"facebook/deit-base-patch16-224\")                                      \n",
    "        # self.img_model.classifier = nn.Sequential(\n",
    "        #     nn.Linear(1536, 768)\n",
    "        # )\n",
    "\n",
    "        self.txt_model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'roberta-base')         #output 768 features\n",
    "        \n",
    "        # image model classification head\n",
    "        self.fc1 = nn.Linear(768, 768)\n",
    "        self.fc2 = nn.Linear(768, 768)\n",
    "        self.img_head = nn.Linear(768, 8)\n",
    "        \n",
    "        # text model classification ehad\n",
    "        self.fc3 = nn.Linear(768, 768)\n",
    "        self.fc4 = nn.Linear(768, 768)\n",
    "        self.text_head = nn.Linear(768, 8)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, img_input = None, input_ids = None, attn_mask = None, contrastive = False):#, text_class = False):\n",
    "        if(contrastive):\n",
    "            # pretraining\n",
    "            out_txt = self.txt_model(input_ids, attn_mask)\n",
    "            out_img = self.img_model(img_input)\n",
    "\n",
    "            out_txt_ret = F.normalize(out_txt['last_hidden_state'], p = 2.0, dim = 1)\n",
    "            out_img_ret = F.normalize(out_img['last_hidden_state'], p = 2.0, dim = 1)\n",
    "\n",
    "            #img path\n",
    "            resi_img = out_img['pooler_output']\n",
    "            img_route_out = self.dropout_layer(out_img['pooler_output'])\n",
    "            img_route_out = F.relu(self.fc1(img_route_out))\n",
    "            img_route_out = self.dropout_layer(img_route_out)\n",
    "            img_route_out = F.relu(self.fc2(img_route_out))\n",
    "            img_route_out = img_route_out + resi_img\n",
    "\n",
    "            img_route_out = F.sigmoid(self.img_head(img_route_out))\n",
    "\n",
    "            #text path\n",
    "            resi_txt = out_txt['pooler_output']\n",
    "            txt_route_out = self.dropout_layer(out_txt['pooler_output'])\n",
    "            txt_route_out = F.relu(self.fc3(txt_route_out))\n",
    "            txt_route_out = self.dropout_layer(txt_route_out)\n",
    "            txt_route_out = F.relu(self.fc4(txt_route_out))\n",
    "            txt_route_out = txt_route_out + resi_txt\n",
    "\n",
    "            txt_route_out = F.sigmoid(self.text_head(txt_route_out))\n",
    "            return out_img_ret, out_txt_ret, img_route_out, txt_route_out\n",
    "\n",
    "            #return out_img, out_txt\n",
    "        else:\n",
    "            #if(not text_class):\n",
    "            out = self.img_model(img_input)['pooler_output']\n",
    "            resi_img = out\n",
    "            img_route_out = self.dropout_layer(out)\n",
    "            img_route_out = F.relu(self.fc1(img_route_out))\n",
    "            img_route_out = self.dropout_layer(img_route_out)\n",
    "            img_route_out = F.relu(self.fc2(img_route_out))\n",
    "            img_route_out = img_route_out + resi_img\n",
    "\n",
    "            out = F.sigmoid(self.img_head(out))\n",
    "            return out\n",
    "            # else:\n",
    "            #     out = self.txt_model(input_ids, attn_mask)['pooler_output']\n",
    "            #     resi = out\n",
    "            #     out = self.dropout_layer(out)\n",
    "            #     out = F.relu(self.fc2(out))\n",
    "            #     out = resi + out\n",
    "\n",
    "            #     out = F.sigmoid(self.text_head(out))\n",
    "            #     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(ten1, ten2, temperature = nn.Parameter(torch.tensor(.25).to(device))):    #...\n",
    "    #steps = hadamard product\n",
    "    # trivial for loop \n",
    "    sim = torch.einsum('i d, j d -> i j', ten1, ten2) * temperature.exp()\n",
    "    labels = torch.arange(ten1.size(0), device = device)\n",
    "    loss = (F.cross_entropy(sim, labels) + F.cross_entropy(sim.t(), labels)) / 2\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    '''Adapted from HobbitLong (www.github.com/HobbitLong/SupContrast)'''\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device = (torch.device('cuda')\n",
    "                  if features.is_cuda\n",
    "                  else torch.device('cpu'))\n",
    "\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type vit to instantiate a model of type deit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at facebook/deit-base-patch16-224 were not used when initializing DeiTModel: ['vit.encoder.layer.5.attention.attention.query.weight', 'vit.encoder.layer.5.attention.attention.key.bias', 'vit.encoder.layer.2.output.dense.bias', 'vit.encoder.layer.1.attention.attention.key.bias', 'vit.encoder.layer.1.attention.attention.value.weight', 'vit.encoder.layer.7.layernorm_before.weight', 'vit.encoder.layer.3.attention.output.dense.bias', 'vit.encoder.layer.3.layernorm_before.bias', 'vit.encoder.layer.10.attention.output.dense.weight', 'vit.encoder.layer.2.layernorm_after.weight', 'vit.encoder.layer.11.attention.attention.key.bias', 'vit.encoder.layer.8.output.dense.weight', 'vit.encoder.layer.9.layernorm_before.bias', 'vit.encoder.layer.0.attention.output.dense.weight', 'vit.encoder.layer.10.layernorm_before.bias', 'vit.encoder.layer.1.output.dense.weight', 'vit.encoder.layer.3.intermediate.dense.weight', 'vit.encoder.layer.8.attention.output.dense.weight', 'vit.encoder.layer.9.attention.attention.query.weight', 'vit.encoder.layer.3.attention.attention.key.weight', 'vit.encoder.layer.6.attention.attention.value.bias', 'vit.encoder.layer.10.attention.attention.key.bias', 'vit.encoder.layer.4.attention.attention.value.weight', 'vit.encoder.layer.5.attention.attention.query.bias', 'vit.encoder.layer.0.attention.attention.value.weight', 'vit.encoder.layer.5.attention.attention.key.weight', 'vit.encoder.layer.2.layernorm_after.bias', 'vit.encoder.layer.9.layernorm_after.weight', 'vit.encoder.layer.0.attention.attention.key.weight', 'vit.encoder.layer.10.intermediate.dense.weight', 'vit.encoder.layer.3.attention.attention.query.bias', 'vit.encoder.layer.8.attention.attention.query.bias', 'vit.encoder.layer.10.intermediate.dense.bias', 'vit.encoder.layer.2.attention.attention.key.weight', 'vit.encoder.layer.2.layernorm_before.weight', 'vit.encoder.layer.0.layernorm_after.bias', 'vit.encoder.layer.3.layernorm_after.bias', 'vit.encoder.layer.3.layernorm_before.weight', 'vit.encoder.layer.5.attention.output.dense.bias', 'vit.encoder.layer.5.output.dense.bias', 'vit.encoder.layer.10.output.dense.bias', 'vit.encoder.layer.9.attention.attention.query.bias', 'vit.encoder.layer.11.layernorm_before.bias', 'vit.encoder.layer.9.attention.output.dense.weight', 'vit.encoder.layer.11.attention.output.dense.weight', 'vit.encoder.layer.3.attention.attention.query.weight', 'vit.encoder.layer.10.attention.attention.value.bias', 'vit.embeddings.position_embeddings', 'vit.encoder.layer.1.layernorm_after.bias', 'vit.encoder.layer.6.intermediate.dense.bias', 'vit.encoder.layer.5.output.dense.weight', 'vit.encoder.layer.1.attention.attention.key.weight', 'vit.encoder.layer.2.output.dense.weight', 'vit.encoder.layer.0.attention.attention.query.weight', 'vit.encoder.layer.6.attention.attention.query.weight', 'vit.encoder.layer.6.output.dense.weight', 'vit.encoder.layer.0.output.dense.weight', 'vit.encoder.layer.3.output.dense.weight', 'vit.encoder.layer.8.attention.attention.value.weight', 'vit.encoder.layer.6.attention.output.dense.weight', 'vit.encoder.layer.11.attention.attention.value.weight', 'vit.encoder.layer.11.intermediate.dense.bias', 'vit.encoder.layer.2.layernorm_before.bias', 'vit.encoder.layer.4.layernorm_after.bias', 'vit.encoder.layer.5.layernorm_before.bias', 'vit.encoder.layer.11.output.dense.weight', 'vit.encoder.layer.4.output.dense.weight', 'vit.encoder.layer.7.attention.attention.value.weight', 'vit.encoder.layer.4.attention.attention.value.bias', 'vit.encoder.layer.5.attention.output.dense.weight', 'vit.encoder.layer.0.layernorm_before.weight', 'vit.encoder.layer.8.layernorm_after.weight', 'vit.encoder.layer.4.layernorm_before.weight', 'vit.encoder.layer.6.layernorm_before.bias', 'vit.encoder.layer.4.output.dense.bias', 'vit.encoder.layer.9.output.dense.bias', 'vit.encoder.layer.4.intermediate.dense.weight', 'vit.encoder.layer.7.intermediate.dense.bias', 'vit.encoder.layer.2.attention.attention.value.bias', 'vit.encoder.layer.6.attention.attention.key.bias', 'vit.encoder.layer.4.layernorm_after.weight', 'vit.encoder.layer.6.attention.attention.query.bias', 'vit.encoder.layer.5.attention.attention.value.weight', 'vit.encoder.layer.11.layernorm_before.weight', 'vit.encoder.layer.10.layernorm_after.weight', 'vit.encoder.layer.5.intermediate.dense.weight', 'vit.encoder.layer.7.attention.attention.key.weight', 'vit.encoder.layer.8.intermediate.dense.bias', 'vit.encoder.layer.11.attention.attention.value.bias', 'classifier.weight', 'vit.encoder.layer.10.attention.attention.query.weight', 'vit.encoder.layer.6.intermediate.dense.weight', 'vit.encoder.layer.11.attention.output.dense.bias', 'vit.encoder.layer.10.attention.output.dense.bias', 'vit.encoder.layer.10.attention.attention.query.bias', 'vit.encoder.layer.0.layernorm_after.weight', 'vit.encoder.layer.2.attention.attention.query.weight', 'vit.encoder.layer.2.intermediate.dense.bias', 'vit.encoder.layer.7.attention.output.dense.bias', 'vit.encoder.layer.3.layernorm_after.weight', 'vit.encoder.layer.0.attention.attention.value.bias', 'vit.encoder.layer.3.attention.attention.value.bias', 'vit.encoder.layer.9.attention.output.dense.bias', 'vit.encoder.layer.8.attention.output.dense.bias', 'vit.encoder.layer.8.layernorm_before.bias', 'vit.encoder.layer.1.attention.output.dense.bias', 'vit.encoder.layer.0.intermediate.dense.bias', 'vit.encoder.layer.3.attention.output.dense.weight', 'vit.encoder.layer.9.layernorm_before.weight', 'vit.encoder.layer.8.intermediate.dense.weight', 'vit.encoder.layer.6.attention.attention.value.weight', 'vit.encoder.layer.8.attention.attention.key.bias', 'vit.encoder.layer.6.attention.attention.key.weight', 'vit.encoder.layer.1.intermediate.dense.weight', 'vit.encoder.layer.0.attention.attention.query.bias', 'vit.encoder.layer.2.attention.output.dense.bias', 'vit.encoder.layer.4.attention.attention.query.bias', 'vit.encoder.layer.11.intermediate.dense.weight', 'vit.encoder.layer.11.layernorm_after.bias', 'vit.encoder.layer.8.attention.attention.query.weight', 'vit.encoder.layer.4.layernorm_before.bias', 'vit.embeddings.patch_embeddings.projection.bias', 'vit.encoder.layer.8.layernorm_before.weight', 'vit.encoder.layer.1.intermediate.dense.bias', 'vit.layernorm.bias', 'vit.encoder.layer.3.intermediate.dense.bias', 'vit.encoder.layer.2.attention.attention.query.bias', 'vit.embeddings.patch_embeddings.projection.weight', 'vit.encoder.layer.5.layernorm_after.bias', 'vit.encoder.layer.6.attention.output.dense.bias', 'vit.encoder.layer.9.output.dense.weight', 'vit.encoder.layer.2.attention.output.dense.weight', 'vit.encoder.layer.4.attention.output.dense.bias', 'vit.encoder.layer.7.attention.attention.value.bias', 'vit.encoder.layer.7.layernorm_before.bias', 'vit.encoder.layer.1.attention.output.dense.weight', 'vit.encoder.layer.4.attention.attention.key.bias', 'vit.encoder.layer.9.intermediate.dense.weight', 'vit.encoder.layer.7.attention.output.dense.weight', 'vit.encoder.layer.7.layernorm_after.bias', 'vit.encoder.layer.1.attention.attention.query.weight', 'vit.encoder.layer.1.layernorm_before.bias', 'vit.encoder.layer.1.layernorm_after.weight', 'vit.encoder.layer.1.layernorm_before.weight', 'vit.encoder.layer.11.attention.attention.key.weight', 'vit.encoder.layer.5.layernorm_before.weight', 'vit.encoder.layer.7.attention.attention.query.weight', 'vit.encoder.layer.8.layernorm_after.bias', 'vit.encoder.layer.4.intermediate.dense.bias', 'vit.encoder.layer.10.layernorm_before.weight', 'vit.encoder.layer.4.attention.attention.key.weight', 'vit.encoder.layer.1.output.dense.bias', 'vit.encoder.layer.9.attention.attention.value.bias', 'vit.encoder.layer.10.attention.attention.value.weight', 'vit.encoder.layer.7.intermediate.dense.weight', 'vit.encoder.layer.1.attention.attention.value.bias', 'vit.encoder.layer.2.attention.attention.key.bias', 'vit.encoder.layer.5.intermediate.dense.bias', 'vit.encoder.layer.0.output.dense.bias', 'vit.embeddings.cls_token', 'vit.encoder.layer.3.attention.attention.value.weight', 'vit.encoder.layer.9.attention.attention.value.weight', 'vit.encoder.layer.9.layernorm_after.bias', 'vit.encoder.layer.0.intermediate.dense.weight', 'vit.encoder.layer.6.output.dense.bias', 'vit.encoder.layer.5.layernorm_after.weight', 'vit.encoder.layer.10.output.dense.weight', 'vit.encoder.layer.9.attention.attention.key.bias', 'vit.encoder.layer.9.attention.attention.key.weight', 'vit.encoder.layer.4.attention.attention.query.weight', 'vit.encoder.layer.8.attention.attention.value.bias', 'vit.encoder.layer.0.layernorm_before.bias', 'vit.encoder.layer.11.attention.attention.query.bias', 'vit.encoder.layer.3.attention.attention.key.bias', 'vit.encoder.layer.8.output.dense.bias', 'vit.encoder.layer.9.intermediate.dense.bias', 'vit.encoder.layer.10.attention.attention.key.weight', 'vit.encoder.layer.7.output.dense.bias', 'vit.encoder.layer.8.attention.attention.key.weight', 'vit.encoder.layer.0.attention.output.dense.bias', 'vit.encoder.layer.0.attention.attention.key.bias', 'vit.encoder.layer.2.intermediate.dense.weight', 'vit.encoder.layer.7.attention.attention.query.bias', 'vit.encoder.layer.3.output.dense.bias', 'vit.encoder.layer.11.layernorm_after.weight', 'vit.encoder.layer.2.attention.attention.value.weight', 'vit.encoder.layer.5.attention.attention.value.bias', 'vit.encoder.layer.6.layernorm_after.bias', 'vit.encoder.layer.4.attention.output.dense.weight', 'vit.encoder.layer.7.attention.attention.key.bias', 'vit.encoder.layer.1.attention.attention.query.bias', 'vit.encoder.layer.7.output.dense.weight', 'vit.layernorm.weight', 'vit.encoder.layer.11.output.dense.bias', 'classifier.bias', 'vit.encoder.layer.6.layernorm_before.weight', 'vit.encoder.layer.11.attention.attention.query.weight', 'vit.encoder.layer.7.layernorm_after.weight', 'vit.encoder.layer.6.layernorm_after.weight', 'vit.encoder.layer.10.layernorm_after.bias']\n",
      "- This IS expected if you are initializing DeiTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DeiTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DeiTModel were not initialized from the model checkpoint at facebook/deit-base-patch16-224 and are newly initialized: ['embeddings.patch_embeddings.projection.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.7.layernorm_after.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'pooler.dense.bias', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.9.intermediate.dense.weight', 'layernorm.weight', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.5.attention.attention.value.weight', 'embeddings.cls_token', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.5.layernorm_before.weight', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.layernorm_before.weight', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.5.layernorm_after.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.9.attention.attention.value.bias', 'layernorm.bias', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.8.layernorm_after.weight', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.0.attention.attention.value.bias', 'embeddings.patch_embeddings.projection.bias', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.8.attention.attention.key.weight', 'embeddings.distillation_token', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.6.layernorm_after.weight', 'embeddings.position_embeddings', 'encoder.layer.1.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.10.attention.attention.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cache found in C:\\Users\\krish/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = ContrastiveLearning().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlignModel, AlignProcessor, AlignConfig\n",
    "\n",
    "m = AlignModel(AlignConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlignModel(\n",
       "  (text_model): AlignTextModel(\n",
       "    (embeddings): AlignTextEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): AlignTextEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x AlignTextLayer(\n",
       "          (attention): AlignTextAttention(\n",
       "            (self): AlignTextSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): AlignTextSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): AlignTextIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): AlignTextOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): AlignTextPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (vision_model): AlignVisionModel(\n",
       "    (embeddings): AlignVisionEmbeddings(\n",
       "      (padding): ZeroPad2d((0, 1, 0, 1))\n",
       "      (convolution): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=valid, bias=False)\n",
       "      (batchnorm): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (activation): SiLUActivation()\n",
       "    )\n",
       "    (encoder): AlignVisionEncoder(\n",
       "      (blocks): ModuleList(\n",
       "        (0): AlignVisionBlock(\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=64, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(32, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): AlignVisionBlock(\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=32, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(32, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(32, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.0036363636363636364, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): AlignVisionBlock(\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=32, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(32, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(32, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.007272727272727273, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): AlignVisionBlock(\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=32, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(32, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(32, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.01090909090909091, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=valid, groups=192, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(192, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(48, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.014545454545454545, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=288, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(288, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(48, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.01818181818181818, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=288, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(288, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(48, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.02181818181818182, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=288, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(288, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(48, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.025454545454545455, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=288, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(288, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(48, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.02909090909090909, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=288, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(288, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(48, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.03272727272727273, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=288, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(288, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(48, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.03636363636363636, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=valid, groups=288, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(288, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(288, 80, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(80, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.04, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=480, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(480, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(80, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.04363636363636364, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=480, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(480, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(80, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.04727272727272727, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=480, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(480, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(80, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.05090909090909091, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=480, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(480, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(80, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.05454545454545454, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=480, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(480, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(80, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.05818181818181818, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=480, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(480, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(80, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.06181818181818183, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(480, 480, kernel_size=(3, 3), stride=(2, 2), padding=valid, groups=480, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(480, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(160, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.06545454545454546, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=960, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(960, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(160, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.06909090909090909, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=960, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(960, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(160, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.07272727272727272, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=960, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(960, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(160, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.07636363636363637, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=960, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(960, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(160, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.08, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=960, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(960, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(160, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.08363636363636365, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (24): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=960, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(960, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(160, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.08727272727272728, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (25): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=960, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(960, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(160, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.09090909090909091, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (26): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=960, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(960, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(160, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.09454545454545454, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (27): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=960, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(960, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(160, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.09818181818181819, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (28): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=960, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(960, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(960, 224, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(224, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.10181818181818182, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (29): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(1344, 1344, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=1344, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(1344, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(1344, 56, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(56, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(224, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.10545454545454547, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (30): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(1344, 1344, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=1344, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(1344, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(1344, 56, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(56, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(224, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.10909090909090909, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (31): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(1344, 1344, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=1344, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(1344, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(1344, 56, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(56, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(224, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.11272727272727273, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (32): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(1344, 1344, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=1344, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(1344, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(1344, 56, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(56, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(224, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.11636363636363636, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (33): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(1344, 1344, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=1344, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(1344, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(1344, 56, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(56, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(224, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.12000000000000001, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (34): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(1344, 1344, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=1344, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(1344, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(1344, 56, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(56, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(224, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.12363636363636366, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (35): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(1344, 1344, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=1344, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(1344, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(1344, 56, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(56, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(224, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.12727272727272726, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (36): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(1344, 1344, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=1344, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(1344, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(1344, 56, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(56, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(224, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.13090909090909092, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (37): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(1344, 1344, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=1344, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(1344, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(1344, 56, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(56, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(224, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.13454545454545455, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (38): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(1344, 1344, kernel_size=(5, 5), stride=(2, 2), padding=valid, groups=1344, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(1344, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(1344, 56, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(56, 1344, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(1344, 384, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(384, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.13818181818181818, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (39): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(2304, 2304, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=2304, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(2304, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(384, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.14181818181818184, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (40): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(2304, 2304, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=2304, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(2304, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(384, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.14545454545454545, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (41): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(2304, 2304, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=2304, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(2304, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(384, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.1490909090909091, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (42): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(2304, 2304, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=2304, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(2304, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(384, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.15272727272727274, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (43): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(2304, 2304, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=2304, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(2304, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(384, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.15636363636363634, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (44): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(2304, 2304, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=2304, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(2304, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(384, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.16, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (45): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(2304, 2304, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=2304, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(2304, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(384, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.16363636363636364, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (46): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(2304, 2304, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=2304, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(2304, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(384, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.1672727272727273, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (47): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(2304, 2304, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=2304, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(2304, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(384, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.17090909090909093, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (48): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(2304, 2304, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=2304, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(2304, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(384, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.17454545454545456, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (49): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(2304, 2304, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=2304, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(2304, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(384, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.1781818181818182, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (50): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((1, 2, 1, 2))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(2304, 2304, kernel_size=(5, 5), stride=(1, 1), padding=same, groups=2304, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(2304, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(384, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.18181818181818182, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (51): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=2304, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(2304, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(2304, 640, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(640, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.18545454545454548, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (52): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(640, 3840, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=3840, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(3840, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(3840, 160, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(160, 3840, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(3840, 640, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(640, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.1890909090909091, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (53): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(640, 3840, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=3840, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(3840, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(3840, 160, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(160, 3840, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(3840, 640, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(640, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.19272727272727275, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (54): AlignVisionBlock(\n",
       "          (expansion): AlignVisionExpansionLayer(\n",
       "            (expand_conv): Conv2d(640, 3840, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (expand_bn): BatchNorm2d(3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (expand_act): SiLUActivation()\n",
       "          )\n",
       "          (depthwise_conv): AlignVisionDepthwiseLayer(\n",
       "            (depthwise_conv_pad): ZeroPad2d((0, 1, 0, 1))\n",
       "            (depthwise_conv): AlignVisionDepthwiseConv2d(3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=3840, bias=False)\n",
       "            (depthwise_norm): BatchNorm2d(3840, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (depthwise_act): SiLUActivation()\n",
       "          )\n",
       "          (squeeze_excite): AlignVisionSqueezeExciteLayer(\n",
       "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "            (reduce): Conv2d(3840, 160, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (expand): Conv2d(160, 3840, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "            (act_reduce): SiLUActivation()\n",
       "            (act_expand): Sigmoid()\n",
       "          )\n",
       "          (projection): AlignVisionFinalBlockLayer(\n",
       "            (project_conv): Conv2d(3840, 640, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "            (project_bn): BatchNorm2d(640, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "            (dropout): Dropout(p=0.19636363636363638, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): AvgPool2d(kernel_size=2560, stride=2560, padding=0)\n",
       "  )\n",
       "  (text_projection): Linear(in_features=768, out_features=640, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 153/153 [01:07<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Train Loss: 7.1651, Train Accuracy: 0.8723 img, 0.9977 txt\n",
      "Epoch [1/15], Val Loss: 7.1367, Val Accuracy: 0.8749 img, 0.9997 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 153/153 [01:07<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Train Loss: 7.1242, Train Accuracy: 0.8735 img, 0.9998 txt\n",
      "Epoch [2/15], Val Loss: 7.1638, Val Accuracy: 0.8747 img, 0.9984 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 153/153 [01:08<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/15], Train Loss: 7.1119, Train Accuracy: 0.8756 img, 0.9990 txt\n",
      "Epoch [3/15], Val Loss: 7.1027, Val Accuracy: 0.8748 img, 0.9997 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 153/153 [01:08<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/15], Train Loss: 7.0751, Train Accuracy: 0.8745 img, 0.9992 txt\n",
      "Epoch [4/15], Val Loss: 7.0631, Val Accuracy: 0.8741 img, 0.9997 txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 9/153 [00:04<01:06,  2.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m input_id \u001b[38;5;241m=\u001b[39m train_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# logits_per_image, logits_per_text\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m out_img, out_txt, predictions_img, predictions_text \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(train_image, input_id, mask, contrastive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#predictions_img =  model.forward(train_image, input_id, mask, contrastive = False)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m acc_train_img(predictions_img, train_label)\n",
      "Cell \u001b[1;32mIn[43], line 28\u001b[0m, in \u001b[0;36mContrastiveLearning.forward\u001b[1;34m(self, img_input, input_ids, attn_mask, contrastive)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(contrastive):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# pretraining\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     out_txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtxt_model(input_ids, attn_mask)\n\u001b[1;32m---> 28\u001b[0m     out_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_model(img_input)\n\u001b[0;32m     30\u001b[0m     out_txt_ret \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(out_txt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m'\u001b[39m], p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     31\u001b[0m     out_img_ret \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(out_img[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m'\u001b[39m], p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\transformers\\models\\deit\\modeling_deit.py:530\u001b[0m, in \u001b[0;36mDeiTModel.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    526\u001b[0m     pixel_values \u001b[39m=\u001b[39m pixel_values\u001b[39m.\u001b[39mto(expected_dtype)\n\u001b[0;32m    528\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(pixel_values, bool_masked_pos\u001b[39m=\u001b[39mbool_masked_pos)\n\u001b[1;32m--> 530\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[0;32m    531\u001b[0m     embedding_output,\n\u001b[0;32m    532\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[0;32m    533\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    534\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    535\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m    536\u001b[0m )\n\u001b[0;32m    537\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    538\u001b[0m sequence_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm(sequence_output)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\transformers\\models\\deit\\modeling_deit.py:373\u001b[0m, in \u001b[0;36mDeiTEncoder.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    367\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    368\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    369\u001b[0m         hidden_states,\n\u001b[0;32m    370\u001b[0m         layer_head_mask,\n\u001b[0;32m    371\u001b[0m     )\n\u001b[0;32m    372\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 373\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(hidden_states, layer_head_mask, output_attentions)\n\u001b[0;32m    375\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    377\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\transformers\\models\\deit\\modeling_deit.py:324\u001b[0m, in \u001b[0;36mDeiTLayer.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39m# in DeiT, layernorm is also applied after self-attention\u001b[39;00m\n\u001b[0;32m    323\u001b[0m layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm_after(hidden_states)\n\u001b[1;32m--> 324\u001b[0m layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(layer_output)\n\u001b[0;32m    326\u001b[0m \u001b[39m# second residual connection is done here\u001b[39;00m\n\u001b[0;32m    327\u001b[0m layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(layer_output, hidden_states)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\transformers\\models\\deit\\modeling_deit.py:269\u001b[0m, in \u001b[0;36mDeiTIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> 269\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[0;32m    270\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    272\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
    "weights = torch.tensor([.7,1.2,1.5,1.5,1.5,1.7, 1.5, 1.2]).to(device)\n",
    "criterion_text = nn.BCELoss(weights)\n",
    "criterion_image = nn.BCELoss(weights)\n",
    "cont_loss = contrastive_loss\n",
    "AVERAGING = 'micro'\n",
    "acc_train_img = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "acc_train_text = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "acc_val_img   = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)\n",
    "acc_val_text   = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)\n",
    "myloss_img = SupConLoss()\n",
    "\n",
    "train_text = False\n",
    "EPOCHS = 15\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "\n",
    "      total_acc_train = 0\n",
    "      total_loss_train = 0\n",
    "\n",
    "      for train_image, train_text, train_label in tqdm(train_dataloader):\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          train_label = train_label.to(device)\n",
    "          train_image = train_image.to(device)\n",
    "          mask = train_text['attention_mask'].to(device)\n",
    "          input_id = train_text['input_ids'].squeeze(1).to(device)\n",
    "          \n",
    "          # logits_per_image, logits_per_text\n",
    "          out_img, out_txt, predictions_img, predictions_text = model.forward(train_image, input_id, mask, contrastive = True)\n",
    "          #predictions_img =  model.forward(train_image, input_id, mask, contrastive = False)\n",
    "          acc_train_img(predictions_img, train_label)\n",
    "          acc_train_text(predictions_text, train_label)\n",
    "\n",
    "          closs_masks = torch.floor(pairwise_cosine_similarity(train_label, zero_diagonal= False))\n",
    "\n",
    "\n",
    "          closs = cont_loss(torch.mean(out_img, axis = 1), torch.mean(out_txt, axis = 1))\n",
    "          text_loss = criterion_text(predictions_text, train_label)\n",
    "          img_loss = criterion_image(predictions_img, train_label)\n",
    "          # if(train_text):\n",
    "          batch_loss = closs*2. + text_loss*2. + img_loss*1.\n",
    "          # else:\n",
    "          #   batch_loss = myloss_img(out_img) + img_loss\n",
    "          batch_loss = closs*2. + text_loss*2. + img_loss*.5\n",
    "          batch_loss.backward()\n",
    "          optimizer.step()\n",
    "          total_loss_train += batch_loss.item()\n",
    "      \n",
    "      total_acc_val = 0\n",
    "      total_loss_val = 0\n",
    "\n",
    "      with torch.no_grad():\n",
    "\n",
    "          for val_image, val_text, val_label in val_dataloader:\n",
    "\n",
    "              val_label = val_label.to(device)\n",
    "              val_image = val_image.to(device)\n",
    "              mask = val_text['attention_mask'].to(device)\n",
    "              input_id = val_text['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              out_img, out_txt, predictions_img, predictions_text = model.forward(val_image, input_id, mask, contrastive = True)\n",
    "              #predictions_img =  model.forward(val_image, input_id, mask, contrastive = False)\n",
    "              acc_val_img(predictions_img, val_label)\n",
    "              acc_val_text(predictions_text, val_label)\n",
    "\n",
    "              closs_masks = torch.floor(pairwise_cosine_similarity(val_label, zero_diagonal= False))\n",
    "\n",
    "\n",
    "              closs = cont_loss(torch.mean(out_img, axis = 1), torch.mean(out_txt, axis = 1))\n",
    "              text_loss = criterion_text(predictions_text, val_label)\n",
    "              img_loss = criterion_image(predictions_img, val_label)\n",
    "              # if(train_text):\n",
    "              batch_loss = closs*2. + text_loss*2. + img_loss*1.\n",
    "              # else:\n",
    "              #   batch_loss = closs\n",
    "              batch_loss = closs*2. + text_loss*2. + img_loss*.5\n",
    "              total_loss_val += batch_loss.item()\n",
    "\n",
    "             # acc_val(predictions, val_label)\n",
    "              \n",
    "      \n",
    "      avg_train_loss = total_loss_train/len(train_df)\n",
    "    #   train_accuracy = total_acc_train/len(train_df)\n",
    "\n",
    "      avg_val_loss = total_loss_val/len(val_df)\n",
    "    #   val_accuracy = total_acc_val/len(dev_df)\n",
    "\n",
    "      # if(acc_train_text.compute() >= 0.99):\n",
    "      #   print(\"Fixing Text model component!\")\n",
    "      #   train_text = False\n",
    "      #   model.txt_model.requires_grad_(False)\n",
    "      #   model.text_head.requires_grad_(False)\n",
    "      # else:\n",
    "      #   print(\"Unfreezing text model component\")\n",
    "      #   train_text = True\n",
    "      #   model.txt_model.requires_grad_(True)\n",
    "      #   model.text_head.requires_grad_(True)\n",
    "\n",
    "      print(\"Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.4f} img, {:.4f} txt\".format(epoch_num+1, EPOCHS, avg_train_loss*BATCH_SIZE, acc_train_img.compute(), acc_train_text.compute()))\n",
    "      print(\"Epoch [{}/{}], Val Loss: {:.4f}, Val Accuracy: {:.4f} img, {:.4f} txt\".format(epoch_num+1, EPOCHS, avg_val_loss*BATCH_SIZE, acc_val_img.compute(), acc_val_text.compute()))\n",
    "\n",
    "      acc_train_img.reset()\n",
    "      acc_train_text.reset()\n",
    "      acc_val_img.reset\n",
    "      acc_val_text.reset()\n",
    "      torch.save(model.state_dict(), './' + 'checkpoint' + '.pt' )\n",
    "\n",
    "torch.save(model.state_dict(), './' + 'finetuned' + '.pt' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"finetuned.pt\", 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]c:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n",
      "100%|| 13/13 [00:02<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.794288\n",
      "Prec: 0.626230\n",
      "Recall: 0.635\n",
      "F1-score: 0.627\n",
      "F-Beta-score: 0.626\n",
      "Kappa: 0.000\n",
      "AUC: 0.816\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss(torch.tensor([0.5, 1, 5, 5, 5, 6, 5, 1]).float().to(device))\n",
    "\n",
    "test_loss = 0\n",
    "test_acc  = 0\n",
    "\n",
    "AVERAGING = 'weighted'\n",
    "PREC = torchmetrics.classification.MultilabelPrecision(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "ACC = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "REC = torchmetrics.classification.MultilabelRecall(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "F1_SCORE = torchmetrics.classification.MultilabelF1Score(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "F_BETA_SCORE = torchmetrics.classification.MultilabelFBetaScore(beta = 0.8, num_classes = 8, num_labels = 8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "KAPPA = torchmetrics.classification.MulticlassCohenKappa(8).to(device)#, validate_args = False)\n",
    "AUC = torchmetrics.classification.MultilabelAUROC(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "\n",
    "for train_image, train_text, train_label in tqdm(test_dataloader): \n",
    "    with torch.no_grad():\n",
    "        train_label = train_label.to(device)\n",
    "        train_image = train_image.to(device)\n",
    "        mask = train_text['attention_mask'].to(device)\n",
    "        input_id = train_text['input_ids'].squeeze(1).to(device)\n",
    "        \n",
    "        # logits_per_image, logits_per_text\n",
    "        #out_img, out_txt = model.forward(train_image, input_id, mask, contrastive = True)\n",
    "        predictions = model.forward(train_image, contrastive = False)\n",
    "\n",
    "\n",
    "\n",
    "        train_label = train_label.long()\n",
    "        PREC(predictions, train_label)\n",
    "        ACC(predictions, train_label)\n",
    "        REC(predictions, train_label)\n",
    "        F1_SCORE(predictions, train_label)\n",
    "        F_BETA_SCORE(predictions, train_label)\n",
    "        KAPPA(predictions, train_label)\n",
    "        AUC(predictions, train_label)\n",
    "\n",
    "\n",
    "add_prec = PREC.compute()\n",
    "add_acc = ACC.compute()\n",
    "add_rec = REC.compute()\n",
    "add_f1 = F1_SCORE.compute()\n",
    "add_fbeta = F_BETA_SCORE.compute()\n",
    "add_kappa = KAPPA.compute()\n",
    "add_auc = AUC.compute()\n",
    "\n",
    "avg_test_loss = test_loss/len(test_df)*BATCH_SIZE\n",
    "avg_test_acc  = test_acc /len(test_df)\n",
    "\n",
    "print(\"Acc: {:3f}\\nPrec: {:3f}\\nRecall: {:.3f}\\nF1-score: {:.3f}\\nF-Beta-score: {:.3f}\\nKappa: {:.3f}\\nAUC: {:.3f}\".format(add_acc, add_prec,add_rec, add_f1, add_fbeta, add_kappa, add_auc))\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('torchnew')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aae2c75324e7fdf5ebd22146e2daffaa477f8ea149f0e685be4c317c2939a685"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
