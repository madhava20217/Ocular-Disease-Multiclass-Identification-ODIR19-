{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import logging\n",
    "import gdown\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "import albumentations as A\n",
    "\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from scipy.io import loadmat\n",
    "from sklearn.manifold import TSNE\n",
    "from torchmetrics.classification import MulticlassF1Score, JaccardIndex, MulticlassPrecision, MulticlassRecall, MulticlassAveragePrecision\n",
    "import pandas as pd\n",
    "from torchinfo import torchinfo\n",
    "\n",
    "from transformers import ConvNextV2Model, BertModel, BertTokenizer, ViTModel, ViTConfig\n",
    "from transformers import AutoTokenizer, AutoModel, RobertaTokenizer, CLIPModel, CLIPTokenizer, CLIPProcessor\n",
    "from transformers import DeiTConfig, DeiTFeatureExtractor, DeiTImageProcessor, DeiTModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import AlignModel, AlignProcessor, AlignConfig, AlignVisionConfig, AlignTextConfig\n",
    "from transformers import CLIPModel, CLIPProcessor, CLIPConfig, CLIPTextConfig, CLIPVisionConfig, CLIPImageProcessor\n",
    "\n",
    "from torchmetrics.functional import pairwise_cosine_similarity\n",
    "from torchmetrics.classification import MultilabelAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    \n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '../Datasets/ocular-disease-recognition-odir5k/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = ROOT_DIR + 'dataset_single_eye.csv'\n",
    "TEST_CSV = ROOT_DIR + 'TESTING_dataset_single_eye.csv'\n",
    "IMG_PATH = ROOT_DIR + 'preprocessed_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.io.read_image(IMG_PATH + '0_left.jpg').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df = pd.read_csv(CSV_PATH)\n",
    "test_df = pd.read_csv(TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df:pd.DataFrame):\n",
    "    df['Keywords'] = df['Keywords'].str.lower()\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: \" \".join(x.split()))\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
    "    return df\n",
    "train_val_df = preprocess_text(train_val_df)\n",
    "test_df = preprocess_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df['Patient Sex'] = train_val_df['Patient Sex'].astype('category').cat.codes\n",
    "test_df['Patient Sex'] = test_df['Patient Sex'].astype('category').cat.codes\n",
    "\n",
    "train_val_df['eye'] = train_val_df['Patient Sex'].astype('category').cat.codes\n",
    "test_df['eye'] = test_df['Patient Sex'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Sex</th>\n",
       "      <th>Image</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>eye</th>\n",
       "      <th>N</th>\n",
       "      <th>D</th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>A</th>\n",
       "      <th>H</th>\n",
       "      <th>M</th>\n",
       "      <th>O</th>\n",
       "      <th>NOT DECISIVE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>970</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>970_right.jpg</td>\n",
       "      <td>cataract</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>127_left.jpg</td>\n",
       "      <td>proliferative diabetic retinopathy，hypertensiv...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>850</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>850_right.jpg</td>\n",
       "      <td>macular epiretinal membrane，moderate non proli...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>37_right.jpg</td>\n",
       "      <td>normal fundus</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4421</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>4421_right.jpg</td>\n",
       "      <td>moderate non proliferative retinopathy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5733</th>\n",
       "      <td>199</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>199_left.jpg</td>\n",
       "      <td>branch retinal vein occlusion</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>516</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>516_right.jpg</td>\n",
       "      <td>moderate non proliferative retinopathy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5735</th>\n",
       "      <td>4603</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>4603_left.jpg</td>\n",
       "      <td>severe nonproliferative retinopathy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736</th>\n",
       "      <td>2132</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>2132_right.jpg</td>\n",
       "      <td>normal fundus</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5737</th>\n",
       "      <td>4487</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>4487_right.jpg</td>\n",
       "      <td>mild nonproliferative retinopathy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5738 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  Patient Age  Patient Sex           Image  \\\n",
       "0      970           56            0   970_right.jpg   \n",
       "1      127           52            1    127_left.jpg   \n",
       "2      850           68            1   850_right.jpg   \n",
       "3       37           41            1    37_right.jpg   \n",
       "4     4421           59            1  4421_right.jpg   \n",
       "...    ...          ...          ...             ...   \n",
       "5733   199           50            0    199_left.jpg   \n",
       "5734   516           42            1   516_right.jpg   \n",
       "5735  4603           47            0   4603_left.jpg   \n",
       "5736  2132           59            0  2132_right.jpg   \n",
       "5737  4487           55            0  4487_right.jpg   \n",
       "\n",
       "                                               Keywords  eye  N  D  G  C  A  \\\n",
       "0                                              cataract    0  0  0  0  1  0   \n",
       "1     proliferative diabetic retinopathy，hypertensiv...    1  0  1  0  0  0   \n",
       "2     macular epiretinal membrane，moderate non proli...    1  0  1  0  0  0   \n",
       "3                                         normal fundus    1  1  0  0  0  0   \n",
       "4                moderate non proliferative retinopathy    1  0  1  0  0  0   \n",
       "...                                                 ...  ... .. .. .. .. ..   \n",
       "5733                      branch retinal vein occlusion    0  0  0  0  0  0   \n",
       "5734             moderate non proliferative retinopathy    1  0  1  0  0  0   \n",
       "5735                severe nonproliferative retinopathy    0  0  1  0  0  0   \n",
       "5736                                      normal fundus    0  1  0  0  0  0   \n",
       "5737                  mild nonproliferative retinopathy    0  0  1  0  0  0   \n",
       "\n",
       "      H  M  O  NOT DECISIVE  \n",
       "0     0  0  0             0  \n",
       "1     0  0  0             0  \n",
       "2     0  0  0             0  \n",
       "3     0  0  0             0  \n",
       "4     0  0  0             0  \n",
       "...  .. .. ..           ...  \n",
       "5733  0  0  1             0  \n",
       "5734  0  0  0             0  \n",
       "5735  0  0  0             0  \n",
       "5736  0  0  0             0  \n",
       "5737  0  0  0             0  \n",
       "\n",
       "[5738 rows x 15 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_val_df['Keywords'].apply(lambda x: len(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4877, 861)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(train_val_df, test_size = 0.15, random_state= 123456)\n",
    "len(train_df), len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "rescale_transform = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.CenterCrop(IMG_SIZE),\n",
    "    torchvision.transforms.Resize(IMG_SIZE, antialias = False, interpolation = torchvision.transforms.InterpolationMode.NEAREST),\n",
    "    torchvision.transforms.Normalize(\n",
    "        timm.data.constants.IMAGENET_DEFAULT_MEAN,\n",
    "        timm.data.constants.IMAGENET_DEFAULT_STD\n",
    "    )\n",
    "])\n",
    "\n",
    "augmentation = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(p = 0.5),\n",
    "    torchvision.transforms.RandomVerticalFlip(p= 0.5),\n",
    "    # torchvision.transforms.RandomRotation(90)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56,  0],\n",
       "       [52,  1],\n",
       "       [68,  1],\n",
       "       ...,\n",
       "       [47,  0],\n",
       "       [59,  0],\n",
       "       [55,  0]], dtype=int64)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.c_[train_val_df['Patient Age'].to_numpy(), train_val_df['Patient Sex'].to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODIRDataset(Dataset) :\n",
    "    def __init__(self, df, IMG_FOLDER, extractor = rescale_transform, augmentation = None) :\n",
    "        '''\n",
    "        id : list of samples ids as string\n",
    "        '''\n",
    "        #self.text = [tokenizer(text = x, padding = 'max_length', max_length = 40, truncation = True, return_tensors = 'pt') for x in df['Keywords']]\n",
    "        sex = df['Patient Age'].to_numpy()\n",
    "        age = (df['Patient Age']/df['Patient Age'].max()).to_numpy()\n",
    "        self.feats = torch.tensor(np.c_[sex, age], requires_grad= True)\n",
    "        self.labels = torch.tensor(df[['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']].to_numpy()).float()\n",
    "        self.img_dir = [IMG_PATH + x for x in df['Image']]\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "        self.images = [extractor(torchvision.io.read_image(x)/255.0) for x in self.img_dir]\n",
    "        #self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        batch_imgs = self.images[idx]\n",
    "        if(self.augmentation is not None):\n",
    "            batch_imgs = self.augmentation(batch_imgs)\n",
    "        return batch_imgs, self.feats[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = ODIRDataset(train_df, IMG_PATH)#, augmentation = augmentation)\n",
    "# val_dataset   = ODIRDataset(val_df, IMG_PATH)\n",
    "# test_dataset  = ODIRDataset(test_df, IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"datasets.trch\", 'wb') as f:\n",
    "#     torch.save([train_dataset, val_dataset, test_dataset], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets.trch\", 'rb') as f:\n",
    "    train_dataset, val_dataset, test_dataset = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.augmentation = augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size= BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrastive learning on training data finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#odict_keys(['loss', 'logits_per_image', 'logits_per_text', 'text_embeds', 'image_embeds', 'text_model_output', 'vision_model_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNet(nn.Module):\n",
    "    def __init__(self, dims = 768, drop_prob = 0.5):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(dims, 768)\n",
    "        self.layer_2 = nn.Linear(768, 768)\n",
    "        self.layer_3 = nn.Linear(768, dims)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        resi = input\n",
    "        out = self.dropout(input)\n",
    "        out = F.relu(self.layer_1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = F.relu(self.layer_2(out))\n",
    "        out = self.dropout(out)\n",
    "        out = F.relu(self.layer_3(out))\n",
    "        out = out + resi\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\krish/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [5, 196, 192]             --\n",
       "├─PatchEmbed: 1-1                        [5, 196, 192]             --\n",
       "│    └─Conv2d: 2-1                       [5, 192, 14, 14]          147,648\n",
       "│    └─Identity: 2-2                     [5, 196, 192]             --\n",
       "├─Dropout: 1-2                           [5, 196, 192]             --\n",
       "├─Identity: 1-3                          [5, 196, 192]             --\n",
       "├─Sequential: 1-4                        [5, 196, 192]             --\n",
       "│    └─Block: 2-3                        [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-1               [5, 196, 192]             384\n",
       "│    │    └─Attention: 3-2               [5, 196, 192]             148,224\n",
       "│    │    └─Identity: 3-3                [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-4                [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-5               [5, 196, 192]             384\n",
       "│    │    └─Mlp: 3-6                     [5, 196, 192]             295,872\n",
       "│    │    └─Identity: 3-7                [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-8                [5, 196, 192]             --\n",
       "│    └─Block: 2-4                        [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-9               [5, 196, 192]             384\n",
       "│    │    └─Attention: 3-10              [5, 196, 192]             148,224\n",
       "│    │    └─Identity: 3-11               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-12               [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-13              [5, 196, 192]             384\n",
       "│    │    └─Mlp: 3-14                    [5, 196, 192]             295,872\n",
       "│    │    └─Identity: 3-15               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-16               [5, 196, 192]             --\n",
       "│    └─Block: 2-5                        [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-17              [5, 196, 192]             384\n",
       "│    │    └─Attention: 3-18              [5, 196, 192]             148,224\n",
       "│    │    └─Identity: 3-19               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-20               [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-21              [5, 196, 192]             384\n",
       "│    │    └─Mlp: 3-22                    [5, 196, 192]             295,872\n",
       "│    │    └─Identity: 3-23               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-24               [5, 196, 192]             --\n",
       "│    └─Block: 2-6                        [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-25              [5, 196, 192]             384\n",
       "│    │    └─Attention: 3-26              [5, 196, 192]             148,224\n",
       "│    │    └─Identity: 3-27               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-28               [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-29              [5, 196, 192]             384\n",
       "│    │    └─Mlp: 3-30                    [5, 196, 192]             295,872\n",
       "│    │    └─Identity: 3-31               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-32               [5, 196, 192]             --\n",
       "│    └─Block: 2-7                        [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-33              [5, 196, 192]             384\n",
       "│    │    └─Attention: 3-34              [5, 196, 192]             148,224\n",
       "│    │    └─Identity: 3-35               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-36               [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-37              [5, 196, 192]             384\n",
       "│    │    └─Mlp: 3-38                    [5, 196, 192]             295,872\n",
       "│    │    └─Identity: 3-39               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-40               [5, 196, 192]             --\n",
       "│    └─Block: 2-8                        [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-41              [5, 196, 192]             384\n",
       "│    │    └─Attention: 3-42              [5, 196, 192]             148,224\n",
       "│    │    └─Identity: 3-43               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-44               [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-45              [5, 196, 192]             384\n",
       "│    │    └─Mlp: 3-46                    [5, 196, 192]             295,872\n",
       "│    │    └─Identity: 3-47               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-48               [5, 196, 192]             --\n",
       "│    └─Block: 2-9                        [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-49              [5, 196, 192]             384\n",
       "│    │    └─Attention: 3-50              [5, 196, 192]             148,224\n",
       "│    │    └─Identity: 3-51               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-52               [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-53              [5, 196, 192]             384\n",
       "│    │    └─Mlp: 3-54                    [5, 196, 192]             295,872\n",
       "│    │    └─Identity: 3-55               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-56               [5, 196, 192]             --\n",
       "│    └─Block: 2-10                       [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-57              [5, 196, 192]             384\n",
       "│    │    └─Attention: 3-58              [5, 196, 192]             148,224\n",
       "│    │    └─Identity: 3-59               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-60               [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-61              [5, 196, 192]             384\n",
       "│    │    └─Mlp: 3-62                    [5, 196, 192]             295,872\n",
       "│    │    └─Identity: 3-63               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-64               [5, 196, 192]             --\n",
       "│    └─Block: 2-11                       [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-65              [5, 196, 192]             384\n",
       "│    │    └─Attention: 3-66              [5, 196, 192]             148,224\n",
       "│    │    └─Identity: 3-67               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-68               [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-69              [5, 196, 192]             384\n",
       "│    │    └─Mlp: 3-70                    [5, 196, 192]             295,872\n",
       "│    │    └─Identity: 3-71               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-72               [5, 196, 192]             --\n",
       "│    └─Block: 2-12                       [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-73              [5, 196, 192]             384\n",
       "│    │    └─Attention: 3-74              [5, 196, 192]             148,224\n",
       "│    │    └─Identity: 3-75               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-76               [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-77              [5, 196, 192]             384\n",
       "│    │    └─Mlp: 3-78                    [5, 196, 192]             295,872\n",
       "│    │    └─Identity: 3-79               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-80               [5, 196, 192]             --\n",
       "│    └─Block: 2-13                       [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-81              [5, 196, 192]             384\n",
       "│    │    └─Attention: 3-82              [5, 196, 192]             148,224\n",
       "│    │    └─Identity: 3-83               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-84               [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-85              [5, 196, 192]             384\n",
       "│    │    └─Mlp: 3-86                    [5, 196, 192]             295,872\n",
       "│    │    └─Identity: 3-87               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-88               [5, 196, 192]             --\n",
       "│    └─Block: 2-14                       [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-89              [5, 196, 192]             384\n",
       "│    │    └─Attention: 3-90              [5, 196, 192]             148,224\n",
       "│    │    └─Identity: 3-91               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-92               [5, 196, 192]             --\n",
       "│    │    └─LayerNorm: 3-93              [5, 196, 192]             384\n",
       "│    │    └─Mlp: 3-94                    [5, 196, 192]             295,872\n",
       "│    │    └─Identity: 3-95               [5, 196, 192]             --\n",
       "│    │    └─Identity: 3-96               [5, 196, 192]             --\n",
       "==========================================================================================\n",
       "Total params: 5,486,016\n",
       "Trainable params: 5,486,016\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 171.39\n",
       "==========================================================================================\n",
       "Input size (MB): 3.01\n",
       "Forward/backward pass size (MB): 200.20\n",
       "Params size (MB): 21.94\n",
       "Estimated Total Size (MB): 225.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True)\n",
    "list(m.children())[-4:-1]\n",
    "\n",
    "m = nn.Sequential(*list(m.children())[:-3])\n",
    "torchinfo.summary(m, (5, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeiT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base = torch.hub.load('facebookresearch/deit:main', 'deit_small_patch16_224', pretrained=True)\n",
    "        self.base.reset_classifier(8)\n",
    "        self.img_net = ClassificationNet(dims = 512, drop_prob=0.5)\n",
    "        self.img_head = nn.Linear(512, 8)\n",
    "    \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, pixel_values, add_info):\n",
    "        out = self.base(pixel_values)\n",
    "\n",
    "        out = F.sigmoid(out)\n",
    "        return out#, img_outs, txt_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\krish/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    }
   ],
   "source": [
    "model= DeiT().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as torchvision_functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:16<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.3847, acc img: 0.8410\n",
      "Epoch [1/10], Val Loss: 0.2960, acc img: 0.8834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:16<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 0.2659, acc img: 0.8877\n",
      "Epoch [2/10], Val Loss: 0.2686, acc img: 0.8902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:15<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 0.2417, acc img: 0.8982\n",
      "Epoch [3/10], Val Loss: 0.2502, acc img: 0.8979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:15<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 0.2247, acc img: 0.9040\n",
      "Epoch [4/10], Val Loss: 0.2398, acc img: 0.9030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:15<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 0.2164, acc img: 0.9062\n",
      "Epoch [5/10], Val Loss: 0.2337, acc img: 0.9010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:15<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Train Loss: 0.2069, acc img: 0.9114\n",
      "Epoch [6/10], Val Loss: 0.2323, acc img: 0.8992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:15<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Train Loss: 0.2001, acc img: 0.9150\n",
      "Epoch [7/10], Val Loss: 0.2227, acc img: 0.9066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:16<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Train Loss: 0.1889, acc img: 0.9184\n",
      "Epoch [8/10], Val Loss: 0.2204, acc img: 0.9085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:15<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Train Loss: 0.1864, acc img: 0.9182\n",
      "Epoch [9/10], Val Loss: 0.2201, acc img: 0.9077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:15<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Train Loss: 0.1815, acc img: 0.9192\n",
      "Epoch [10/10], Val Loss: 0.2204, acc img: 0.9082\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "weights = torch.tensor([0.5, 1., 1.5, 1.5, 1.5, 1.8, 1.5, .9]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)\n",
    "train_img_acc = MultilabelAccuracy(8, average = 'micro').to(device)\n",
    "val_img_acc = MultilabelAccuracy(8, average = 'micro').to(device)\n",
    "\n",
    "img_loss_fn = nn.BCELoss(weights)\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "\n",
    "  total_acc_train = 0\n",
    "  total_loss_train = 0\n",
    "\n",
    "  for train_image, train_metadata, train_label in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        train_label = train_label.to(device)\n",
    "        train_image = train_image.to(device)\n",
    "        train_metadata = train_metadata.to(device)\n",
    "      \n",
    "\n",
    "        output = model(train_image, train_metadata)\n",
    "\n",
    "        batch_loss = img_loss_fn(output, train_label)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_train += batch_loss.item()\n",
    "      \n",
    "        train_img_acc(output, train_label)\n",
    "\n",
    "  total_loss_val = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "      #Validation\n",
    "      for val_image, val_metadata, val_label in val_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        val_label = val_label.to(device)\n",
    "        val_image = val_image.to(device)\n",
    "        val_metadata = val_metadata.to(device)\n",
    "      \n",
    "\n",
    "        output = model(val_image, val_metadata)\n",
    "\n",
    "        batch_loss = img_loss_fn(output, val_label)\n",
    "        total_loss_val += batch_loss.item()\n",
    "        val_img_acc(output, val_label)\n",
    "              \n",
    "      \n",
    "  avg_train_loss = total_loss_train/len(train_df)\n",
    "\n",
    "  avg_val_loss = total_loss_val/len(val_df)\n",
    "\n",
    "\n",
    "  print(\"Epoch [{}/{}], Train Loss: {:.4f}, acc img: {:.4f}\".format(epoch_num+1, EPOCHS, avg_train_loss*BATCH_SIZE, train_img_acc.compute()))\n",
    "  print(\"Epoch [{}/{}], Val Loss: {:.4f}, acc img: {:.4f}\".format(epoch_num+1, EPOCHS, avg_val_loss*BATCH_SIZE, val_img_acc.compute()))\n",
    "\n",
    "  train_img_acc.reset()\n",
    "  val_img_acc.reset()\n",
    "\n",
    "  torch.save(model.state_dict(), './' + 'checkpoint_deit' + '.pt' )\n",
    "\n",
    "torch.save(model.state_dict(), './' + 'finetuned_deit' + '.pt' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"checkpoint_deit.pt\", 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.905956\n",
      "Prec: 0.653696\n",
      "Recall: 0.527\n",
      "F1-score: 0.583\n",
      "F-Beta-score: 0.597\n",
      "Kappa: 0.000\n",
      "AUC: 0.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#criterion = nn.BCELoss(torch.tensor([0.5, 1, 5, 5, 5, 6, 5, 1]).float().to(device))\n",
    "\n",
    "test_loss = 0\n",
    "test_acc  = 0\n",
    "\n",
    "AVERAGING = 'micro'\n",
    "PREC = torchmetrics.classification.MultilabelPrecision(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "ACC = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "REC = torchmetrics.classification.MultilabelRecall(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "F1_SCORE = torchmetrics.classification.MultilabelF1Score(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "F_BETA_SCORE = torchmetrics.classification.MultilabelFBetaScore(beta = 0.8, num_classes = 8, num_labels = 8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "KAPPA = torchmetrics.classification.MulticlassCohenKappa(8).to(device)#, validate_args = False)\n",
    "AUC = torchmetrics.classification.MultilabelAUROC(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "\n",
    "for train_image, train_metadata, train_label in tqdm(test_dataloader): \n",
    "    with torch.no_grad():\n",
    "        optimizer.zero_grad()\n",
    "        train_label = train_label.to(device)\n",
    "        train_image = train_image.to(device)\n",
    "        train_metadata = train_metadata.to(device)\n",
    "      \n",
    "\n",
    "        predictions = model(train_image, train_metadata)\n",
    "\n",
    "\n",
    "        train_label = train_label.long()\n",
    "        PREC(predictions, train_label)\n",
    "        ACC(predictions, train_label)\n",
    "        REC(predictions, train_label)\n",
    "        F1_SCORE(predictions, train_label)\n",
    "        F_BETA_SCORE(predictions, train_label)\n",
    "        KAPPA(predictions, train_label)\n",
    "        AUC(predictions, train_label)\n",
    "\n",
    "\n",
    "add_prec = PREC.compute()\n",
    "add_acc = ACC.compute()\n",
    "add_rec = REC.compute()\n",
    "add_f1 = F1_SCORE.compute()\n",
    "add_fbeta = F_BETA_SCORE.compute()\n",
    "add_kappa = KAPPA.compute()\n",
    "add_auc = AUC.compute()\n",
    "\n",
    "avg_test_loss = test_loss/len(test_df)*BATCH_SIZE\n",
    "avg_test_acc  = test_acc /len(test_df)\n",
    "\n",
    "print(\"Acc: {:3f}\\nPrec: {:3f}\\nRecall: {:.3f}\\nF1-score: {:.3f}\\nF-Beta-score: {:.3f}\\nKappa: {:.3f}\\nAUC: {:.3f}\".format(add_acc, add_prec,add_rec, add_f1, add_fbeta, add_kappa, add_auc))\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('torchnew')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aae2c75324e7fdf5ebd22146e2daffaa477f8ea149f0e685be4c317c2939a685"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
