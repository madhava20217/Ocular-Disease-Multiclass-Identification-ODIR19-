{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import logging\n",
    "import gdown\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "import albumentations as A\n",
    "\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from scipy.io import loadmat\n",
    "from sklearn.manifold import TSNE\n",
    "from torchmetrics.classification import MulticlassF1Score, JaccardIndex, MulticlassPrecision, MulticlassRecall, MulticlassAveragePrecision\n",
    "import pandas as pd\n",
    "from torchinfo import torchinfo\n",
    "\n",
    "from transformers import ConvNextV2Model, BertModel, BertTokenizer, ViTModel, ViTConfig\n",
    "from transformers import AutoTokenizer, AutoModel, RobertaTokenizer, CLIPModel, CLIPTokenizer, CLIPProcessor\n",
    "from transformers import DeiTConfig, DeiTFeatureExtractor, DeiTImageProcessor, DeiTModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import AlignModel, AlignProcessor, AlignConfig, AlignVisionConfig, AlignTextConfig\n",
    "from transformers import CLIPModel, CLIPProcessor, CLIPConfig, CLIPTextConfig, CLIPVisionConfig, CLIPImageProcessor\n",
    "\n",
    "from torchmetrics.functional import pairwise_cosine_similarity\n",
    "from torchmetrics.classification import MultilabelAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    \n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '../Datasets/ocular-disease-recognition-odir5k/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = ROOT_DIR + 'dataset_single_eye.csv'\n",
    "TEST_CSV = ROOT_DIR + 'TESTING_dataset_single_eye.csv'\n",
    "IMG_PATH = ROOT_DIR + 'preprocessed_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.io.read_image(IMG_PATH + '0_left.jpg').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df = pd.read_csv(CSV_PATH)\n",
    "test_df = pd.read_csv(TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df:pd.DataFrame):\n",
    "    df['Keywords'] = df['Keywords'].str.lower()\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: \" \".join(x.split()))\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
    "    return df\n",
    "train_val_df = preprocess_text(train_val_df)\n",
    "test_df = preprocess_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df['Patient Sex'] = train_val_df['Patient Sex'].astype('category').cat.codes\n",
    "test_df['Patient Sex'] = test_df['Patient Sex'].astype('category').cat.codes\n",
    "\n",
    "train_val_df['eye'] = train_val_df['Patient Sex'].astype('category').cat.codes\n",
    "test_df['eye'] = test_df['Patient Sex'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Sex</th>\n",
       "      <th>Image</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>eye</th>\n",
       "      <th>N</th>\n",
       "      <th>D</th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>A</th>\n",
       "      <th>H</th>\n",
       "      <th>M</th>\n",
       "      <th>O</th>\n",
       "      <th>NOT DECISIVE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>970</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>970_right.jpg</td>\n",
       "      <td>cataract</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>127_left.jpg</td>\n",
       "      <td>proliferative diabetic retinopathy，hypertensiv...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>850</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>850_right.jpg</td>\n",
       "      <td>macular epiretinal membrane，moderate non proli...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>37_right.jpg</td>\n",
       "      <td>normal fundus</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4421</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>4421_right.jpg</td>\n",
       "      <td>moderate non proliferative retinopathy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5733</th>\n",
       "      <td>199</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>199_left.jpg</td>\n",
       "      <td>branch retinal vein occlusion</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>516</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>516_right.jpg</td>\n",
       "      <td>moderate non proliferative retinopathy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5735</th>\n",
       "      <td>4603</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>4603_left.jpg</td>\n",
       "      <td>severe nonproliferative retinopathy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736</th>\n",
       "      <td>2132</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>2132_right.jpg</td>\n",
       "      <td>normal fundus</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5737</th>\n",
       "      <td>4487</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>4487_right.jpg</td>\n",
       "      <td>mild nonproliferative retinopathy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5738 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  Patient Age  Patient Sex           Image  \\\n",
       "0      970           56            0   970_right.jpg   \n",
       "1      127           52            1    127_left.jpg   \n",
       "2      850           68            1   850_right.jpg   \n",
       "3       37           41            1    37_right.jpg   \n",
       "4     4421           59            1  4421_right.jpg   \n",
       "...    ...          ...          ...             ...   \n",
       "5733   199           50            0    199_left.jpg   \n",
       "5734   516           42            1   516_right.jpg   \n",
       "5735  4603           47            0   4603_left.jpg   \n",
       "5736  2132           59            0  2132_right.jpg   \n",
       "5737  4487           55            0  4487_right.jpg   \n",
       "\n",
       "                                               Keywords  eye  N  D  G  C  A  \\\n",
       "0                                              cataract    0  0  0  0  1  0   \n",
       "1     proliferative diabetic retinopathy，hypertensiv...    1  0  1  0  0  0   \n",
       "2     macular epiretinal membrane，moderate non proli...    1  0  1  0  0  0   \n",
       "3                                         normal fundus    1  1  0  0  0  0   \n",
       "4                moderate non proliferative retinopathy    1  0  1  0  0  0   \n",
       "...                                                 ...  ... .. .. .. .. ..   \n",
       "5733                      branch retinal vein occlusion    0  0  0  0  0  0   \n",
       "5734             moderate non proliferative retinopathy    1  0  1  0  0  0   \n",
       "5735                severe nonproliferative retinopathy    0  0  1  0  0  0   \n",
       "5736                                      normal fundus    0  1  0  0  0  0   \n",
       "5737                  mild nonproliferative retinopathy    0  0  1  0  0  0   \n",
       "\n",
       "      H  M  O  NOT DECISIVE  \n",
       "0     0  0  0             0  \n",
       "1     0  0  0             0  \n",
       "2     0  0  0             0  \n",
       "3     0  0  0             0  \n",
       "4     0  0  0             0  \n",
       "...  .. .. ..           ...  \n",
       "5733  0  0  1             0  \n",
       "5734  0  0  0             0  \n",
       "5735  0  0  0             0  \n",
       "5736  0  0  0             0  \n",
       "5737  0  0  0             0  \n",
       "\n",
       "[5738 rows x 15 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_val_df['Keywords'].apply(lambda x: len(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4877, 861)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(train_val_df, test_size = 0.15, random_state= 123456)\n",
    "len(train_df), len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "rescale_transform = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.CenterCrop(IMG_SIZE),\n",
    "    torchvision.transforms.Resize(IMG_SIZE, antialias = False, interpolation = torchvision.transforms.InterpolationMode.NEAREST),\n",
    "    torchvision.transforms.Normalize(\n",
    "        timm.data.constants.IMAGENET_DEFAULT_MEAN,\n",
    "        timm.data.constants.IMAGENET_DEFAULT_STD\n",
    "    )\n",
    "])\n",
    "\n",
    "augmentation = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(p = 0.5),\n",
    "    torchvision.transforms.RandomVerticalFlip(p= 0.5),\n",
    "    #torchvision.transforms.RandomRotation(90)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56,  0],\n",
       "       [52,  1],\n",
       "       [68,  1],\n",
       "       ...,\n",
       "       [47,  0],\n",
       "       [59,  0],\n",
       "       [55,  0]], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.c_[train_val_df['Patient Age'].to_numpy(), train_val_df['Patient Sex'].to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODIRDataset(Dataset) :\n",
    "    def __init__(self, df, IMG_FOLDER, extractor = rescale_transform, augmentation = None) :\n",
    "        '''\n",
    "        id : list of samples ids as string\n",
    "        '''\n",
    "        #self.text = [tokenizer(text = x, padding = 'max_length', max_length = 40, truncation = True, return_tensors = 'pt') for x in df['Keywords']]\n",
    "        sex = df['Patient Age'].to_numpy()\n",
    "        age = (df['Patient Age']/df['Patient Age'].max()).to_numpy()\n",
    "        self.feats = torch.tensor(np.c_[sex, age])\n",
    "        self.labels = torch.tensor(df[['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']].to_numpy()).float()\n",
    "        self.img_dir = [IMG_PATH + x for x in df['Image']]\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "        self.images = [extractor(torchvision.io.read_image(x)/255.0) for x in self.img_dir]\n",
    "        #self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        batch_imgs = self.images[idx]\n",
    "        if(self.augmentation is not None):\n",
    "            batch_imgs = self.augmentation(batch_imgs)\n",
    "        return batch_imgs, self.feats[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = ODIRDataset(train_df, IMG_PATH)#, augmentation = augmentation)\n",
    "# val_dataset   = ODIRDataset(val_df, IMG_PATH)\n",
    "# test_dataset  = ODIRDataset(test_df, IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"datasets.trch\", 'wb') as f:\n",
    "#     torch.save([train_dataset, val_dataset, test_dataset], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets.trch\", 'rb') as f:\n",
    "    train_dataset, val_dataset, test_dataset = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size= BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrastive learning on training data finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#odict_keys(['loss', 'logits_per_image', 'logits_per_text', 'text_embeds', 'image_embeds', 'text_model_output', 'vision_model_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNet(nn.Module):\n",
    "    def __init__(self, dims = 768, drop_prob = 0.5):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(dims, 768)\n",
    "        self.layer_2 = nn.Linear(768, 768)\n",
    "        self.layer_3 = nn.Linear(768, dims)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        resi = input\n",
    "        out = self.dropout(input)\n",
    "        out = F.relu(self.layer_1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = F.relu(self.layer_2(out))\n",
    "        out = self.dropout(out)\n",
    "        out = F.relu(self.layer_3(out))\n",
    "        out = out + resi\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\krish/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Sequential(\n",
       "   (0): Block(\n",
       "     (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (attn): Attention(\n",
       "       (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "       (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "       (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "       (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls1): Identity()\n",
       "     (drop_path1): Identity()\n",
       "     (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (mlp): Mlp(\n",
       "       (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "       (act): GELU(approximate='none')\n",
       "       (drop1): Dropout(p=0.0, inplace=False)\n",
       "       (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "       (drop2): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls2): Identity()\n",
       "     (drop_path2): Identity()\n",
       "   )\n",
       "   (1): Block(\n",
       "     (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (attn): Attention(\n",
       "       (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "       (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "       (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "       (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls1): Identity()\n",
       "     (drop_path1): Identity()\n",
       "     (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (mlp): Mlp(\n",
       "       (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "       (act): GELU(approximate='none')\n",
       "       (drop1): Dropout(p=0.0, inplace=False)\n",
       "       (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "       (drop2): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls2): Identity()\n",
       "     (drop_path2): Identity()\n",
       "   )\n",
       "   (2): Block(\n",
       "     (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (attn): Attention(\n",
       "       (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "       (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "       (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "       (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls1): Identity()\n",
       "     (drop_path1): Identity()\n",
       "     (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (mlp): Mlp(\n",
       "       (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "       (act): GELU(approximate='none')\n",
       "       (drop1): Dropout(p=0.0, inplace=False)\n",
       "       (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "       (drop2): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls2): Identity()\n",
       "     (drop_path2): Identity()\n",
       "   )\n",
       "   (3): Block(\n",
       "     (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (attn): Attention(\n",
       "       (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "       (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "       (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "       (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls1): Identity()\n",
       "     (drop_path1): Identity()\n",
       "     (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (mlp): Mlp(\n",
       "       (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "       (act): GELU(approximate='none')\n",
       "       (drop1): Dropout(p=0.0, inplace=False)\n",
       "       (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "       (drop2): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls2): Identity()\n",
       "     (drop_path2): Identity()\n",
       "   )\n",
       "   (4): Block(\n",
       "     (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (attn): Attention(\n",
       "       (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "       (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "       (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "       (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls1): Identity()\n",
       "     (drop_path1): Identity()\n",
       "     (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (mlp): Mlp(\n",
       "       (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "       (act): GELU(approximate='none')\n",
       "       (drop1): Dropout(p=0.0, inplace=False)\n",
       "       (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "       (drop2): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls2): Identity()\n",
       "     (drop_path2): Identity()\n",
       "   )\n",
       "   (5): Block(\n",
       "     (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (attn): Attention(\n",
       "       (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "       (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "       (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "       (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls1): Identity()\n",
       "     (drop_path1): Identity()\n",
       "     (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (mlp): Mlp(\n",
       "       (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "       (act): GELU(approximate='none')\n",
       "       (drop1): Dropout(p=0.0, inplace=False)\n",
       "       (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "       (drop2): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls2): Identity()\n",
       "     (drop_path2): Identity()\n",
       "   )\n",
       "   (6): Block(\n",
       "     (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (attn): Attention(\n",
       "       (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "       (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "       (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "       (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls1): Identity()\n",
       "     (drop_path1): Identity()\n",
       "     (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (mlp): Mlp(\n",
       "       (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "       (act): GELU(approximate='none')\n",
       "       (drop1): Dropout(p=0.0, inplace=False)\n",
       "       (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "       (drop2): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls2): Identity()\n",
       "     (drop_path2): Identity()\n",
       "   )\n",
       "   (7): Block(\n",
       "     (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (attn): Attention(\n",
       "       (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "       (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "       (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "       (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls1): Identity()\n",
       "     (drop_path1): Identity()\n",
       "     (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (mlp): Mlp(\n",
       "       (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "       (act): GELU(approximate='none')\n",
       "       (drop1): Dropout(p=0.0, inplace=False)\n",
       "       (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "       (drop2): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls2): Identity()\n",
       "     (drop_path2): Identity()\n",
       "   )\n",
       "   (8): Block(\n",
       "     (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (attn): Attention(\n",
       "       (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "       (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "       (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "       (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls1): Identity()\n",
       "     (drop_path1): Identity()\n",
       "     (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (mlp): Mlp(\n",
       "       (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "       (act): GELU(approximate='none')\n",
       "       (drop1): Dropout(p=0.0, inplace=False)\n",
       "       (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "       (drop2): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls2): Identity()\n",
       "     (drop_path2): Identity()\n",
       "   )\n",
       "   (9): Block(\n",
       "     (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (attn): Attention(\n",
       "       (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "       (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "       (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "       (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls1): Identity()\n",
       "     (drop_path1): Identity()\n",
       "     (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (mlp): Mlp(\n",
       "       (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "       (act): GELU(approximate='none')\n",
       "       (drop1): Dropout(p=0.0, inplace=False)\n",
       "       (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "       (drop2): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls2): Identity()\n",
       "     (drop_path2): Identity()\n",
       "   )\n",
       "   (10): Block(\n",
       "     (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (attn): Attention(\n",
       "       (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "       (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "       (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "       (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls1): Identity()\n",
       "     (drop_path1): Identity()\n",
       "     (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (mlp): Mlp(\n",
       "       (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "       (act): GELU(approximate='none')\n",
       "       (drop1): Dropout(p=0.0, inplace=False)\n",
       "       (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "       (drop2): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls2): Identity()\n",
       "     (drop_path2): Identity()\n",
       "   )\n",
       "   (11): Block(\n",
       "     (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (attn): Attention(\n",
       "       (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "       (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "       (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "       (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls1): Identity()\n",
       "     (drop_path1): Identity()\n",
       "     (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (mlp): Mlp(\n",
       "       (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "       (act): GELU(approximate='none')\n",
       "       (drop1): Dropout(p=0.0, inplace=False)\n",
       "       (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "       (drop2): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (ls2): Identity()\n",
       "     (drop_path2): Identity()\n",
       "   )\n",
       " ),\n",
       " LayerNorm((192,), eps=1e-06, elementwise_affine=True),\n",
       " Identity()]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True)\n",
    "\n",
    "list(m.children())[-4:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VisionTransformer                        [5, 1000]                 38,016\n",
       "├─PatchEmbed: 1-1                        [5, 196, 192]             --\n",
       "│    └─Conv2d: 2-1                       [5, 192, 14, 14]          147,648\n",
       "│    └─Identity: 2-2                     [5, 196, 192]             --\n",
       "├─Dropout: 1-2                           [5, 197, 192]             --\n",
       "├─Identity: 1-3                          [5, 197, 192]             --\n",
       "├─Sequential: 1-4                        [5, 197, 192]             --\n",
       "│    └─Block: 2-3                        [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-1               [5, 197, 192]             384\n",
       "│    │    └─Attention: 3-2               [5, 197, 192]             148,224\n",
       "│    │    └─Identity: 3-3                [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-4                [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-5               [5, 197, 192]             384\n",
       "│    │    └─Mlp: 3-6                     [5, 197, 192]             295,872\n",
       "│    │    └─Identity: 3-7                [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-8                [5, 197, 192]             --\n",
       "│    └─Block: 2-4                        [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-9               [5, 197, 192]             384\n",
       "│    │    └─Attention: 3-10              [5, 197, 192]             148,224\n",
       "│    │    └─Identity: 3-11               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-12               [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-13              [5, 197, 192]             384\n",
       "│    │    └─Mlp: 3-14                    [5, 197, 192]             295,872\n",
       "│    │    └─Identity: 3-15               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-16               [5, 197, 192]             --\n",
       "│    └─Block: 2-5                        [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-17              [5, 197, 192]             384\n",
       "│    │    └─Attention: 3-18              [5, 197, 192]             148,224\n",
       "│    │    └─Identity: 3-19               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-20               [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-21              [5, 197, 192]             384\n",
       "│    │    └─Mlp: 3-22                    [5, 197, 192]             295,872\n",
       "│    │    └─Identity: 3-23               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-24               [5, 197, 192]             --\n",
       "│    └─Block: 2-6                        [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-25              [5, 197, 192]             384\n",
       "│    │    └─Attention: 3-26              [5, 197, 192]             148,224\n",
       "│    │    └─Identity: 3-27               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-28               [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-29              [5, 197, 192]             384\n",
       "│    │    └─Mlp: 3-30                    [5, 197, 192]             295,872\n",
       "│    │    └─Identity: 3-31               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-32               [5, 197, 192]             --\n",
       "│    └─Block: 2-7                        [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-33              [5, 197, 192]             384\n",
       "│    │    └─Attention: 3-34              [5, 197, 192]             148,224\n",
       "│    │    └─Identity: 3-35               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-36               [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-37              [5, 197, 192]             384\n",
       "│    │    └─Mlp: 3-38                    [5, 197, 192]             295,872\n",
       "│    │    └─Identity: 3-39               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-40               [5, 197, 192]             --\n",
       "│    └─Block: 2-8                        [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-41              [5, 197, 192]             384\n",
       "│    │    └─Attention: 3-42              [5, 197, 192]             148,224\n",
       "│    │    └─Identity: 3-43               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-44               [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-45              [5, 197, 192]             384\n",
       "│    │    └─Mlp: 3-46                    [5, 197, 192]             295,872\n",
       "│    │    └─Identity: 3-47               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-48               [5, 197, 192]             --\n",
       "│    └─Block: 2-9                        [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-49              [5, 197, 192]             384\n",
       "│    │    └─Attention: 3-50              [5, 197, 192]             148,224\n",
       "│    │    └─Identity: 3-51               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-52               [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-53              [5, 197, 192]             384\n",
       "│    │    └─Mlp: 3-54                    [5, 197, 192]             295,872\n",
       "│    │    └─Identity: 3-55               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-56               [5, 197, 192]             --\n",
       "│    └─Block: 2-10                       [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-57              [5, 197, 192]             384\n",
       "│    │    └─Attention: 3-58              [5, 197, 192]             148,224\n",
       "│    │    └─Identity: 3-59               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-60               [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-61              [5, 197, 192]             384\n",
       "│    │    └─Mlp: 3-62                    [5, 197, 192]             295,872\n",
       "│    │    └─Identity: 3-63               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-64               [5, 197, 192]             --\n",
       "│    └─Block: 2-11                       [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-65              [5, 197, 192]             384\n",
       "│    │    └─Attention: 3-66              [5, 197, 192]             148,224\n",
       "│    │    └─Identity: 3-67               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-68               [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-69              [5, 197, 192]             384\n",
       "│    │    └─Mlp: 3-70                    [5, 197, 192]             295,872\n",
       "│    │    └─Identity: 3-71               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-72               [5, 197, 192]             --\n",
       "│    └─Block: 2-12                       [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-73              [5, 197, 192]             384\n",
       "│    │    └─Attention: 3-74              [5, 197, 192]             148,224\n",
       "│    │    └─Identity: 3-75               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-76               [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-77              [5, 197, 192]             384\n",
       "│    │    └─Mlp: 3-78                    [5, 197, 192]             295,872\n",
       "│    │    └─Identity: 3-79               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-80               [5, 197, 192]             --\n",
       "│    └─Block: 2-13                       [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-81              [5, 197, 192]             384\n",
       "│    │    └─Attention: 3-82              [5, 197, 192]             148,224\n",
       "│    │    └─Identity: 3-83               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-84               [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-85              [5, 197, 192]             384\n",
       "│    │    └─Mlp: 3-86                    [5, 197, 192]             295,872\n",
       "│    │    └─Identity: 3-87               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-88               [5, 197, 192]             --\n",
       "│    └─Block: 2-14                       [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-89              [5, 197, 192]             384\n",
       "│    │    └─Attention: 3-90              [5, 197, 192]             148,224\n",
       "│    │    └─Identity: 3-91               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-92               [5, 197, 192]             --\n",
       "│    │    └─LayerNorm: 3-93              [5, 197, 192]             384\n",
       "│    │    └─Mlp: 3-94                    [5, 197, 192]             295,872\n",
       "│    │    └─Identity: 3-95               [5, 197, 192]             --\n",
       "│    │    └─Identity: 3-96               [5, 197, 192]             --\n",
       "├─LayerNorm: 1-5                         [5, 197, 192]             384\n",
       "├─Identity: 1-6                          [5, 192]                  --\n",
       "├─Linear: 1-7                            [5, 1000]                 193,000\n",
       "==========================================================================================\n",
       "Total params: 5,717,416\n",
       "Trainable params: 5,717,416\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 172.35\n",
       "==========================================================================================\n",
       "Input size (MB): 3.01\n",
       "Forward/backward pass size (MB): 202.77\n",
       "Params size (MB): 22.72\n",
       "Estimated Total Size (MB): 228.50\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(m, (5, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLearning(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
    "        self.text_net = ClassificationNet(dims = 512, drop_prob=0.5)\n",
    "        self.text_head = nn.Linear(512, 8)\n",
    "        self.img_net = ClassificationNet(dims = 512, drop_prob=0.5)\n",
    "        self.img_head = nn.Linear(512, 8)\n",
    "    \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, pixel_values, ):\n",
    "        out = self.base(imgs)\n",
    "\n",
    "\n",
    "        return out, img_outs, txt_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= ContrastiveLearning().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:22<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 4.8960, acc img: 0.4004, txt : 0.5090\n",
      "Epoch [1/100], Val Loss: 6.5442, acc img: 0.3884, txt : 0.5302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:19<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Train Loss: 4.8751, acc img: 0.3878, txt : 0.5282\n",
      "Epoch [2/100], Val Loss: 6.5286, acc img: 0.3942, txt : 0.5302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:19<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Train Loss: 4.8635, acc img: 0.3890, txt : 0.5282\n",
      "Epoch [3/100], Val Loss: 6.5200, acc img: 0.3765, txt : 0.5173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:19<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Train Loss: 4.8580, acc img: 0.3734, txt : 0.5325\n",
      "Epoch [4/100], Val Loss: 6.5192, acc img: 0.3741, txt : 0.5405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:19<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Train Loss: 4.8479, acc img: 0.3727, txt : 0.5353\n",
      "Epoch [5/100], Val Loss: 6.5068, acc img: 0.3614, txt : 0.5411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:19<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Train Loss: 4.8445, acc img: 0.3718, txt : 0.5479\n",
      "Epoch [6/100], Val Loss: 6.5166, acc img: 0.3647, txt : 0.5377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:19<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Train Loss: 4.8455, acc img: 0.3622, txt : 0.5364\n",
      "Epoch [7/100], Val Loss: 6.5024, acc img: 0.3582, txt : 0.5421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:19<00:00,  2.05it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m input_id \u001b[38;5;241m=\u001b[39m val_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     46\u001b[0m output, img_outs, txt_outs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(pixel_values \u001b[38;5;241m=\u001b[39m val_image, input_ids \u001b[38;5;241m=\u001b[39m input_id, attention_mask \u001b[38;5;241m=\u001b[39m mask)\n\u001b[1;32m---> 47\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m img_loss_fn(img_outs, val_label) \u001b[38;5;241m+\u001b[39m txt_loss_fn(txt_outs, val_label)\n\u001b[0;32m     48\u001b[0m total_loss_val \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     50\u001b[0m val_img_acc(img_outs, val_label)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 619\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mbinary_cross_entropy(\u001b[39minput\u001b[39m, target, weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduction)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torch\\nn\\functional.py:3118\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3115\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n\u001b[0;32m   3116\u001b[0m     weight \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3118\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mbinary_cross_entropy(\u001b[39minput\u001b[39m, target, weight, reduction_enum)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "weights = torch.tensor([0.5, 1., 1.25, 1.25, 1.25, 1.3, 1.25, .9]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-6)\n",
    "train_img_acc = MultilabelAccuracy(8, average = 'micro').to(device)\n",
    "train_txt_acc = MultilabelAccuracy(8, average = 'micro').to(device)\n",
    "val_img_acc = MultilabelAccuracy(8, average = 'micro').to(device)\n",
    "val_txt_acc = MultilabelAccuracy(8, average = 'micro').to(device)\n",
    "\n",
    "img_loss_fn = nn.BCELoss(weights)\n",
    "txt_loss_fn = nn.BCELoss(weights)\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "\n",
    "  total_acc_train = 0\n",
    "  total_loss_train = 0\n",
    "\n",
    "  for train_image, train_text, train_label in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        train_label = train_label.to(device)\n",
    "        train_image = train_image.to(device)\n",
    "        mask = train_text['attention_mask'].to(device).squeeze(1)\n",
    "        input_id = train_text['input_ids'].squeeze(1).to(device).squeeze(1)\n",
    "        output, img_outs, txt_outs = model.forward(pixel_values = train_image, input_ids = input_id, attention_mask = mask)\n",
    "\n",
    "        batch_loss = output['loss'] #+ img_loss_fn(img_outs, train_label) + txt_loss_fn(txt_outs, train_label)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_train += batch_loss.item()\n",
    "      \n",
    "        train_img_acc(img_outs, train_label)\n",
    "        train_txt_acc(txt_outs, train_label)\n",
    "\n",
    "  total_loss_val = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "      #Validation\n",
    "      for val_image, val_text, val_label in val_dataloader:\n",
    "\n",
    "          val_label = val_label.to(device)\n",
    "          val_image = val_image.to(device)\n",
    "          mask = val_text['attention_mask'].to(device).squeeze(1)\n",
    "          input_id = val_text['input_ids'].squeeze(1).to(device).squeeze(1)\n",
    "\n",
    "          output, img_outs, txt_outs = model.forward(pixel_values = val_image, input_ids = input_id, attention_mask = mask)\n",
    "          batch_loss = output['loss'] + img_loss_fn(img_outs, val_label) + txt_loss_fn(txt_outs, val_label)\n",
    "          total_loss_val += batch_loss.item()\n",
    "          \n",
    "          val_img_acc(img_outs, val_label)\n",
    "          val_txt_acc(txt_outs, val_label)\n",
    "              \n",
    "      \n",
    "  avg_train_loss = total_loss_train/len(train_df)\n",
    "\n",
    "  avg_val_loss = total_loss_val/len(val_df)\n",
    "\n",
    "\n",
    "  print(\"Epoch [{}/{}], Train Loss: {:.4f}, acc img: {:.4f}, txt : {:.4f}\".format(epoch_num+1, EPOCHS, avg_train_loss*BATCH_SIZE, train_img_acc.compute(), train_txt_acc.compute()))\n",
    "  print(\"Epoch [{}/{}], Val Loss: {:.4f}, acc img: {:.4f}, txt : {:.4f}\".format(epoch_num+1, EPOCHS, avg_val_loss*BATCH_SIZE, val_img_acc.compute(), val_txt_acc.compute()))\n",
    "\n",
    "  train_img_acc.reset()\n",
    "  train_txt_acc.reset()\n",
    "  val_img_acc.reset()\n",
    "  val_txt_acc.reset()\n",
    "\n",
    "  torch.save(model.state_dict(), './' + 'checkpoint' + '.pt' )\n",
    "\n",
    "torch.save(model.state_dict(), './' + 'finetuned' + '.pt' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"finetuned.pt\", 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/27 [00:00<?, ?it/s]c:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n",
      "100%|██████████| 27/27 [00:05<00:00,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.681057\n",
      "Prec: 0.105590\n",
      "Recall: 0.031\n",
      "F1-score: 0.040\n",
      "F-Beta-score: 0.044\n",
      "Kappa: 0.000\n",
      "AUC: 0.536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#criterion = nn.BCELoss(torch.tensor([0.5, 1, 5, 5, 5, 6, 5, 1]).float().to(device))\n",
    "\n",
    "test_loss = 0\n",
    "test_acc  = 0\n",
    "\n",
    "AVERAGING = 'weighted'\n",
    "PREC = torchmetrics.classification.MultilabelPrecision(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "ACC = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "REC = torchmetrics.classification.MultilabelRecall(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "F1_SCORE = torchmetrics.classification.MultilabelF1Score(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "F_BETA_SCORE = torchmetrics.classification.MultilabelFBetaScore(beta = 0.8, num_classes = 8, num_labels = 8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "KAPPA = torchmetrics.classification.MulticlassCohenKappa(8).to(device)#, validate_args = False)\n",
    "AUC = torchmetrics.classification.MultilabelAUROC(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "\n",
    "for train_image, train_text, train_label in tqdm(test_dataloader): \n",
    "    with torch.no_grad():\n",
    "        train_label = train_label.to(device)\n",
    "        train_image = train_image.to(device)\n",
    "        mask = train_text['attention_mask'].to(device)\n",
    "        input_id = train_text['input_ids'].squeeze(1).to(device)\n",
    "        \n",
    "        # logits_per_image, logits_per_text\n",
    "        #out_img, out_txt = model.forward(train_image, input_id, mask, contrastive = True)\n",
    "        output, predictions, txt_outs = model.forward(pixel_values = train_image, input_ids = input_id, attention_mask = mask)\n",
    "\n",
    "\n",
    "\n",
    "        train_label = train_label.long()\n",
    "        PREC(predictions, train_label)\n",
    "        ACC(predictions, train_label)\n",
    "        REC(predictions, train_label)\n",
    "        F1_SCORE(predictions, train_label)\n",
    "        F_BETA_SCORE(predictions, train_label)\n",
    "        KAPPA(predictions, train_label)\n",
    "        AUC(predictions, train_label)\n",
    "\n",
    "\n",
    "add_prec = PREC.compute()\n",
    "add_acc = ACC.compute()\n",
    "add_rec = REC.compute()\n",
    "add_f1 = F1_SCORE.compute()\n",
    "add_fbeta = F_BETA_SCORE.compute()\n",
    "add_kappa = KAPPA.compute()\n",
    "add_auc = AUC.compute()\n",
    "\n",
    "avg_test_loss = test_loss/len(test_df)*BATCH_SIZE\n",
    "avg_test_acc  = test_acc /len(test_df)\n",
    "\n",
    "print(\"Acc: {:3f}\\nPrec: {:3f}\\nRecall: {:.3f}\\nF1-score: {:.3f}\\nF-Beta-score: {:.3f}\\nKappa: {:.3f}\\nAUC: {:.3f}\".format(add_acc, add_prec,add_rec, add_f1, add_fbeta, add_kappa, add_auc))\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('torchnew')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aae2c75324e7fdf5ebd22146e2daffaa477f8ea149f0e685be4c317c2939a685"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
