{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import logging\n",
    "import gdown\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "import albumentations as A\n",
    "\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from scipy.io import loadmat\n",
    "from sklearn.manifold import TSNE\n",
    "from torchmetrics.classification import MulticlassF1Score, JaccardIndex, MulticlassPrecision, MulticlassRecall, MulticlassAveragePrecision\n",
    "import pandas as pd\n",
    "from torchinfo import torchinfo\n",
    "\n",
    "from transformers import ConvNextV2Model, BertModel, BertTokenizer, ViTModel, ViTConfig\n",
    "from transformers import AutoTokenizer, AutoModel, RobertaTokenizer, CLIPModel, CLIPTokenizer, CLIPProcessor\n",
    "from transformers import DeiTConfig, DeiTFeatureExtractor, DeiTImageProcessor, DeiTModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import AlignModel, AlignProcessor, AlignConfig, AlignVisionConfig, AlignTextConfig\n",
    "from transformers import CLIPModel, CLIPProcessor, CLIPConfig, CLIPTextConfig, CLIPVisionConfig, CLIPImageProcessor\n",
    "\n",
    "from torchmetrics.functional import pairwise_cosine_similarity\n",
    "from torchmetrics.classification import MultilabelAccuracy\n",
    "from transformers import AutoImageProcessor, Swinv2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    \n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '../Datasets/ocular-disease-recognition-odir5k/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = ROOT_DIR + 'dataset_single_eye.csv'\n",
    "TEST_CSV = ROOT_DIR + 'TESTING_dataset_single_eye.csv'\n",
    "IMG_PATH = ROOT_DIR + 'preprocessed_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.io.read_image(IMG_PATH + '0_left.jpg').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df = pd.read_csv(CSV_PATH)\n",
    "test_df = pd.read_csv(TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df:pd.DataFrame):\n",
    "    df['Keywords'] = df['Keywords'].str.lower()\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: \" \".join(x.split()))\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
    "    return df\n",
    "train_val_df = preprocess_text(train_val_df)\n",
    "test_df = preprocess_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df['Patient Sex'] = train_val_df['Patient Sex'].astype('category').cat.codes\n",
    "test_df['Patient Sex'] = test_df['Patient Sex'].astype('category').cat.codes\n",
    "\n",
    "train_val_df['eye'] = train_val_df['Patient Sex'].astype('category').cat.codes\n",
    "test_df['eye'] = test_df['Patient Sex'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Sex</th>\n",
       "      <th>Image</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>eye</th>\n",
       "      <th>N</th>\n",
       "      <th>D</th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>A</th>\n",
       "      <th>H</th>\n",
       "      <th>M</th>\n",
       "      <th>O</th>\n",
       "      <th>NOT DECISIVE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>970</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>970_right.jpg</td>\n",
       "      <td>cataract</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>127_left.jpg</td>\n",
       "      <td>proliferative diabetic retinopathy，hypertensiv...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>850</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>850_right.jpg</td>\n",
       "      <td>macular epiretinal membrane，moderate non proli...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>37_right.jpg</td>\n",
       "      <td>normal fundus</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4421</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>4421_right.jpg</td>\n",
       "      <td>moderate non proliferative retinopathy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5733</th>\n",
       "      <td>199</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>199_left.jpg</td>\n",
       "      <td>branch retinal vein occlusion</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>516</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>516_right.jpg</td>\n",
       "      <td>moderate non proliferative retinopathy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5735</th>\n",
       "      <td>4603</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>4603_left.jpg</td>\n",
       "      <td>severe nonproliferative retinopathy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736</th>\n",
       "      <td>2132</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>2132_right.jpg</td>\n",
       "      <td>normal fundus</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5737</th>\n",
       "      <td>4487</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>4487_right.jpg</td>\n",
       "      <td>mild nonproliferative retinopathy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5738 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  Patient Age  Patient Sex           Image  \\\n",
       "0      970           56            0   970_right.jpg   \n",
       "1      127           52            1    127_left.jpg   \n",
       "2      850           68            1   850_right.jpg   \n",
       "3       37           41            1    37_right.jpg   \n",
       "4     4421           59            1  4421_right.jpg   \n",
       "...    ...          ...          ...             ...   \n",
       "5733   199           50            0    199_left.jpg   \n",
       "5734   516           42            1   516_right.jpg   \n",
       "5735  4603           47            0   4603_left.jpg   \n",
       "5736  2132           59            0  2132_right.jpg   \n",
       "5737  4487           55            0  4487_right.jpg   \n",
       "\n",
       "                                               Keywords  eye  N  D  G  C  A  \\\n",
       "0                                              cataract    0  0  0  0  1  0   \n",
       "1     proliferative diabetic retinopathy，hypertensiv...    1  0  1  0  0  0   \n",
       "2     macular epiretinal membrane，moderate non proli...    1  0  1  0  0  0   \n",
       "3                                         normal fundus    1  1  0  0  0  0   \n",
       "4                moderate non proliferative retinopathy    1  0  1  0  0  0   \n",
       "...                                                 ...  ... .. .. .. .. ..   \n",
       "5733                      branch retinal vein occlusion    0  0  0  0  0  0   \n",
       "5734             moderate non proliferative retinopathy    1  0  1  0  0  0   \n",
       "5735                severe nonproliferative retinopathy    0  0  1  0  0  0   \n",
       "5736                                      normal fundus    0  1  0  0  0  0   \n",
       "5737                  mild nonproliferative retinopathy    0  0  1  0  0  0   \n",
       "\n",
       "      H  M  O  NOT DECISIVE  \n",
       "0     0  0  0             0  \n",
       "1     0  0  0             0  \n",
       "2     0  0  0             0  \n",
       "3     0  0  0             0  \n",
       "4     0  0  0             0  \n",
       "...  .. .. ..           ...  \n",
       "5733  0  0  1             0  \n",
       "5734  0  0  0             0  \n",
       "5735  0  0  0             0  \n",
       "5736  0  0  0             0  \n",
       "5737  0  0  0             0  \n",
       "\n",
       "[5738 rows x 15 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_val_df['Keywords'].apply(lambda x: len(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4877, 861)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(train_val_df, test_size = 0.15, random_state= 123456)\n",
    "len(train_df), len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "rescale_transform = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.CenterCrop(IMG_SIZE),\n",
    "    torchvision.transforms.Resize(IMG_SIZE, antialias = False, interpolation = torchvision.transforms.InterpolationMode.NEAREST),\n",
    "    torchvision.transforms.Normalize(\n",
    "        timm.data.constants.IMAGENET_DEFAULT_MEAN,\n",
    "        timm.data.constants.IMAGENET_DEFAULT_STD\n",
    "    )\n",
    "])\n",
    "\n",
    "augmentation = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.PILToTensor(),\n",
    "    torchvision.transforms.Resize(IMG_SIZE, antialias = False, interpolation = torchvision.transforms.InterpolationMode.NEAREST),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p = 0.5),\n",
    "#     torchvision.transforms.RandomVerticalFlip(p= 0.5),\n",
    "    #torchvision.transforms.RandomRotation(90)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56,  0],\n",
       "       [52,  1],\n",
       "       [68,  1],\n",
       "       ...,\n",
       "       [47,  0],\n",
       "       [59,  0],\n",
       "       [55,  0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.c_[train_val_df['Patient Age'].to_numpy(), train_val_df['Patient Sex'].to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODIRDataset(Dataset) :\n",
    "    def __init__(self, df, IMG_FOLDER, extractor = rescale_transform, augmentation = None) :\n",
    "        '''\n",
    "        id : list of samples ids as string\n",
    "        '''\n",
    "        #self.text = [tokenizer(text = x, padding = 'max_length', max_length = 40, truncation = True, return_tensors = 'pt') for x in df['Keywords']]\n",
    "        self.images = [Image.open(IMG_PATH + x).convert(\"RGB\") for x in df['Image']]\n",
    "        processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "        self.images = [processor(x, return_tensors=\"pt\") for x in self.images]\n",
    "        sex = df['Patient Age'].to_numpy()\n",
    "        age = (df['Patient Age']/df['Patient Age'].max()).to_numpy()\n",
    "        self.feats = torch.tensor(np.c_[sex, age], requires_grad= True)\n",
    "        self.labels = torch.tensor(df[['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']].to_numpy()).float()\n",
    "        self.img_dir = [IMG_PATH + x for x in df['Image']]\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "#         self.images = [extractor(torchvision.io.read_image(x)/255.0) for x in self.img_dir]\n",
    "        #self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        batch_imgs = self.images[idx]\n",
    "#         if(self.augmentation is not None):\n",
    "#             batch_imgs = self.augmentation(batch_imgs)\n",
    "        return batch_imgs, self.feats[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = ODIRDataset(train_df, IMG_PATH, augmentation = augmentation)\n",
    "# val_dataset   = ODIRDataset(val_df, IMG_PATH)\n",
    "# test_dataset  = ODIRDataset(test_df, IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"datasets.trch\", 'wb') as f:\n",
    "#     torch.save([train_dataset, val_dataset, test_dataset], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets.trch\", 'rb') as f:\n",
    "    train_dataset, val_dataset, test_dataset = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size= BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTv2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base = Swinv2Model.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "        self.base.pooler = nn.AdaptiveAvgPool1d(output_size=1)\n",
    "        self.fc = nn.Linear(768+2, 1000)\n",
    "        self.img_head = nn.Linear(1000, 8)\n",
    "\n",
    "    \n",
    "    def forward(self, pixel_values, add_info):\n",
    "        pixel_values = pixel_values['pixel_values']\n",
    "        pixel_values = pixel_values.squeeze(1)\n",
    "        out = self.base(pixel_values)['pooler_output']\n",
    "        out = torch.hstack([out, add_info])\n",
    "        out = F.relu(self.fc(out))\n",
    "        out = self.img_head(out)\n",
    "\n",
    "        out = F.sigmoid(out)\n",
    "        return out#, img_outs, txt_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swinv2-tiny-patch4-window8-256 were not used when initializing Swinv2Model: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing Swinv2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Swinv2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model= SwinTv2().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "\n",
    "    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "early_stopping_rgb = EarlyStopping(patience=4, verbose=True, path = 'finetuned_swint_rgb.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:45<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.2686, acc img: 0.8838\n",
      "Epoch [1/10], Val Loss: 0.2149, acc img: 0.9027\n",
      "Validation loss decreased (inf --> 0.006717).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:44<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 0.1919, acc img: 0.9098\n",
      "Epoch [2/10], Val Loss: 0.1926, acc img: 0.9096\n",
      "Validation loss decreased (0.006717 --> 0.006018).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:44<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 0.1679, acc img: 0.9224\n",
      "Epoch [3/10], Val Loss: 0.1806, acc img: 0.9191\n",
      "Validation loss decreased (0.006018 --> 0.005644).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:46<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 0.1492, acc img: 0.9302\n",
      "Epoch [4/10], Val Loss: 0.1777, acc img: 0.9183\n",
      "Validation loss decreased (0.005644 --> 0.005554).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:43<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 0.1283, acc img: 0.9431\n",
      "Epoch [5/10], Val Loss: 0.1811, acc img: 0.9181\n",
      "EarlyStopping counter: 1 out of 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:46<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Train Loss: 0.1062, acc img: 0.9541\n",
      "Epoch [6/10], Val Loss: 0.1805, acc img: 0.9183\n",
      "EarlyStopping counter: 2 out of 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:43<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Train Loss: 0.0801, acc img: 0.9673\n",
      "Epoch [7/10], Val Loss: 0.1952, acc img: 0.9194\n",
      "EarlyStopping counter: 3 out of 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:43<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Train Loss: 0.0550, acc img: 0.9792\n",
      "Epoch [8/10], Val Loss: 0.2207, acc img: 0.9175\n",
      "EarlyStopping counter: 4 out of 4\n",
      "Early stopping\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "weights = torch.tensor([0.5, 1., 1.25, 1.25, 1.25, 1.3, 1.25, .9]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)\n",
    "train_img_acc = MultilabelAccuracy(8, average = 'micro').to(device)\n",
    "val_img_acc = MultilabelAccuracy(8, average = 'micro').to(device)\n",
    "\n",
    "img_loss_fn = nn.BCELoss(weights)\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "\n",
    "  total_acc_train = 0\n",
    "  total_loss_train = 0\n",
    "\n",
    "  for train_image, train_metadata, train_label in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        train_label = train_label.to(device)\n",
    "        train_image = train_image.to(device)\n",
    "        train_metadata = train_metadata.to(device).float()\n",
    "      \n",
    "\n",
    "        output = model(train_image, train_metadata)\n",
    "\n",
    "        batch_loss = img_loss_fn(output, train_label)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_train += batch_loss.item()\n",
    "      \n",
    "        train_img_acc(output, train_label)\n",
    "\n",
    "  total_loss_val = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "      #Validation\n",
    "      for val_image, val_metadata, val_label in val_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        val_label = val_label.to(device)\n",
    "        val_image = val_image.to(device)\n",
    "        val_metadata = val_metadata.to(device).float()\n",
    "      \n",
    "\n",
    "        output = model(val_image, val_metadata)\n",
    "\n",
    "        batch_loss = img_loss_fn(output, val_label)\n",
    "        total_loss_val += batch_loss.item()\n",
    "        val_img_acc(output, val_label)\n",
    "              \n",
    "      \n",
    "  avg_train_loss = total_loss_train/len(train_df)\n",
    "\n",
    "  avg_val_loss = total_loss_val/len(val_df)\n",
    "\n",
    "\n",
    "  print(\"Epoch [{}/{}], Train Loss: {:.4f}, acc img: {:.4f}\".format(epoch_num+1, EPOCHS, avg_train_loss*BATCH_SIZE, train_img_acc.compute()))\n",
    "  print(\"Epoch [{}/{}], Val Loss: {:.4f}, acc img: {:.4f}\".format(epoch_num+1, EPOCHS, avg_val_loss*BATCH_SIZE, val_img_acc.compute()))\n",
    "  early_stopping_rgb(avg_val_loss, model)\n",
    "\n",
    "  if early_stopping_rgb.early_stop:\n",
    "      print(\"Early stopping\")\n",
    "      print('-'*60)\n",
    "      break\n",
    "\n",
    "  train_acc.append(train_img_acc.compute())\n",
    "  val_acc.append(val_img_acc.compute())\n",
    "  train_loss.append(avg_train_loss)\n",
    "  val_loss.append(avg_val_loss)\n",
    "  train_img_acc.reset()\n",
    "  val_img_acc.reset()\n",
    "\n",
    "  torch.save(model.state_dict(), './' + 'checkpoint_swint_rgb' + '.pt' )\n",
    "\n",
    "#torch.save(model.state_dict(), './' + 'finetuned_swint_rgb' + '.pt' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_t = [x.item() for x in train_acc] \n",
    "ac_t = [x.item() for x in val_acc] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x162d54e31d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (12, 6))\n",
    "ax[0].plot(range(len(train_acc)), ac_t, label = 'Train Accuracy')\n",
    "ax[0].plot(range(len(train_acc)), ac_t, label = 'Val Accuracy')\n",
    "ax[0].set_title(\"Accuracy vs Epochs\")\n",
    "ax[0].legend(loc = 'upper left')\n",
    "\n",
    "ax[1].plot(range(len(train_acc)), train_loss, label = 'Train Loss')\n",
    "ax[1].plot(range(len(train_acc)), val_loss, label = 'Val Loss')\n",
    "ax[1].set_title(\"Loss vs Epochs\")\n",
    "ax[1].legend(loc = 'upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"finetuned_swint_rgb.pt\", 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.913793\n",
      "Prec: 0.701219\n",
      "Recall: 0.541\n",
      "F1-score: 0.611\n",
      "F-Beta-score: 0.628\n",
      "Kappa: 0.000\n",
      "AUC: 0.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#criterion = nn.BCELoss(torch.tensor([0.5, 1, 5, 5, 5, 6, 5, 1]).float().to(device))\n",
    "\n",
    "test_loss = 0\n",
    "test_acc  = 0\n",
    "\n",
    "AVERAGING = 'micro'\n",
    "PREC = torchmetrics.classification.MultilabelPrecision(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "ACC = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "REC = torchmetrics.classification.MultilabelRecall(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "F1_SCORE = torchmetrics.classification.MultilabelF1Score(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "F_BETA_SCORE = torchmetrics.classification.MultilabelFBetaScore(beta = 0.8, num_classes = 8, num_labels = 8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "KAPPA = torchmetrics.classification.MulticlassCohenKappa(8).to(device)#, validate_args = False)\n",
    "AUC = torchmetrics.classification.MultilabelAUROC(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "\n",
    "for train_image, train_metadata, train_label in tqdm(test_dataloader): \n",
    "    with torch.no_grad():\n",
    "        optimizer.zero_grad()\n",
    "        train_label = train_label.to(device)\n",
    "        train_image = train_image.to(device)\n",
    "        train_metadata = train_metadata.to(device).float()\n",
    "      \n",
    "\n",
    "        predictions = model(train_image, train_metadata)\n",
    "\n",
    "\n",
    "        train_label = train_label.long()\n",
    "        PREC(predictions, train_label)\n",
    "        ACC(predictions, train_label)\n",
    "        REC(predictions, train_label)\n",
    "        F1_SCORE(predictions, train_label)\n",
    "        F_BETA_SCORE(predictions, train_label)\n",
    "        KAPPA(predictions, train_label)\n",
    "        AUC(predictions, train_label)\n",
    "\n",
    "\n",
    "add_prec = PREC.compute()\n",
    "add_acc = ACC.compute()\n",
    "add_rec = REC.compute()\n",
    "add_f1 = F1_SCORE.compute()\n",
    "add_fbeta = F_BETA_SCORE.compute()\n",
    "add_kappa = KAPPA.compute()\n",
    "add_auc = AUC.compute()\n",
    "\n",
    "avg_test_loss = test_loss/len(test_df)*BATCH_SIZE\n",
    "avg_test_acc  = test_acc /len(test_df)\n",
    "\n",
    "print(\"Acc: {:3f}\\nPrec: {:3f}\\nRecall: {:.3f}\\nF1-score: {:.3f}\\nF-Beta-score: {:.3f}\\nKappa: {:.3f}\\nAUC: {:.3f}\".format(add_acc, add_prec,add_rec, add_f1, add_fbeta, add_kappa, add_auc))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = ROOT_DIR + 'dataset_single_eye.csv'\n",
    "TEST_CSV = ROOT_DIR + 'TESTING_dataset_single_eye.csv'\n",
    "IMG_PATH = ROOT_DIR + 'preprocessed_histeq_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 512])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.io.read_image(IMG_PATH + '0_left.jpg').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df = pd.read_csv(CSV_PATH)\n",
    "test_df = pd.read_csv(TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df:pd.DataFrame):\n",
    "    df['Keywords'] = df['Keywords'].str.lower()\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: \" \".join(x.split()))\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
    "    return df\n",
    "train_val_df = preprocess_text(train_val_df)\n",
    "test_df = preprocess_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df['Patient Sex'] = train_val_df['Patient Sex'].astype('category').cat.codes\n",
    "test_df['Patient Sex'] = test_df['Patient Sex'].astype('category').cat.codes\n",
    "\n",
    "train_val_df['eye'] = train_val_df['Patient Sex'].astype('category').cat.codes\n",
    "test_df['eye'] = test_df['Patient Sex'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Sex</th>\n",
       "      <th>Image</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>eye</th>\n",
       "      <th>N</th>\n",
       "      <th>D</th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>A</th>\n",
       "      <th>H</th>\n",
       "      <th>M</th>\n",
       "      <th>O</th>\n",
       "      <th>NOT DECISIVE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>970</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>970_right.jpg</td>\n",
       "      <td>cataract</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>127_left.jpg</td>\n",
       "      <td>proliferative diabetic retinopathy，hypertensiv...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>850</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>850_right.jpg</td>\n",
       "      <td>macular epiretinal membrane，moderate non proli...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>37_right.jpg</td>\n",
       "      <td>normal fundus</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4421</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>4421_right.jpg</td>\n",
       "      <td>moderate non proliferative retinopathy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5733</th>\n",
       "      <td>199</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>199_left.jpg</td>\n",
       "      <td>branch retinal vein occlusion</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>516</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>516_right.jpg</td>\n",
       "      <td>moderate non proliferative retinopathy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5735</th>\n",
       "      <td>4603</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>4603_left.jpg</td>\n",
       "      <td>severe nonproliferative retinopathy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736</th>\n",
       "      <td>2132</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>2132_right.jpg</td>\n",
       "      <td>normal fundus</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5737</th>\n",
       "      <td>4487</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>4487_right.jpg</td>\n",
       "      <td>mild nonproliferative retinopathy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5738 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  Patient Age  Patient Sex           Image  \\\n",
       "0      970           56            0   970_right.jpg   \n",
       "1      127           52            1    127_left.jpg   \n",
       "2      850           68            1   850_right.jpg   \n",
       "3       37           41            1    37_right.jpg   \n",
       "4     4421           59            1  4421_right.jpg   \n",
       "...    ...          ...          ...             ...   \n",
       "5733   199           50            0    199_left.jpg   \n",
       "5734   516           42            1   516_right.jpg   \n",
       "5735  4603           47            0   4603_left.jpg   \n",
       "5736  2132           59            0  2132_right.jpg   \n",
       "5737  4487           55            0  4487_right.jpg   \n",
       "\n",
       "                                               Keywords  eye  N  D  G  C  A  \\\n",
       "0                                              cataract    0  0  0  0  1  0   \n",
       "1     proliferative diabetic retinopathy，hypertensiv...    1  0  1  0  0  0   \n",
       "2     macular epiretinal membrane，moderate non proli...    1  0  1  0  0  0   \n",
       "3                                         normal fundus    1  1  0  0  0  0   \n",
       "4                moderate non proliferative retinopathy    1  0  1  0  0  0   \n",
       "...                                                 ...  ... .. .. .. .. ..   \n",
       "5733                      branch retinal vein occlusion    0  0  0  0  0  0   \n",
       "5734             moderate non proliferative retinopathy    1  0  1  0  0  0   \n",
       "5735                severe nonproliferative retinopathy    0  0  1  0  0  0   \n",
       "5736                                      normal fundus    0  1  0  0  0  0   \n",
       "5737                  mild nonproliferative retinopathy    0  0  1  0  0  0   \n",
       "\n",
       "      H  M  O  NOT DECISIVE  \n",
       "0     0  0  0             0  \n",
       "1     0  0  0             0  \n",
       "2     0  0  0             0  \n",
       "3     0  0  0             0  \n",
       "4     0  0  0             0  \n",
       "...  .. .. ..           ...  \n",
       "5733  0  0  1             0  \n",
       "5734  0  0  0             0  \n",
       "5735  0  0  0             0  \n",
       "5736  0  0  0             0  \n",
       "5737  0  0  0             0  \n",
       "\n",
       "[5738 rows x 15 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_val_df['Keywords'].apply(lambda x: len(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4877, 861)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(train_val_df, test_size = 0.15, random_state= 123456)\n",
    "len(train_df), len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "rescale_transform = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.CenterCrop(IMG_SIZE),\n",
    "    torchvision.transforms.Resize(IMG_SIZE, antialias = False, interpolation = torchvision.transforms.InterpolationMode.NEAREST),\n",
    "    torchvision.transforms.Normalize(\n",
    "        timm.data.constants.IMAGENET_DEFAULT_MEAN,\n",
    "        timm.data.constants.IMAGENET_DEFAULT_STD\n",
    "    )\n",
    "])\n",
    "\n",
    "augmentation = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.PILToTensor(),\n",
    "    torchvision.transforms.Resize(IMG_SIZE, antialias = False, interpolation = torchvision.transforms.InterpolationMode.NEAREST),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p = 0.5),\n",
    "#     torchvision.transforms.RandomVerticalFlip(p= 0.5),\n",
    "    #torchvision.transforms.RandomRotation(90)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56,  0],\n",
       "       [52,  1],\n",
       "       [68,  1],\n",
       "       ...,\n",
       "       [47,  0],\n",
       "       [59,  0],\n",
       "       [55,  0]], dtype=int64)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.c_[train_val_df['Patient Age'].to_numpy(), train_val_df['Patient Sex'].to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODIRDataset(Dataset) :\n",
    "    def __init__(self, df, IMG_FOLDER, extractor = rescale_transform, augmentation = None) :\n",
    "        '''\n",
    "        id : list of samples ids as string\n",
    "        '''\n",
    "        #self.text = [tokenizer(text = x, padding = 'max_length', max_length = 40, truncation = True, return_tensors = 'pt') for x in df['Keywords']]\n",
    "        self.images = [Image.open(IMG_PATH + x).convert(\"RGB\") for x in df['Image']]\n",
    "        processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "        self.images = [processor(x, return_tensors=\"pt\") for x in self.images]\n",
    "        sex = df['Patient Age'].to_numpy()\n",
    "        age = (df['Patient Age']/df['Patient Age'].max()).to_numpy()\n",
    "        self.feats = torch.tensor(np.c_[sex, age], requires_grad= True)\n",
    "        self.labels = torch.tensor(df[['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']].to_numpy()).float()\n",
    "        self.img_dir = [IMG_PATH + x for x in df['Image']]\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "#         self.images = [extractor(torchvision.io.read_image(x)/255.0) for x in self.img_dir]\n",
    "        #self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        batch_imgs = self.images[idx]\n",
    "#         if(self.augmentation is not None):\n",
    "#             batch_imgs = self.augmentation(batch_imgs)\n",
    "        return batch_imgs, self.feats[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = ODIRDataset(train_df, IMG_PATH, augmentation = augmentation)\n",
    "# val_dataset   = ODIRDataset(val_df, IMG_PATH)\n",
    "# test_dataset  = ODIRDataset(test_df, IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"datasets_hist.trch\", 'wb') as f:\n",
    "#     torch.save([train_dataset, val_dataset, test_dataset], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets_hist.trch\", 'rb') as f:\n",
    "    train_dataset, val_dataset, test_dataset = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size= BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTv2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base = Swinv2Model.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "        self.base.pooler = nn.AdaptiveAvgPool1d(output_size=1)\n",
    "        self.fc = nn.Linear(768+2, 1000)\n",
    "        self.img_head = nn.Linear(1000, 8)\n",
    "\n",
    "    \n",
    "    def forward(self, pixel_values, add_info):\n",
    "        pixel_values = pixel_values['pixel_values']\n",
    "        pixel_values = pixel_values.squeeze(1)\n",
    "        out = self.base(pixel_values)['pooler_output']\n",
    "        out = torch.hstack([out, add_info])\n",
    "        out = F.relu(self.fc(out))\n",
    "        out = self.img_head(out)\n",
    "\n",
    "        out = F.sigmoid(out)\n",
    "        return out#, img_outs, txt_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swinv2-tiny-patch4-window8-256 were not used when initializing Swinv2Model: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing Swinv2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Swinv2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model= SwinTv2().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "\n",
    "    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "early_stopping = EarlyStopping(patience=4, verbose=True, path = 'finetuned_swint_hist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:50<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.2916, acc img: 0.8645\n",
      "Epoch [1/10], Val Loss: 0.2304, acc img: 0.8939\n",
      "Validation loss decreased (inf --> 0.007200).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:51<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 0.2061, acc img: 0.9042\n",
      "Epoch [2/10], Val Loss: 0.1996, acc img: 0.9106\n",
      "Validation loss decreased (0.007200 --> 0.006237).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:52<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 0.1811, acc img: 0.9165\n",
      "Epoch [3/10], Val Loss: 0.1891, acc img: 0.9152\n",
      "Validation loss decreased (0.006237 --> 0.005908).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:52<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 0.1596, acc img: 0.9261\n",
      "Epoch [4/10], Val Loss: 0.1889, acc img: 0.9116\n",
      "Validation loss decreased (0.005908 --> 0.005902).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:49<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 0.1420, acc img: 0.9354\n",
      "Epoch [5/10], Val Loss: 0.1831, acc img: 0.9164\n",
      "Validation loss decreased (0.005902 --> 0.005722).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:49<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Train Loss: 0.1222, acc img: 0.9451\n",
      "Epoch [6/10], Val Loss: 0.1854, acc img: 0.9181\n",
      "EarlyStopping counter: 1 out of 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:52<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Train Loss: 0.0984, acc img: 0.9584\n",
      "Epoch [7/10], Val Loss: 0.1972, acc img: 0.9165\n",
      "EarlyStopping counter: 2 out of 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [01:00<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Train Loss: 0.0739, acc img: 0.9705\n",
      "Epoch [8/10], Val Loss: 0.2107, acc img: 0.9191\n",
      "EarlyStopping counter: 3 out of 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:56<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Train Loss: 0.0504, acc img: 0.9824\n",
      "Epoch [9/10], Val Loss: 0.2413, acc img: 0.9148\n",
      "EarlyStopping counter: 4 out of 4\n",
      "Early stopping\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)\n",
    "train_img_acc = MultilabelAccuracy(8, average = 'micro').to(device)\n",
    "val_img_acc = MultilabelAccuracy(8, average = 'micro').to(device)\n",
    "\n",
    "img_loss_fn = nn.BCELoss(weights)\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "\n",
    "  total_acc_train = 0\n",
    "  total_loss_train = 0\n",
    "\n",
    "  for train_image, train_metadata, train_label in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        train_label = train_label.to(device)\n",
    "        train_image = train_image.to(device)\n",
    "        train_metadata = train_metadata.to(device).float()\n",
    "      \n",
    "\n",
    "        output = model(train_image, train_metadata)\n",
    "\n",
    "        batch_loss = img_loss_fn(output, train_label)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_train += batch_loss.item()\n",
    "      \n",
    "        train_img_acc(output, train_label)\n",
    "\n",
    "  total_loss_val = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "      #Validation\n",
    "      for val_image, val_metadata, val_label in val_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        val_label = val_label.to(device)\n",
    "        val_image = val_image.to(device)\n",
    "        val_metadata = val_metadata.to(device).float()\n",
    "      \n",
    "\n",
    "        output = model(val_image, val_metadata)\n",
    "\n",
    "        batch_loss = img_loss_fn(output, val_label)\n",
    "        total_loss_val += batch_loss.item()\n",
    "        val_img_acc(output, val_label)\n",
    "              \n",
    "      \n",
    "  avg_train_loss = total_loss_train/len(train_df)\n",
    "\n",
    "  avg_val_loss = total_loss_val/len(val_df)\n",
    "\n",
    "\n",
    "  print(\"Epoch [{}/{}], Train Loss: {:.4f}, acc img: {:.4f}\".format(epoch_num+1, EPOCHS, avg_train_loss*BATCH_SIZE, train_img_acc.compute()))\n",
    "  print(\"Epoch [{}/{}], Val Loss: {:.4f}, acc img: {:.4f}\".format(epoch_num+1, EPOCHS, avg_val_loss*BATCH_SIZE, val_img_acc.compute()))\n",
    "  early_stopping(avg_val_loss, model)\n",
    "\n",
    "  if early_stopping.early_stop:\n",
    "      print(\"Early stopping\")\n",
    "      print('-'*60)\n",
    "      break\n",
    "  train_img_acc.reset()\n",
    "  val_img_acc.reset()\n",
    "\n",
    "  #torch.save(model.state_dict(), './' + 'checkpoint_swint_hist' + '.pt' )\n",
    "\n",
    "#torch.save(model.state_dict(), './' + 'finetuned_swint_hist' + '.pt' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"finetuned_swint_hist.pt\", 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.913401\n",
      "Prec: 0.697581\n",
      "Recall: 0.542\n",
      "F1-score: 0.610\n",
      "F-Beta-score: 0.627\n",
      "Kappa: 0.000\n",
      "AUC: 0.932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#criterion = nn.BCELoss(torch.tensor([0.5, 1, 5, 5, 5, 6, 5, 1]).float().to(device))\n",
    "\n",
    "test_loss = 0\n",
    "test_acc  = 0\n",
    "\n",
    "AVERAGING = 'micro'\n",
    "PREC = torchmetrics.classification.MultilabelPrecision(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "ACC = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "REC = torchmetrics.classification.MultilabelRecall(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "F1_SCORE = torchmetrics.classification.MultilabelF1Score(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "F_BETA_SCORE = torchmetrics.classification.MultilabelFBetaScore(beta = 0.8, num_classes = 8, num_labels = 8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "KAPPA = torchmetrics.classification.MulticlassCohenKappa(8).to(device)#, validate_args = False)\n",
    "AUC = torchmetrics.classification.MultilabelAUROC(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "\n",
    "for train_image, train_metadata, train_label in tqdm(test_dataloader): \n",
    "    with torch.no_grad():\n",
    "        optimizer.zero_grad()\n",
    "        train_label = train_label.to(device)\n",
    "        train_image = train_image.to(device)\n",
    "        train_metadata = train_metadata.to(device).float()\n",
    "      \n",
    "\n",
    "        predictions = model(train_image, train_metadata)\n",
    "\n",
    "\n",
    "        train_label = train_label.long()\n",
    "        PREC(predictions, train_label)\n",
    "        ACC(predictions, train_label)\n",
    "        REC(predictions, train_label)\n",
    "        F1_SCORE(predictions, train_label)\n",
    "        F_BETA_SCORE(predictions, train_label)\n",
    "        KAPPA(predictions, train_label)\n",
    "        AUC(predictions, train_label)\n",
    "\n",
    "\n",
    "add_prec = PREC.compute()\n",
    "add_acc = ACC.compute()\n",
    "add_rec = REC.compute()\n",
    "add_f1 = F1_SCORE.compute()\n",
    "add_fbeta = F_BETA_SCORE.compute()\n",
    "add_kappa = KAPPA.compute()\n",
    "add_auc = AUC.compute()\n",
    "\n",
    "avg_test_loss = test_loss/len(test_df)*BATCH_SIZE\n",
    "avg_test_acc  = test_acc /len(test_df)\n",
    "\n",
    "print(\"Acc: {:3f}\\nPrec: {:3f}\\nRecall: {:.3f}\\nF1-score: {:.3f}\\nF-Beta-score: {:.3f}\\nKappa: {:.3f}\\nAUC: {:.3f}\".format(add_acc, add_prec,add_rec, add_f1, add_fbeta, add_kappa, add_auc))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSEMBLE STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = ROOT_DIR + 'dataset_single_eye.csv'\n",
    "TEST_CSV = ROOT_DIR + 'TESTING_dataset_single_eye.csv'\n",
    "IMG_PATH = ROOT_DIR + 'preprocessed_images/'\n",
    "HIST_IMG_PATH = ROOT_DIR + 'preprocessed_histeq_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.io.read_image(IMG_PATH + '0_left.jpg').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df = pd.read_csv(CSV_PATH)\n",
    "test_df = pd.read_csv(TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df:pd.DataFrame):\n",
    "    df['Keywords'] = df['Keywords'].str.lower()\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: \" \".join(x.split()))\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
    "    return df\n",
    "train_val_df = preprocess_text(train_val_df)\n",
    "test_df = preprocess_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df['Patient Sex'] = train_val_df['Patient Sex'].astype('category').cat.codes\n",
    "test_df['Patient Sex'] = test_df['Patient Sex'].astype('category').cat.codes\n",
    "\n",
    "train_val_df['eye'] = train_val_df['Patient Sex'].astype('category').cat.codes\n",
    "test_df['eye'] = test_df['Patient Sex'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Sex</th>\n",
       "      <th>Image</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>eye</th>\n",
       "      <th>N</th>\n",
       "      <th>D</th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>A</th>\n",
       "      <th>H</th>\n",
       "      <th>M</th>\n",
       "      <th>O</th>\n",
       "      <th>NOT DECISIVE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>970</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>970_right.jpg</td>\n",
       "      <td>cataract</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>127_left.jpg</td>\n",
       "      <td>proliferative diabetic retinopathy，hypertensiv...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>850</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>850_right.jpg</td>\n",
       "      <td>macular epiretinal membrane，moderate non proli...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>37_right.jpg</td>\n",
       "      <td>normal fundus</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4421</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>4421_right.jpg</td>\n",
       "      <td>moderate non proliferative retinopathy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5733</th>\n",
       "      <td>199</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>199_left.jpg</td>\n",
       "      <td>branch retinal vein occlusion</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>516</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>516_right.jpg</td>\n",
       "      <td>moderate non proliferative retinopathy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5735</th>\n",
       "      <td>4603</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>4603_left.jpg</td>\n",
       "      <td>severe nonproliferative retinopathy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736</th>\n",
       "      <td>2132</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>2132_right.jpg</td>\n",
       "      <td>normal fundus</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5737</th>\n",
       "      <td>4487</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>4487_right.jpg</td>\n",
       "      <td>mild nonproliferative retinopathy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5738 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  Patient Age  Patient Sex           Image  \\\n",
       "0      970           56            0   970_right.jpg   \n",
       "1      127           52            1    127_left.jpg   \n",
       "2      850           68            1   850_right.jpg   \n",
       "3       37           41            1    37_right.jpg   \n",
       "4     4421           59            1  4421_right.jpg   \n",
       "...    ...          ...          ...             ...   \n",
       "5733   199           50            0    199_left.jpg   \n",
       "5734   516           42            1   516_right.jpg   \n",
       "5735  4603           47            0   4603_left.jpg   \n",
       "5736  2132           59            0  2132_right.jpg   \n",
       "5737  4487           55            0  4487_right.jpg   \n",
       "\n",
       "                                               Keywords  eye  N  D  G  C  A  \\\n",
       "0                                              cataract    0  0  0  0  1  0   \n",
       "1     proliferative diabetic retinopathy，hypertensiv...    1  0  1  0  0  0   \n",
       "2     macular epiretinal membrane，moderate non proli...    1  0  1  0  0  0   \n",
       "3                                         normal fundus    1  1  0  0  0  0   \n",
       "4                moderate non proliferative retinopathy    1  0  1  0  0  0   \n",
       "...                                                 ...  ... .. .. .. .. ..   \n",
       "5733                      branch retinal vein occlusion    0  0  0  0  0  0   \n",
       "5734             moderate non proliferative retinopathy    1  0  1  0  0  0   \n",
       "5735                severe nonproliferative retinopathy    0  0  1  0  0  0   \n",
       "5736                                      normal fundus    0  1  0  0  0  0   \n",
       "5737                  mild nonproliferative retinopathy    0  0  1  0  0  0   \n",
       "\n",
       "      H  M  O  NOT DECISIVE  \n",
       "0     0  0  0             0  \n",
       "1     0  0  0             0  \n",
       "2     0  0  0             0  \n",
       "3     0  0  0             0  \n",
       "4     0  0  0             0  \n",
       "...  .. .. ..           ...  \n",
       "5733  0  0  1             0  \n",
       "5734  0  0  0             0  \n",
       "5735  0  0  0             0  \n",
       "5736  0  0  0             0  \n",
       "5737  0  0  0             0  \n",
       "\n",
       "[5738 rows x 15 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_val_df['Keywords'].apply(lambda x: len(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4877, 861)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(train_val_df, test_size = 0.15, random_state= 123456)\n",
    "len(train_df), len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "rescale_transform = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.CenterCrop(IMG_SIZE),\n",
    "    torchvision.transforms.Resize(IMG_SIZE, antialias = False, interpolation = torchvision.transforms.InterpolationMode.NEAREST),\n",
    "    torchvision.transforms.Normalize(\n",
    "        timm.data.constants.IMAGENET_DEFAULT_MEAN,\n",
    "        timm.data.constants.IMAGENET_DEFAULT_STD\n",
    "    )\n",
    "])\n",
    "\n",
    "augmentation = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.PILToTensor(),\n",
    "    torchvision.transforms.Resize(IMG_SIZE, antialias = False, interpolation = torchvision.transforms.InterpolationMode.NEAREST),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p = 0.5),\n",
    "#     torchvision.transforms.RandomVerticalFlip(p= 0.5),\n",
    "    #torchvision.transforms.RandomRotation(90)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56,  0],\n",
       "       [52,  1],\n",
       "       [68,  1],\n",
       "       ...,\n",
       "       [47,  0],\n",
       "       [59,  0],\n",
       "       [55,  0]], dtype=int64)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.c_[train_val_df['Patient Age'].to_numpy(), train_val_df['Patient Sex'].to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODIRDataset(Dataset) :\n",
    "    def __init__(self, df, IMG_FOLDER, extractor = rescale_transform, augmentation = None) :\n",
    "        '''\n",
    "        id : list of samples ids as string\n",
    "        '''\n",
    "        #self.text = [tokenizer(text = x, padding = 'max_length', max_length = 40, truncation = True, return_tensors = 'pt') for x in df['Keywords']]\n",
    "        self.images = [Image.open(IMG_PATH + x).convert(\"RGB\") for x in df['Image']]\n",
    "        self.hist_images = [Image.open(HIST_IMG_PATH + x).convert(\"RGB\") for x in df['Image']]\n",
    "        processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "        self.images = [processor(x, return_tensors=\"pt\") for x in self.images]\n",
    "        self.hist_images = [processor(x, return_tensors=\"pt\") for x in self.hist_images]\n",
    "        sex = df['Patient Age'].to_numpy()\n",
    "        age = (df['Patient Age']/df['Patient Age'].max()).to_numpy()\n",
    "        self.feats = torch.tensor(np.c_[sex, age], requires_grad= True)\n",
    "        self.labels = torch.tensor(df[['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']].to_numpy()).float()\n",
    "        self.img_dir = [IMG_PATH + x for x in df['Image']]\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "#         self.images = [extractor(torchvision.io.read_image(x)/255.0) for x in self.img_dir]\n",
    "        #self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        batch_imgs = self.images[idx]\n",
    "        batch_hist_imgs = self.hist_images[idx]\n",
    "#         if(self.augmentation is not None):\n",
    "#             batch_imgs = self.augmentation(batch_imgs)\n",
    "        return batch_imgs, batch_hist_imgs, self.feats[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = ODIRDataset(train_df, IMG_PATH, augmentation = augmentation)\n",
    "# val_dataset   = ODIRDataset(val_df, IMG_PATH)\n",
    "# test_dataset  = ODIRDataset(test_df, IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"datasets_en.trch\", 'wb') as f:\n",
    "#     torch.save([train_dataset, val_dataset, test_dataset], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets_en.trch\", 'rb') as f:\n",
    "    train_dataset, val_dataset, test_dataset = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size= BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTv2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base = Swinv2Model.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "        self.base.pooler = nn.AdaptiveAvgPool1d(output_size=1)\n",
    "        self.fc = nn.Linear(768+2, 1000)\n",
    "        self.img_head = nn.Linear(1000, 8)\n",
    "\n",
    "    \n",
    "    def forward(self, pixel_values, add_info):\n",
    "        pixel_values = pixel_values['pixel_values']\n",
    "        pixel_values = pixel_values.squeeze(1)\n",
    "        out = self.base(pixel_values)['pooler_output']\n",
    "        out = torch.hstack([out, add_info])\n",
    "        out = F.relu(self.fc(out))\n",
    "        out = self.img_head(out)\n",
    "\n",
    "        out = F.sigmoid(out)\n",
    "        return out#, img_outs, txt_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path, device=\"cpu\"):\n",
    "    model = SwinTv2().to(device)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class en_SwinTv2():    \n",
    "    def __init__(self, normal_path, grey_path) :\n",
    "        self.normal_SwinTv2 = load_model(normal_path,device=\"cuda\")\n",
    "        self.grey_SwinTv2 = load_model(grey_path,device=\"cuda\")\n",
    "        \n",
    "    def test_sample(self, sample, sample_hist, train_metadata):\n",
    "        output_normal = self.normal_SwinTv2(sample, train_metadata)\n",
    "        output_grey = self.grey_SwinTv2(sample_hist, train_metadata)\n",
    "        outputs = torch.add(output_normal, output_grey)\n",
    "        outputs = torch.div(outputs, 2)\n",
    "        return outputs\n",
    "\n",
    "    def test(self, test_loader):\n",
    "        self.normal_SwinTv2.eval()\n",
    "        self.grey_SwinTv2.eval()\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "            test_loss = 0\n",
    "            test_acc  = 0\n",
    "\n",
    "            AVERAGING = 'micro'\n",
    "            PREC = torchmetrics.classification.MultilabelPrecision(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "            ACC = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "            REC = torchmetrics.classification.MultilabelRecall(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "            F1_SCORE = torchmetrics.classification.MultilabelF1Score(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "            F_BETA_SCORE = torchmetrics.classification.MultilabelFBetaScore(beta = 0.8, num_classes = 8, num_labels = 8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "            KAPPA = torchmetrics.classification.MulticlassCohenKappa(8).to(device)#, validate_args = False)\n",
    "            AUC = torchmetrics.classification.MultilabelAUROC(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "\n",
    "            for train_image, train_hist_image, train_metadata, train_label in tqdm(test_dataloader): \n",
    "                with torch.no_grad():\n",
    "                    optimizer.zero_grad()\n",
    "                    train_label = train_label.to(device)\n",
    "                    train_image = train_image.to(device)\n",
    "                    train_hist_image = train_hist_image.to(device)\n",
    "                    train_metadata = train_metadata.to(device).float()\n",
    "\n",
    "\n",
    "                    predictions = self.test_sample(train_image, train_hist_image, train_metadata)\n",
    "\n",
    "\n",
    "                    train_label = train_label.long()\n",
    "                    PREC(predictions, train_label)\n",
    "                    ACC(predictions, train_label)\n",
    "                    REC(predictions, train_label)\n",
    "                    F1_SCORE(predictions, train_label)\n",
    "                    F_BETA_SCORE(predictions, train_label)\n",
    "                    KAPPA(predictions, train_label)\n",
    "                    AUC(predictions, train_label)\n",
    "\n",
    "\n",
    "            add_prec = PREC.compute()\n",
    "            add_acc = ACC.compute()\n",
    "            add_rec = REC.compute()\n",
    "            add_f1 = F1_SCORE.compute()\n",
    "            add_fbeta = F_BETA_SCORE.compute()\n",
    "            add_kappa = KAPPA.compute()\n",
    "            add_auc = AUC.compute()\n",
    "\n",
    "            avg_test_loss = test_loss/len(test_df)*BATCH_SIZE\n",
    "            avg_test_acc  = test_acc /len(test_df)\n",
    "\n",
    "            print(\"Acc: {:3f}\\nPrec: {:3f}\\nRecall: {:.3f}\\nF1-score: {:.3f}\\nF-Beta-score: {:.3f}\\nKappa: {:.3f}\\nAUC: {:.3f}\".format(add_acc, add_prec,add_rec, add_f1, add_fbeta, add_kappa, add_auc))\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #criterion = nn.BCELoss(torch.tensor([0.5, 1, 5, 5, 5, 6, 5, 1]).float().to(device))\n",
    "\n",
    "# test_loss = 0\n",
    "# test_acc  = 0\n",
    "\n",
    "# AVERAGING = 'micro'\n",
    "# PREC = torchmetrics.classification.MultilabelPrecision(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "# ACC = torchmetrics.classification.MultilabelAccuracy(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "# REC = torchmetrics.classification.MultilabelRecall(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "# F1_SCORE = torchmetrics.classification.MultilabelF1Score(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "# F_BETA_SCORE = torchmetrics.classification.MultilabelFBetaScore(beta = 0.8, num_classes = 8, num_labels = 8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "# KAPPA = torchmetrics.classification.MulticlassCohenKappa(8).to(device)#, validate_args = False)\n",
    "# AUC = torchmetrics.classification.MultilabelAUROC(8, average = AVERAGING).to(device)#, validate_args = False)\n",
    "\n",
    "# for train_image, train_metadata, train_label in tqdm(test_dataloader): \n",
    "#     with torch.no_grad():\n",
    "#         optimizer.zero_grad()\n",
    "#         train_label = train_label.to(device)\n",
    "#         train_image = train_image.to(device)\n",
    "#         train_metadata = train_metadata.to(device)\n",
    "      \n",
    "\n",
    "#         predictions = model(train_image, train_metadata)\n",
    "\n",
    "\n",
    "#         train_label = train_label.long()\n",
    "#         PREC(predictions, train_label)\n",
    "#         ACC(predictions, train_label)\n",
    "#         REC(predictions, train_label)\n",
    "#         F1_SCORE(predictions, train_label)\n",
    "#         F_BETA_SCORE(predictions, train_label)\n",
    "#         KAPPA(predictions, train_label)\n",
    "#         AUC(predictions, train_label)\n",
    "\n",
    "\n",
    "# add_prec = PREC.compute()\n",
    "# add_acc = ACC.compute()\n",
    "# add_rec = REC.compute()\n",
    "# add_f1 = F1_SCORE.compute()\n",
    "# add_fbeta = F_BETA_SCORE.compute()\n",
    "# add_kappa = KAPPA.compute()\n",
    "# add_auc = AUC.compute()\n",
    "\n",
    "# avg_test_loss = test_loss/len(test_df)*BATCH_SIZE\n",
    "# avg_test_acc  = test_acc /len(test_df)\n",
    "\n",
    "# print(\"Acc: {:3f}\\nPrec: {:3f}\\nRecall: {:.3f}\\nF1-score: {:.3f}\\nF-Beta-score: {:.3f}\\nKappa: {:.3f}\\nAUC: {:.3f}\".format(add_acc, add_prec,add_rec, add_f1, add_fbeta, add_kappa, add_auc))\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab06b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a98638b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swinv2-tiny-patch4-window8-256 were not used when initializing Swinv2Model: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing Swinv2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Swinv2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/swinv2-tiny-patch4-window8-256 were not used when initializing Swinv2Model: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing Swinv2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Swinv2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "SwinTv2_en = en_SwinTv2(\"finetuned_swint_rgb.pt\", \"finetuned_swint_hist.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                                                 Param #\n",
       "===============================================================================================\n",
       "SwinTv2                                                                --\n",
       "├─Swinv2Model: 1-1                                                     --\n",
       "│    └─Swinv2Embeddings: 2-1                                           --\n",
       "│    │    └─Swinv2PatchEmbeddings: 3-1                                 4,704\n",
       "│    │    └─LayerNorm: 3-2                                             192\n",
       "│    │    └─Dropout: 3-3                                               --\n",
       "│    └─Swinv2Encoder: 2-2                                              --\n",
       "│    │    └─ModuleList: 3-4                                            27,571,722\n",
       "│    └─LayerNorm: 2-3                                                  1,536\n",
       "│    └─AdaptiveAvgPool1d: 2-4                                          --\n",
       "├─Linear: 1-2                                                          771,000\n",
       "├─Linear: 1-3                                                          8,008\n",
       "===============================================================================================\n",
       "Total params: 28,357,162\n",
       "Trainable params: 28,357,162\n",
       "Non-trainable params: 0\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "\n",
    "torchinfo.torchinfo.summary(SwinTv2_en.grey_SwinTv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "da6451a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:10<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.917124\n",
      "Prec: 0.725367\n",
      "Recall: 0.542\n",
      "F1-score: 0.621\n",
      "F-Beta-score: 0.641\n",
      "Kappa: 0.000\n",
      "AUC: 0.943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SwinTv2_en.test(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aae2c75324e7fdf5ebd22146e2daffaa477f8ea149f0e685be4c317c2939a685"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
